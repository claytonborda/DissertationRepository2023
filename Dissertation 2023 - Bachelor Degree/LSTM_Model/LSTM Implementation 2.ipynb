{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib as plt \n",
    "\n",
    "tweets = pd.read_csv('combined.csv', encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['tweet_date_created'], axis=1)\n",
    "tweets = tweets.drop(['sentiment_score'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for duplicates and deleting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 762643 duplicate tweet ids. Removing duplicates...\n"
     ]
    }
   ],
   "source": [
    "duplicates = tweets[tweets.duplicated(subset=['tweet_id'], keep=False)]\n",
    "\n",
    "if not duplicates.empty:\n",
    "    print(f\"Found {len(duplicates)} duplicate tweet ids. Removing duplicates...\")\n",
    "    tweets.drop_duplicates(subset=['tweet_id'], inplace=True)\n",
    "else:\n",
    "    print(\"No duplicate tweet ids found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['tweet_id'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting NULLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_text    0\n",
      "language      0\n",
      "sentiment     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets.dropna()\n",
    "print(tweets.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values in the 'language' column are 'en'\n"
     ]
    }
   ],
   "source": [
    "all_english = (tweets['language'] == 'en').all()\n",
    "\n",
    "if all_english:\n",
    "    print(\"All values in the 'language' column are 'en'\")\n",
    "else:\n",
    "    print(\"Not all values in the 'language' column are 'en'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['language'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          2\n",
      "1          2\n",
      "2          2\n",
      "3          0\n",
      "4          2\n",
      "          ..\n",
      "5393957    2\n",
      "5393958    0\n",
      "5393959    2\n",
      "5393960    2\n",
      "5393961    2\n",
      "Name: sentiment_values, Length: 5012534, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bayer Leverkusen goalkeeper Bernd Leno will no...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gary Speed v Blackburn at St James in 2001/02 ...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ChelseaFC Don't make him regret it and start ...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@LiverpoolFF @AnfieldEdition He's a liar, made...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@theesk @Everton Didn't realise Kenwright is d...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text sentiment   \n",
       "0  Bayer Leverkusen goalkeeper Bernd Leno will no...   NEUTRAL  \\\n",
       "1  Gary Speed v Blackburn at St James in 2001/02 ...   NEUTRAL   \n",
       "2  @ChelseaFC Don't make him regret it and start ...   NEUTRAL   \n",
       "3  @LiverpoolFF @AnfieldEdition He's a liar, made...  NEGATIVE   \n",
       "4  @theesk @Everton Didn't realise Kenwright is d...   NEUTRAL   \n",
       "\n",
       "   sentiment_values  \n",
       "0                 2  \n",
       "1                 2  \n",
       "2                 2  \n",
       "3                 0  \n",
       "4                 2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_map = {\"NEGATIVE\": 0, \"POSITIVE\": 1, \"NEUTRAL\": 2, \"MIXED\": 3}\n",
    "\n",
    "# Map the sentiment labels to their numeric values\n",
    "tweets['sentiment_values'] = tweets['sentiment'].map(sentiment_map)\n",
    "\n",
    "# Print the new column that contains the mapped values\n",
    "print(tweets['sentiment_values'])\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices of rows with Mixed sentiment\n",
    "mixed_indices = tweets[tweets['sentiment_values'] == 3].index\n",
    "tweets = tweets.drop(mixed_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of positive tagged tweets is: 1070334\n",
      "No of negative tagged tweets is: 354501\n",
      "No of neutral tagged tweets is: 3549918\n"
     ]
    }
   ],
   "source": [
    "negative_tweets = tweets[tweets['sentiment_values'] == 0]\n",
    "positive_tweets = tweets[tweets['sentiment_values'] == 1]\n",
    "neutral_tweets = tweets[tweets['sentiment_values'] == 2]\n",
    "\n",
    "print('No of positive tagged tweets is: {}'.format(len(positive_tweets)))\n",
    "print('No of negative tagged tweets is: {}'.format(len(negative_tweets)))\n",
    "print('No of neutral tagged tweets is: {}'.format(len(neutral_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of positive tagged tweets is: 354501\n",
      "No of negative tagged tweets is: 354501\n",
      "No of neutral tagged tweets is: 354501\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Shuffle the DataFrame to ensure that the downsampling is random\n",
    "tweets = tweets.sample(frac=1, random_state=42)\n",
    "\n",
    "# Count the number of tweets in each sentiment class\n",
    "counts = tweets['sentiment_values'].value_counts()\n",
    "\n",
    "# Find the smallest class size\n",
    "smallest_size = counts.min()\n",
    "\n",
    "# Downsample each class to the smallest size\n",
    "positive_tweets = tweets[tweets['sentiment_values'] == 1].sample(n=smallest_size, random_state=42)\n",
    "negative_tweets = tweets[tweets['sentiment_values'] == 0].sample(n=smallest_size, random_state=42)\n",
    "neutral_tweets = tweets[tweets['sentiment_values'] == 2].sample(n=smallest_size, random_state=42)\n",
    "\n",
    "# Concatenate the downsampled DataFrames\n",
    "tweets = pd.concat([positive_tweets, negative_tweets, neutral_tweets], ignore_index=True)\n",
    "\n",
    "# Print the new counts of tweets in each class\n",
    "print('No of positive tagged tweets is: {}'.format(len(tweets[tweets['sentiment_values'] == 1])))\n",
    "print('No of negative tagged tweets is: {}'.format(len(tweets[tweets['sentiment_values'] == 0])))\n",
    "print('No of neutral tagged tweets is: {}'.format(len(tweets[tweets['sentiment_values'] == 2])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tweet_text'] = tweets['tweet_text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"â‚¬\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", \n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "     \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_abbrev_in_text(tweets):\n",
    "    t = []\n",
    "    words = tweets.split() \n",
    "    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove whitespace\n",
    "    text = text.strip()\n",
    "    # Remove stopwords and join the words in a single string\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Apply the preprocessing function to the 'text' column\n",
    "tweets['processed_text'] = tweets['tweet_text'].apply(preprocess_text)\n",
    "\n",
    "tweets =  shuffle(tweets).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_values</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Firmino! The @LFC forward finishes off the pas...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "      <td>firmino forward finishes pass lead 20 late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The range of emotions IÂ’ve experienced tonight...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>1</td>\n",
       "      <td>range emotions ive experienced tonight crazy w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@guardian Following on #SocialMedia &amp;amp; #MOT...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>1</td>\n",
       "      <td>following amp tad cheaper still rewarding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Pschmeichel1 @HKane @FIFAWorldCup IÂ’m glad yo...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>1</td>\n",
       "      <td>im glad werent blind united keeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolutely gutted by this.... miss you @JackWi...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>absolutely gutted miss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text sentiment   \n",
       "0  Firmino! The @LFC forward finishes off the pas...   NEUTRAL  \\\n",
       "1  The range of emotions IÂ’ve experienced tonight...  POSITIVE   \n",
       "2  @guardian Following on #SocialMedia &amp; #MOT...  POSITIVE   \n",
       "3  @Pschmeichel1 @HKane @FIFAWorldCup IÂ’m glad yo...  POSITIVE   \n",
       "4  Absolutely gutted by this.... miss you @JackWi...  NEGATIVE   \n",
       "\n",
       "   sentiment_values                                     processed_text  \n",
       "0                 2         firmino forward finishes pass lead 20 late  \n",
       "1                 1  range emotions ive experienced tonight crazy w...  \n",
       "2                 1          following amp tad cheaper still rewarding  \n",
       "3                 1                 im glad werent blind united keeper  \n",
       "4                 0                             absolutely gutted miss  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_na(text):\n",
    "    # Remove \"nan\" and \"na\" and join the words in a single string\n",
    "    text = ' '.join([word for word in text.split() if word not in ('nan', 'na')])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['sentiment'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets['processed_text']\n",
    "y = tweets['sentiment_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 1063503\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "total_rows = tweets.shape[0]\n",
    "print(\"Total number of rows:\", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tweet length: 11.16871320532241\n",
      "Standard deviation of tweet length: 6.7132675050307205\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tweet_lengths = tweets['processed_text'].apply(lambda x: len(x.split()))\n",
    "mean_length = np.mean(tweet_lengths)\n",
    "std_length = np.std(tweet_lengths)\n",
    "\n",
    "print(\"Average tweet length:\", mean_length)\n",
    "print(\"Standard deviation of tweet length:\", std_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "max_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, maxlen=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 744452\n",
      "Validation data size: 159525\n",
      "Testing data size: 159526\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into 70% training and 30% combined validation and testing\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the temporary dataset (30% of the entire dataset) into 50% validation and 50% testing\n",
    "# This results in 15% validation and 15% testing of the entire dataset\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Training data size:\", len(X_train))\n",
    "print(\"Validation data size:\", len(X_val))\n",
    "print(\"Testing data size:\", len(X_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU with LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load GloVe embeddings\n",
    "# def load_glove_embeddings(file_path, embedding_dim, word_index, max_features):\n",
    "#     embeddings_index = {}\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "#             coefs = np.asarray(values[1:], dtype='float32')\n",
    "#             embeddings_index[word] = coefs\n",
    "    \n",
    "#     embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "#     for word, i in word_index.items():\n",
    "#         if i >= max_features:\n",
    "#             continue\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             embedding_matrix[i] = embedding_vector\n",
    "#     return torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "# # Load the GloVe embeddings matrix\n",
    "# glove_file_path = 'glove.twitter.27B.200d.txt'  # Update the path to the downloaded GloVe file\n",
    "# embedding_dim = 200\n",
    "# embedding_matrix = load_glove_embeddings(glove_file_path, embedding_dim, tokenizer.word_index, max_features)\n",
    "\n",
    "# # Define the LSTM model with GloVe embeddings\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, embedding_matrix, embed_dim=200, lstm_out=256, dropout_rate=0.2, num_classes=3):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "#         self.lstm = nn.LSTM(embed_dim, lstm_out, bidirectional=True, batch_first=True, dropout=dropout_rate)\n",
    "#         self.fc = nn.Linear(lstm_out * 2, num_classes)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         x, _ = self.lstm(x)\n",
    "#         x = self.dropout(x[:, -1, :])  # Get the last hidden state of the LSTM\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# # Create and train the model with GloVe embeddings\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = LSTMModel(embedding_matrix).to(device)\n",
    "\n",
    "# X_train_torch = torch.LongTensor(X_train).to(device)\n",
    "# y_train_torch = y_train.to(torch.int64).to(device)  # Change data type to int64\n",
    "# X_test_torch = torch.LongTensor(X_test).to(device)\n",
    "# y_test_torch = y_test.to(torch.int64).to(device)  # Change data type to int64\n",
    "\n",
    "# train_data = TensorDataset(X_train_torch, y_train_torch)\n",
    "# train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# num_epochs = 5\n",
    "# accumulation_steps = 4  # Adjust this value based on your needs\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for i, (batch_X, batch_y) in enumerate(train_dataloader):\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(batch_X)\n",
    "#         loss = criterion(outputs, batch_y) / accumulation_steps  # Normalize the loss\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # Accumulate gradients and update weights every accumulation_steps\n",
    "#         if (i + 1) % accumulation_steps == 0:\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#         running_loss += loss.item() * accumulation_steps \n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Step [{i + 1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# # Create DataLoader for the test set\n",
    "# test_data = TensorDataset(X_test_torch, y_test_torch)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# model.eval()\n",
    "# y_pred = []\n",
    "# y_true = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch_X, batch_y in test_dataloader:\n",
    "#         test_outputs = model(batch_X)\n",
    "#         _, batch_pred = torch.max(test_outputs, 1)\n",
    "#         y_pred.extend(batch_pred.cpu().numpy())\n",
    "#         y_true.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# test_accuracy = accuracy_score(y_true, y_pred)\n",
    "# print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specified Hyper-parameters Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameters Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m[I 2023-04-15 20:43:17,878]\u001b[0m A new study created in memory with name: no-name-07f02b22-6b0b-45e6-b6c8-aa733a96076c\u001b[0m\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 20:47:33,810]\u001b[0m Trial 0 finished with value: 0.7961009246199655 and parameters: {'num_filters': 256.0, 'filter_size': 7.0, 'pool_size': 2.0, 'lstm_out': 256.0, 'dropout_rate': 0.2}. Best is trial 0 with value: 0.7961009246199655.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7961 with hyperparameters: {'num_filters': 256.0, 'filter_size': 7.0, 'pool_size': 2.0, 'lstm_out': 256.0, 'dropout_rate': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 20:50:02,408]\u001b[0m Trial 1 finished with value: 0.7987588152327222 and parameters: {'num_filters': 160.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 192.0, 'dropout_rate': 0.30000000000000004}. Best is trial 1 with value: 0.7987588152327222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7988 with hyperparameters: {'num_filters': 160.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 192.0, 'dropout_rate': 0.30000000000000004}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 20:52:15,942]\u001b[0m Trial 2 finished with value: 0.7253847359348065 and parameters: {'num_filters': 128.0, 'filter_size': 7.0, 'pool_size': 4.0, 'lstm_out': 256.0, 'dropout_rate': 0.4}. Best is trial 1 with value: 0.7987588152327222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7254 with hyperparameters: {'num_filters': 128.0, 'filter_size': 7.0, 'pool_size': 4.0, 'lstm_out': 256.0, 'dropout_rate': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 20:55:36,991]\u001b[0m Trial 3 finished with value: 0.7270208431280364 and parameters: {'num_filters': 160.0, 'filter_size': 7.0, 'pool_size': 4.0, 'lstm_out': 512.0, 'dropout_rate': 0.4}. Best is trial 1 with value: 0.7987588152327222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7270 with hyperparameters: {'num_filters': 160.0, 'filter_size': 7.0, 'pool_size': 4.0, 'lstm_out': 512.0, 'dropout_rate': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 20:57:44,067]\u001b[0m Trial 4 finished with value: 0.7960382385206081 and parameters: {'num_filters': 96.0, 'filter_size': 7.0, 'pool_size': 2.0, 'lstm_out': 192.0, 'dropout_rate': 0.2}. Best is trial 1 with value: 0.7987588152327222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7960 with hyperparameters: {'num_filters': 96.0, 'filter_size': 7.0, 'pool_size': 2.0, 'lstm_out': 192.0, 'dropout_rate': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 21:00:07,347]\u001b[0m Trial 5 finished with value: 0.7959065977119574 and parameters: {'num_filters': 256.0, 'filter_size': 7.0, 'pool_size': 2.0, 'lstm_out': 64.0, 'dropout_rate': 0.30000000000000004}. Best is trial 1 with value: 0.7987588152327222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7959 with hyperparameters: {'num_filters': 256.0, 'filter_size': 7.0, 'pool_size': 2.0, 'lstm_out': 64.0, 'dropout_rate': 0.30000000000000004}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 21:03:34,804]\u001b[0m Trial 6 finished with value: 0.798144491459019 and parameters: {'num_filters': 160.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 256.0, 'dropout_rate': 0.4}. Best is trial 1 with value: 0.7987588152327222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7981 with hyperparameters: {'num_filters': 160.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 256.0, 'dropout_rate': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 21:10:11,953]\u001b[0m Trial 7 finished with value: 0.7937501958940605 and parameters: {'num_filters': 64.0, 'filter_size': 7.0, 'pool_size': 2.0, 'lstm_out': 256.0, 'dropout_rate': 0.4}. Best is trial 1 with value: 0.7987588152327222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7938 with hyperparameters: {'num_filters': 64.0, 'filter_size': 7.0, 'pool_size': 2.0, 'lstm_out': 256.0, 'dropout_rate': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 21:13:15,270]\u001b[0m Trial 8 finished with value: 0.7292211252154834 and parameters: {'num_filters': 64.0, 'filter_size': 3.0, 'pool_size': 4.0, 'lstm_out': 192.0, 'dropout_rate': 0.30000000000000004}. Best is trial 1 with value: 0.7987588152327222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7292 with hyperparameters: {'num_filters': 64.0, 'filter_size': 3.0, 'pool_size': 4.0, 'lstm_out': 192.0, 'dropout_rate': 0.30000000000000004}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 21:15:23,872]\u001b[0m Trial 9 finished with value: 0.797994044820561 and parameters: {'num_filters': 128.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 192.0, 'dropout_rate': 0.2}. Best is trial 1 with value: 0.7987588152327222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7980 with hyperparameters: {'num_filters': 128.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 192.0, 'dropout_rate': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 21:18:44,677]\u001b[0m Trial 10 finished with value: 0.73242438489265 and parameters: {'num_filters': 192.0, 'filter_size': 3.0, 'pool_size': 4.0, 'lstm_out': 512.0, 'dropout_rate': 0.1}. Best is trial 1 with value: 0.7987588152327222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7324 with hyperparameters: {'num_filters': 192.0, 'filter_size': 3.0, 'pool_size': 4.0, 'lstm_out': 512.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 21:36:23,026]\u001b[0m Trial 11 finished with value: 0.7960194326908008 and parameters: {'num_filters': 192.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.30000000000000004}. Best is trial 1 with value: 0.7987588152327222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7960 with hyperparameters: {'num_filters': 192.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.30000000000000004}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 21:38:12,148]\u001b[0m Trial 12 finished with value: 0.7982824008776054 and parameters: {'num_filters': 192.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 64.0, 'dropout_rate': 0.4}. Best is trial 1 with value: 0.7987588152327222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7983 with hyperparameters: {'num_filters': 192.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 64.0, 'dropout_rate': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 21:40:13,666]\u001b[0m Trial 13 finished with value: 0.7984579219558063 and parameters: {'num_filters': 224.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 64.0, 'dropout_rate': 0.30000000000000004}. Best is trial 1 with value: 0.7987588152327222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7985 with hyperparameters: {'num_filters': 224.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 64.0, 'dropout_rate': 0.30000000000000004}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 21:42:25,101]\u001b[0m Trial 14 finished with value: 0.8016987932925874 and parameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 128.0, 'dropout_rate': 0.30000000000000004}. Best is trial 14 with value: 0.8016987932925874.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8017 with hyperparameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 128.0, 'dropout_rate': 0.30000000000000004}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 21:44:34,897]\u001b[0m Trial 15 finished with value: 0.8006832784829964 and parameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 128.0, 'dropout_rate': 0.1}. Best is trial 14 with value: 0.8016987932925874.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8007 with hyperparameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 128.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 21:46:45,132]\u001b[0m Trial 16 finished with value: 0.8040745964582354 and parameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 128.0, 'dropout_rate': 0.1}. Best is trial 16 with value: 0.8040745964582354.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8041 with hyperparameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 128.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 21:48:28,769]\u001b[0m Trial 17 finished with value: 0.7304999216423758 and parameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 4.0, 'lstm_out': 128.0, 'dropout_rate': 0.1}. Best is trial 16 with value: 0.8040745964582354.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7305 with hyperparameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 4.0, 'lstm_out': 128.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 22:06:40,771]\u001b[0m Trial 18 finished with value: 0.804851904090268 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.2}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8049 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 22:24:52,608]\u001b[0m Trial 19 finished with value: 0.8033975865851747 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.1}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8034 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 22:28:17,425]\u001b[0m Trial 20 finished with value: 0.725767121140887 and parameters: {'num_filters': 32.0, 'filter_size': 3.0, 'pool_size': 4.0, 'lstm_out': 448.0, 'dropout_rate': 0.2}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7258 with hyperparameters: {'num_filters': 32.0, 'filter_size': 3.0, 'pool_size': 4.0, 'lstm_out': 448.0, 'dropout_rate': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 22:46:26,056]\u001b[0m Trial 21 finished with value: 0.8023068484563548 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.1}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8023 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 23:04:33,836]\u001b[0m Trial 22 finished with value: 0.8044695188841874 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.1}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8045 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 23:17:37,979]\u001b[0m Trial 23 finished with value: 0.8041247453377214 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.1}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8041 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 23:30:40,922]\u001b[0m Trial 24 finished with value: 0.8044507130543802 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.2}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8045 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 23:43:26,215]\u001b[0m Trial 25 finished with value: 0.8027707255916 and parameters: {'num_filters': 192.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.2}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8028 with hyperparameters: {'num_filters': 192.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 23:48:28,443]\u001b[0m Trial 26 finished with value: 0.8014417802852217 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 448.0, 'dropout_rate': 0.2}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8014 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 448.0, 'dropout_rate': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-15 23:53:25,645]\u001b[0m Trial 27 finished with value: 0.799899702241028 and parameters: {'num_filters': 224.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 448.0, 'dropout_rate': 0.2}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7999 with hyperparameters: {'num_filters': 224.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 448.0, 'dropout_rate': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 00:06:27,264]\u001b[0m Trial 28 finished with value: 0.8027644569816643 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.2}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8028 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 00:24:34,463]\u001b[0m Trial 29 finished with value: 0.7984453847359348 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.2}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7984 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 00:37:09,218]\u001b[0m Trial 30 finished with value: 0.7987400094029149 and parameters: {'num_filters': 192.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.1}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7987 with hyperparameters: {'num_filters': 192.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 00:50:10,958]\u001b[0m Trial 31 finished with value: 0.804851904090268 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.1}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8049 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 01:08:07,160]\u001b[0m Trial 32 finished with value: 0.804356683905344 and parameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.1}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8044 with hyperparameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 01:21:10,423]\u001b[0m Trial 33 finished with value: 0.8047829493809747 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.1}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8048 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 01:26:14,245]\u001b[0m Trial 34 finished with value: 0.8047829493809747 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 448.0, 'dropout_rate': 0.1}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8048 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 448.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 01:31:09,212]\u001b[0m Trial 35 finished with value: 0.800890142610876 and parameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 448.0, 'dropout_rate': 0.1}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8009 with hyperparameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 448.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 01:35:37,974]\u001b[0m Trial 36 finished with value: 0.7972794232878859 and parameters: {'num_filters': 128.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 512.0, 'dropout_rate': 0.1}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7973 with hyperparameters: {'num_filters': 128.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 512.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 01:38:58,428]\u001b[0m Trial 37 finished with value: 0.7984516533458705 and parameters: {'num_filters': 256.0, 'filter_size': 5.0, 'pool_size': 4.0, 'lstm_out': 448.0, 'dropout_rate': 0.1}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7985 with hyperparameters: {'num_filters': 256.0, 'filter_size': 5.0, 'pool_size': 4.0, 'lstm_out': 448.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 01:42:18,266]\u001b[0m Trial 38 finished with value: 0.7984704591756778 and parameters: {'num_filters': 160.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 256.0, 'dropout_rate': 0.1}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7985 with hyperparameters: {'num_filters': 160.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 256.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 01:45:15,344]\u001b[0m Trial 39 finished with value: 0.7946027268453221 and parameters: {'num_filters': 96.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 256.0, 'dropout_rate': 0.2}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7946 with hyperparameters: {'num_filters': 96.0, 'filter_size': 5.0, 'pool_size': 2.0, 'lstm_out': 256.0, 'dropout_rate': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 01:58:08,877]\u001b[0m Trial 40 finished with value: 0.8010217834195267 and parameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.1}. Best is trial 18 with value: 0.804851904090268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8010 with hyperparameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 02:16:18,953]\u001b[0m Trial 41 finished with value: 0.8054223475944209 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.1}. Best is trial 41 with value: 0.8054223475944209.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8054 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 02:34:27,285]\u001b[0m Trial 42 finished with value: 0.8024384892650055 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.1}. Best is trial 41 with value: 0.8054223475944209.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8024 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 384.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 02:39:30,911]\u001b[0m Trial 43 finished with value: 0.8040934022880426 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 448.0, 'dropout_rate': 0.1}. Best is trial 41 with value: 0.8054223475944209.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8041 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 448.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 02:52:33,778]\u001b[0m Trial 44 finished with value: 0.8049145901896254 and parameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.1}. Best is trial 41 with value: 0.8054223475944209.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8049 with hyperparameters: {'num_filters': 256.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 03:05:26,514]\u001b[0m Trial 45 finished with value: 0.8036734054223476 and parameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.2}. Best is trial 41 with value: 0.8054223475944209.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8037 with hyperparameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 03:08:54,534]\u001b[0m Trial 46 finished with value: 0.7989217990910515 and parameters: {'num_filters': 192.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 256.0, 'dropout_rate': 0.1}. Best is trial 41 with value: 0.8054223475944209.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7989 with hyperparameters: {'num_filters': 192.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 256.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 03:21:55,499]\u001b[0m Trial 47 finished with value: 0.7950289923209528 and parameters: {'num_filters': 256.0, 'filter_size': 7.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.1}. Best is trial 41 with value: 0.8054223475944209.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7950 with hyperparameters: {'num_filters': 256.0, 'filter_size': 7.0, 'pool_size': 2.0, 'lstm_out': 320.0, 'dropout_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 03:31:43,528]\u001b[0m Trial 48 finished with value: 0.7969471869612913 and parameters: {'num_filters': 224.0, 'filter_size': 5.0, 'pool_size': 4.0, 'lstm_out': 384.0, 'dropout_rate': 0.30000000000000004}. Best is trial 41 with value: 0.8054223475944209.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7969 with hyperparameters: {'num_filters': 224.0, 'filter_size': 5.0, 'pool_size': 4.0, 'lstm_out': 384.0, 'dropout_rate': 0.30000000000000004}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:81: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_4724\\3957003435.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[32m[I 2023-04-16 03:34:25,340]\u001b[0m Trial 49 finished with value: 0.8039742986992634 and parameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 192.0, 'dropout_rate': 0.2}. Best is trial 41 with value: 0.8054223475944209.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8040 with hyperparameters: {'num_filters': 224.0, 'filter_size': 3.0, 'pool_size': 2.0, 'lstm_out': 192.0, 'dropout_rate': 0.2}\n",
      "Best trial:\n",
      "  Value:  0.8054223475944209\n",
      "  Params: \n",
      "    num_filters: 256.0\n",
      "    filter_size: 3.0\n",
      "    pool_size: 2.0\n",
      "    lstm_out: 384.0\n",
      "    dropout_rate: 0.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import optuna\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "accumulation_steps = 4 \n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path, embedding_dim, word_index, max_features):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Load the GloVe embeddings matrix\n",
    "glove_file_path = 'glove.twitter.27B.200d.txt'  # Update the path to the downloaded GloVe file\n",
    "embedding_dim = 200\n",
    "embedding_matrix = load_glove_embeddings(glove_file_path, embedding_dim, tokenizer.word_index, max_features)\n",
    "\n",
    "class CNNTLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_dim=200, lstm_out=256, dropout_rate=0.2, num_classes=3, num_filters=64, filter_size=5, pool_size=2):\n",
    "        super(CNNTLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.conv1d = nn.Conv1d(embed_dim, num_filters, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool1d = nn.MaxPool1d(pool_size)\n",
    "        self.bi_lstm = nn.LSTM(num_filters, lstm_out // 2, batch_first=True, bidirectional=True, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(lstm_out, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.bi_lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_val_torch = torch.tensor(X_val, dtype=torch.long)\n",
    "y_val_torch = torch.tensor(y_val.values, dtype=torch.long)\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "y_train_np = y_train.values.reshape(-1, 1)\n",
    "y_val_np = y_val.values.reshape(-1, 1)  # Added this line\n",
    "y_test_np = y_test.values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "hyperparameters_accuracies = []\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be optimized\n",
    "    num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
    "    filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
    "    pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
    "    lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.4, step=0.1)\n",
    "\n",
    "    model = CNNTLSTM(embedding_matrix, embed_dim=embedding_dim, lstm_out=lstm_out, dropout_rate=dropout_rate, num_classes=3, num_filters=num_filters, filter_size=filter_size, pool_size=pool_size).cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Define DataLoader within the objective function\n",
    "    train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train the model with the given hyperparameters\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (texts, labels) in enumerate(train_loader):\n",
    "            texts, labels = texts.cuda(), labels.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels) / accumulation_steps\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_dataset = TensorDataset(X_val_torch, y_val_torch)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_val_predictions = []\n",
    "    all_val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            texts, labels = texts.cuda(), labels.cuda()\n",
    "            val_outputs = model(texts)\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            all_val_predictions.extend(predicted.cpu().numpy())\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate the validation accuracy\n",
    "    val_accuracy = accuracy_score(y_val_np, all_val_predictions)\n",
    "\n",
    "    # Print the accuracy for each set of hyperparameters\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f} with hyperparameters: {trial.params}\")\n",
    "\n",
    "    # Append the accuracy and hyperparameters to the list\n",
    "    hyperparameters_accuracies.append((val_accuracy, trial.params))\n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "    # Optimize using Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/5817], Loss: 0.2752, Accuracy: 0.3125\n",
      "Epoch [1/10], Step [20/5817], Loss: 0.2709, Accuracy: 0.3594\n",
      "Epoch [1/10], Step [30/5817], Loss: 0.2626, Accuracy: 0.3672\n",
      "Epoch [1/10], Step [40/5817], Loss: 0.2582, Accuracy: 0.4609\n",
      "Epoch [1/10], Step [50/5817], Loss: 0.2470, Accuracy: 0.6016\n",
      "Epoch [1/10], Step [60/5817], Loss: 0.2349, Accuracy: 0.6562\n",
      "Epoch [1/10], Step [70/5817], Loss: 0.2264, Accuracy: 0.6875\n",
      "Epoch [1/10], Step [80/5817], Loss: 0.2208, Accuracy: 0.6719\n",
      "Epoch [1/10], Step [90/5817], Loss: 0.2066, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [100/5817], Loss: 0.2090, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [110/5817], Loss: 0.2166, Accuracy: 0.6797\n",
      "Epoch [1/10], Step [120/5817], Loss: 0.2065, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [130/5817], Loss: 0.2097, Accuracy: 0.7031\n",
      "Epoch [1/10], Step [140/5817], Loss: 0.2158, Accuracy: 0.6719\n",
      "Epoch [1/10], Step [150/5817], Loss: 0.2131, Accuracy: 0.7031\n",
      "Epoch [1/10], Step [160/5817], Loss: 0.2158, Accuracy: 0.6797\n",
      "Epoch [1/10], Step [170/5817], Loss: 0.2168, Accuracy: 0.6797\n",
      "Epoch [1/10], Step [180/5817], Loss: 0.1878, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [190/5817], Loss: 0.2120, Accuracy: 0.6953\n",
      "Epoch [1/10], Step [200/5817], Loss: 0.2034, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [210/5817], Loss: 0.2000, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [220/5817], Loss: 0.1909, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [230/5817], Loss: 0.2153, Accuracy: 0.6953\n",
      "Epoch [1/10], Step [240/5817], Loss: 0.2106, Accuracy: 0.7031\n",
      "Epoch [1/10], Step [250/5817], Loss: 0.2096, Accuracy: 0.7031\n",
      "Epoch [1/10], Step [260/5817], Loss: 0.1984, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [270/5817], Loss: 0.1932, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [280/5817], Loss: 0.1963, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [290/5817], Loss: 0.1958, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [300/5817], Loss: 0.1815, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [310/5817], Loss: 0.1912, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [320/5817], Loss: 0.2030, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [330/5817], Loss: 0.1986, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [340/5817], Loss: 0.1996, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [350/5817], Loss: 0.2035, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [360/5817], Loss: 0.2078, Accuracy: 0.6953\n",
      "Epoch [1/10], Step [370/5817], Loss: 0.2006, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [380/5817], Loss: 0.2054, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [390/5817], Loss: 0.1979, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [400/5817], Loss: 0.1958, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [410/5817], Loss: 0.2173, Accuracy: 0.6406\n",
      "Epoch [1/10], Step [420/5817], Loss: 0.1947, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [430/5817], Loss: 0.1857, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [440/5817], Loss: 0.2061, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [450/5817], Loss: 0.1954, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [460/5817], Loss: 0.1836, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [470/5817], Loss: 0.2027, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [480/5817], Loss: 0.2019, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [490/5817], Loss: 0.2038, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [500/5817], Loss: 0.2020, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [510/5817], Loss: 0.2007, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [520/5817], Loss: 0.2000, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [530/5817], Loss: 0.1962, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [540/5817], Loss: 0.1951, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [550/5817], Loss: 0.2018, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [560/5817], Loss: 0.2039, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [570/5817], Loss: 0.1987, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [580/5817], Loss: 0.2015, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [590/5817], Loss: 0.1977, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [600/5817], Loss: 0.1819, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [610/5817], Loss: 0.1933, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [620/5817], Loss: 0.1933, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [630/5817], Loss: 0.2129, Accuracy: 0.6875\n",
      "Epoch [1/10], Step [640/5817], Loss: 0.1893, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [650/5817], Loss: 0.1834, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [660/5817], Loss: 0.1972, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [670/5817], Loss: 0.1961, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [680/5817], Loss: 0.1905, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [690/5817], Loss: 0.1994, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [700/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [710/5817], Loss: 0.1931, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [720/5817], Loss: 0.1961, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [730/5817], Loss: 0.1885, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [740/5817], Loss: 0.1900, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [750/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [760/5817], Loss: 0.1940, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [770/5817], Loss: 0.1943, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [780/5817], Loss: 0.1964, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [790/5817], Loss: 0.1927, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [800/5817], Loss: 0.1988, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [810/5817], Loss: 0.1995, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [820/5817], Loss: 0.1893, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [830/5817], Loss: 0.1838, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [840/5817], Loss: 0.2021, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [850/5817], Loss: 0.1848, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [860/5817], Loss: 0.1982, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [870/5817], Loss: 0.1974, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [880/5817], Loss: 0.1967, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [890/5817], Loss: 0.1921, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [900/5817], Loss: 0.2011, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [910/5817], Loss: 0.2011, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [920/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [930/5817], Loss: 0.1949, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [940/5817], Loss: 0.1995, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [950/5817], Loss: 0.1823, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [960/5817], Loss: 0.2164, Accuracy: 0.6562\n",
      "Epoch [1/10], Step [970/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [980/5817], Loss: 0.1976, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [990/5817], Loss: 0.1990, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [1000/5817], Loss: 0.1852, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [1010/5817], Loss: 0.1903, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1020/5817], Loss: 0.1939, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1030/5817], Loss: 0.1984, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [1040/5817], Loss: 0.1986, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [1050/5817], Loss: 0.2012, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [1060/5817], Loss: 0.1924, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1070/5817], Loss: 0.1969, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [1080/5817], Loss: 0.1944, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [1090/5817], Loss: 0.1954, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [1100/5817], Loss: 0.1913, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1110/5817], Loss: 0.1945, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [1120/5817], Loss: 0.1831, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [1130/5817], Loss: 0.1902, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1140/5817], Loss: 0.2101, Accuracy: 0.6875\n",
      "Epoch [1/10], Step [1150/5817], Loss: 0.2066, Accuracy: 0.7031\n",
      "Epoch [1/10], Step [1160/5817], Loss: 0.1918, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1170/5817], Loss: 0.1923, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1180/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [1190/5817], Loss: 0.1906, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1200/5817], Loss: 0.1925, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [1210/5817], Loss: 0.2032, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [1220/5817], Loss: 0.2016, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [1230/5817], Loss: 0.2093, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [1240/5817], Loss: 0.1968, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1250/5817], Loss: 0.1868, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [1260/5817], Loss: 0.1960, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [1270/5817], Loss: 0.1944, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1280/5817], Loss: 0.1842, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [1290/5817], Loss: 0.2066, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [1300/5817], Loss: 0.1930, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1310/5817], Loss: 0.1932, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1320/5817], Loss: 0.2071, Accuracy: 0.7031\n",
      "Epoch [1/10], Step [1330/5817], Loss: 0.2041, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [1340/5817], Loss: 0.1939, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1350/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [1360/5817], Loss: 0.2032, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [1370/5817], Loss: 0.1891, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1380/5817], Loss: 0.1914, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1390/5817], Loss: 0.1781, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [1400/5817], Loss: 0.2022, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [1410/5817], Loss: 0.1979, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [1420/5817], Loss: 0.1949, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1430/5817], Loss: 0.1778, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [1440/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1450/5817], Loss: 0.1988, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [1460/5817], Loss: 0.1946, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [1470/5817], Loss: 0.1806, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [1480/5817], Loss: 0.1988, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1490/5817], Loss: 0.2023, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [1500/5817], Loss: 0.1861, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [1510/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [1520/5817], Loss: 0.1822, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [1530/5817], Loss: 0.1954, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [1540/5817], Loss: 0.1918, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1550/5817], Loss: 0.2004, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [1560/5817], Loss: 0.1915, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1570/5817], Loss: 0.1929, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1580/5817], Loss: 0.1913, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1590/5817], Loss: 0.1930, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1600/5817], Loss: 0.2017, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [1610/5817], Loss: 0.1968, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [1620/5817], Loss: 0.1926, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1630/5817], Loss: 0.1965, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [1640/5817], Loss: 0.1913, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1650/5817], Loss: 0.1845, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1660/5817], Loss: 0.1747, Accuracy: 0.8672\n",
      "Epoch [1/10], Step [1670/5817], Loss: 0.2050, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [1680/5817], Loss: 0.2031, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [1690/5817], Loss: 0.2022, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [1700/5817], Loss: 0.1979, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [1710/5817], Loss: 0.1893, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1720/5817], Loss: 0.1976, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1730/5817], Loss: 0.1999, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [1740/5817], Loss: 0.1946, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1750/5817], Loss: 0.1884, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1760/5817], Loss: 0.1975, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1770/5817], Loss: 0.1889, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [1780/5817], Loss: 0.2028, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [1790/5817], Loss: 0.1865, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [1800/5817], Loss: 0.1890, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1810/5817], Loss: 0.1884, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1820/5817], Loss: 0.1832, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [1830/5817], Loss: 0.1922, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1840/5817], Loss: 0.1904, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1850/5817], Loss: 0.1938, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1860/5817], Loss: 0.2077, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [1870/5817], Loss: 0.1933, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1880/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [1890/5817], Loss: 0.1859, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [1900/5817], Loss: 0.2051, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [1910/5817], Loss: 0.1875, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [1920/5817], Loss: 0.1907, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1930/5817], Loss: 0.1936, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1940/5817], Loss: 0.1988, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [1950/5817], Loss: 0.1824, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [1960/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [1970/5817], Loss: 0.1930, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [1980/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [1990/5817], Loss: 0.1927, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [2000/5817], Loss: 0.1907, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [2010/5817], Loss: 0.1830, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2020/5817], Loss: 0.2040, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [2030/5817], Loss: 0.1884, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [2040/5817], Loss: 0.2038, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [2050/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [2060/5817], Loss: 0.1860, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2070/5817], Loss: 0.2010, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [2080/5817], Loss: 0.1943, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [2090/5817], Loss: 0.1989, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [2100/5817], Loss: 0.1979, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2110/5817], Loss: 0.1898, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2120/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2130/5817], Loss: 0.1997, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [2140/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2150/5817], Loss: 0.2024, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [2160/5817], Loss: 0.1922, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2170/5817], Loss: 0.1843, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2180/5817], Loss: 0.1999, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [2190/5817], Loss: 0.1879, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2200/5817], Loss: 0.1943, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [2210/5817], Loss: 0.1901, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2220/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [2230/5817], Loss: 0.1945, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2240/5817], Loss: 0.1929, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [2250/5817], Loss: 0.2075, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [2260/5817], Loss: 0.1804, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [2270/5817], Loss: 0.1878, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2280/5817], Loss: 0.1882, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2290/5817], Loss: 0.1953, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2300/5817], Loss: 0.1776, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [2310/5817], Loss: 0.2031, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [2320/5817], Loss: 0.1851, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [2330/5817], Loss: 0.1887, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2340/5817], Loss: 0.1819, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [2350/5817], Loss: 0.1903, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2360/5817], Loss: 0.1881, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [2370/5817], Loss: 0.1938, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2380/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [2390/5817], Loss: 0.1962, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2400/5817], Loss: 0.1821, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2410/5817], Loss: 0.1882, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2420/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [2430/5817], Loss: 0.1942, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [2440/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [2450/5817], Loss: 0.2066, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [2460/5817], Loss: 0.1927, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [2470/5817], Loss: 0.2048, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [2480/5817], Loss: 0.2029, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [2490/5817], Loss: 0.1909, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2500/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [2510/5817], Loss: 0.1913, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [2520/5817], Loss: 0.1965, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [2530/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2540/5817], Loss: 0.1907, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2550/5817], Loss: 0.1903, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [2560/5817], Loss: 0.1850, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2570/5817], Loss: 0.1892, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [2580/5817], Loss: 0.1807, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [2590/5817], Loss: 0.2084, Accuracy: 0.6953\n",
      "Epoch [1/10], Step [2600/5817], Loss: 0.2009, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [2610/5817], Loss: 0.1903, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [2620/5817], Loss: 0.1853, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2630/5817], Loss: 0.1981, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [2640/5817], Loss: 0.1844, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [2650/5817], Loss: 0.1851, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2660/5817], Loss: 0.1896, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [2670/5817], Loss: 0.2047, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [2680/5817], Loss: 0.1951, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2690/5817], Loss: 0.1911, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [2700/5817], Loss: 0.1931, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2710/5817], Loss: 0.2029, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [2720/5817], Loss: 0.1971, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [2730/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [2740/5817], Loss: 0.1843, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2750/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [2760/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [2770/5817], Loss: 0.1838, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2780/5817], Loss: 0.1994, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [2790/5817], Loss: 0.1943, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [2800/5817], Loss: 0.2026, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [2810/5817], Loss: 0.1957, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2820/5817], Loss: 0.1991, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [2830/5817], Loss: 0.1923, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2840/5817], Loss: 0.1880, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2850/5817], Loss: 0.1991, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [2860/5817], Loss: 0.1856, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [2870/5817], Loss: 0.1932, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2880/5817], Loss: 0.1892, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2890/5817], Loss: 0.1973, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2900/5817], Loss: 0.1959, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [2910/5817], Loss: 0.1948, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2920/5817], Loss: 0.1774, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [2930/5817], Loss: 0.2019, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [2940/5817], Loss: 0.1961, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2950/5817], Loss: 0.1862, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2960/5817], Loss: 0.1947, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2970/5817], Loss: 0.1978, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [2980/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2990/5817], Loss: 0.1850, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3000/5817], Loss: 0.1883, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3010/5817], Loss: 0.1945, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3020/5817], Loss: 0.2005, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [3030/5817], Loss: 0.2019, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [3040/5817], Loss: 0.1969, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [3050/5817], Loss: 0.1841, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [3060/5817], Loss: 0.1908, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3070/5817], Loss: 0.1938, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3080/5817], Loss: 0.1934, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3090/5817], Loss: 0.1991, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [3100/5817], Loss: 0.1927, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3110/5817], Loss: 0.1851, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3120/5817], Loss: 0.1832, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [3130/5817], Loss: 0.1919, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3140/5817], Loss: 0.1956, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3150/5817], Loss: 0.1856, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [3160/5817], Loss: 0.1949, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3170/5817], Loss: 0.1896, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3180/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3190/5817], Loss: 0.1861, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3200/5817], Loss: 0.1974, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [3210/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [3220/5817], Loss: 0.1864, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3230/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [3240/5817], Loss: 0.1879, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3250/5817], Loss: 0.1937, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3260/5817], Loss: 0.1814, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [3270/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [3280/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [3290/5817], Loss: 0.1887, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [3300/5817], Loss: 0.2113, Accuracy: 0.7031\n",
      "Epoch [1/10], Step [3310/5817], Loss: 0.1954, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3320/5817], Loss: 0.1997, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [3330/5817], Loss: 0.1833, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [3340/5817], Loss: 0.2003, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3350/5817], Loss: 0.1970, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [3360/5817], Loss: 0.1943, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3370/5817], Loss: 0.1707, Accuracy: 0.8750\n",
      "Epoch [1/10], Step [3380/5817], Loss: 0.1874, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3390/5817], Loss: 0.1873, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3400/5817], Loss: 0.1939, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3410/5817], Loss: 0.1819, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [3420/5817], Loss: 0.1950, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3430/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [3440/5817], Loss: 0.1889, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [3450/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [3460/5817], Loss: 0.2007, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [3470/5817], Loss: 0.2007, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [3480/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [3490/5817], Loss: 0.1906, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3500/5817], Loss: 0.2027, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [3510/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [3520/5817], Loss: 0.1938, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3530/5817], Loss: 0.1915, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3540/5817], Loss: 0.1938, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3550/5817], Loss: 0.1819, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [3560/5817], Loss: 0.1979, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [3570/5817], Loss: 0.1871, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3580/5817], Loss: 0.1882, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3590/5817], Loss: 0.1868, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3600/5817], Loss: 0.2068, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [3610/5817], Loss: 0.1955, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [3620/5817], Loss: 0.2002, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [3630/5817], Loss: 0.2036, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [3640/5817], Loss: 0.1955, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3650/5817], Loss: 0.1844, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [3660/5817], Loss: 0.1823, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [3670/5817], Loss: 0.1889, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3680/5817], Loss: 0.1943, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3690/5817], Loss: 0.1887, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3700/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3710/5817], Loss: 0.2060, Accuracy: 0.7031\n",
      "Epoch [1/10], Step [3720/5817], Loss: 0.2052, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [3730/5817], Loss: 0.1973, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [3740/5817], Loss: 0.1866, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [3750/5817], Loss: 0.1912, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3760/5817], Loss: 0.1879, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [3770/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3780/5817], Loss: 0.1936, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3790/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [3800/5817], Loss: 0.2016, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [3810/5817], Loss: 0.1928, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3820/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3830/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3840/5817], Loss: 0.1856, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3850/5817], Loss: 0.1883, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3860/5817], Loss: 0.2047, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [3870/5817], Loss: 0.1865, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3880/5817], Loss: 0.1896, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3890/5817], Loss: 0.1985, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [3900/5817], Loss: 0.2000, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [3910/5817], Loss: 0.1895, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3920/5817], Loss: 0.1931, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3930/5817], Loss: 0.1915, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3940/5817], Loss: 0.1847, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [3950/5817], Loss: 0.1790, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [3960/5817], Loss: 0.1959, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [3970/5817], Loss: 0.1899, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3980/5817], Loss: 0.1875, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [3990/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [4000/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [1/10], Step [4010/5817], Loss: 0.1865, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4020/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4030/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4040/5817], Loss: 0.2073, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [4050/5817], Loss: 0.1864, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4060/5817], Loss: 0.1911, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4070/5817], Loss: 0.1810, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [4080/5817], Loss: 0.1864, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4090/5817], Loss: 0.1874, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4100/5817], Loss: 0.1918, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [4110/5817], Loss: 0.1939, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [4120/5817], Loss: 0.1979, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [4130/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4140/5817], Loss: 0.1989, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [4150/5817], Loss: 0.1861, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4160/5817], Loss: 0.2090, Accuracy: 0.7031\n",
      "Epoch [1/10], Step [4170/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [4180/5817], Loss: 0.2010, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [4190/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [4200/5817], Loss: 0.2015, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [4210/5817], Loss: 0.1895, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4220/5817], Loss: 0.1902, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [4230/5817], Loss: 0.2069, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [4240/5817], Loss: 0.1938, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [4250/5817], Loss: 0.1956, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [4260/5817], Loss: 0.1875, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4270/5817], Loss: 0.1952, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [4280/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4290/5817], Loss: 0.1864, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4300/5817], Loss: 0.1935, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4310/5817], Loss: 0.1844, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4320/5817], Loss: 0.1986, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [4330/5817], Loss: 0.1829, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [4340/5817], Loss: 0.2068, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [4350/5817], Loss: 0.1918, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4360/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4370/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [4380/5817], Loss: 0.1980, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [4390/5817], Loss: 0.1936, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4400/5817], Loss: 0.1957, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [4410/5817], Loss: 0.2014, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [4420/5817], Loss: 0.1762, Accuracy: 0.8594\n",
      "Epoch [1/10], Step [4430/5817], Loss: 0.1915, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [4440/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4450/5817], Loss: 0.1827, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4460/5817], Loss: 0.1928, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4470/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4480/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4490/5817], Loss: 0.1991, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [4500/5817], Loss: 0.1925, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4510/5817], Loss: 0.1848, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4520/5817], Loss: 0.1863, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4530/5817], Loss: 0.1897, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4540/5817], Loss: 0.1895, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [4550/5817], Loss: 0.2025, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [4560/5817], Loss: 0.1717, Accuracy: 0.8672\n",
      "Epoch [1/10], Step [4570/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4580/5817], Loss: 0.1832, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [4590/5817], Loss: 0.1917, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [4600/5817], Loss: 0.1834, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [4610/5817], Loss: 0.1957, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [4620/5817], Loss: 0.1798, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [4630/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4640/5817], Loss: 0.1898, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4650/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4660/5817], Loss: 0.2002, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [4670/5817], Loss: 0.1990, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [4680/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [4690/5817], Loss: 0.1910, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4700/5817], Loss: 0.1886, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4710/5817], Loss: 0.1996, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [4720/5817], Loss: 0.1809, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4730/5817], Loss: 0.1830, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4740/5817], Loss: 0.1864, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4750/5817], Loss: 0.2034, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [4760/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [4770/5817], Loss: 0.1921, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4780/5817], Loss: 0.1893, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4790/5817], Loss: 0.1915, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [4800/5817], Loss: 0.1789, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [4810/5817], Loss: 0.1844, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4820/5817], Loss: 0.1951, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [4830/5817], Loss: 0.1851, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4840/5817], Loss: 0.1965, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [4850/5817], Loss: 0.1824, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4860/5817], Loss: 0.1746, Accuracy: 0.8594\n",
      "Epoch [1/10], Step [4870/5817], Loss: 0.2023, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [4880/5817], Loss: 0.1972, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [4890/5817], Loss: 0.1881, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4900/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4910/5817], Loss: 0.2037, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [4920/5817], Loss: 0.1890, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4930/5817], Loss: 0.1956, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [4940/5817], Loss: 0.1898, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4950/5817], Loss: 0.1810, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [4960/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4970/5817], Loss: 0.1864, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4980/5817], Loss: 0.1940, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [4990/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5000/5817], Loss: 0.1982, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [5010/5817], Loss: 0.1961, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [5020/5817], Loss: 0.1762, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5030/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [5040/5817], Loss: 0.1930, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5050/5817], Loss: 0.1896, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [5060/5817], Loss: 0.1948, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [5070/5817], Loss: 0.1955, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [5080/5817], Loss: 0.2057, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [5090/5817], Loss: 0.1989, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [5100/5817], Loss: 0.1921, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5110/5817], Loss: 0.1986, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [5120/5817], Loss: 0.2037, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [5130/5817], Loss: 0.1963, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [5140/5817], Loss: 0.1888, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5150/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [5160/5817], Loss: 0.1905, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [5170/5817], Loss: 0.1879, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [5180/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [5190/5817], Loss: 0.1970, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [5200/5817], Loss: 0.1797, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [5210/5817], Loss: 0.1908, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5220/5817], Loss: 0.1903, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [5230/5817], Loss: 0.1839, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [5240/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [5250/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [5260/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [5270/5817], Loss: 0.1922, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [5280/5817], Loss: 0.1918, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [5290/5817], Loss: 0.2018, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [5300/5817], Loss: 0.1889, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [5310/5817], Loss: 0.1903, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [5320/5817], Loss: 0.1942, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [5330/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [5340/5817], Loss: 0.1810, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5350/5817], Loss: 0.1929, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [5360/5817], Loss: 0.2062, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [5370/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [5380/5817], Loss: 0.1959, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [5390/5817], Loss: 0.1955, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [5400/5817], Loss: 0.1909, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5410/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [5420/5817], Loss: 0.1980, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [5430/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [5440/5817], Loss: 0.1951, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [5450/5817], Loss: 0.2059, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [5460/5817], Loss: 0.1859, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [5470/5817], Loss: 0.2027, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [5480/5817], Loss: 0.2019, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [5490/5817], Loss: 0.1902, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [5500/5817], Loss: 0.1921, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [5510/5817], Loss: 0.1816, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [5520/5817], Loss: 0.1792, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [5530/5817], Loss: 0.1916, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [5540/5817], Loss: 0.1803, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [5550/5817], Loss: 0.1854, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [5560/5817], Loss: 0.1896, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5570/5817], Loss: 0.1991, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [5580/5817], Loss: 0.1873, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5590/5817], Loss: 0.1916, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [5600/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [5610/5817], Loss: 0.1840, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5620/5817], Loss: 0.1888, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5630/5817], Loss: 0.1873, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [5640/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [5650/5817], Loss: 0.1956, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [5660/5817], Loss: 0.2038, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [5670/5817], Loss: 0.1944, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [5680/5817], Loss: 0.1950, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [5690/5817], Loss: 0.1921, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5700/5817], Loss: 0.1912, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [5710/5817], Loss: 0.1885, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [5720/5817], Loss: 0.1889, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [5730/5817], Loss: 0.1923, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [5740/5817], Loss: 0.1949, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5750/5817], Loss: 0.1990, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [5760/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5770/5817], Loss: 0.1886, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [5780/5817], Loss: 0.1926, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [5790/5817], Loss: 0.1954, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [5800/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [5810/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [10/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [20/5817], Loss: 0.1868, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [30/5817], Loss: 0.1834, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [40/5817], Loss: 0.1801, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [50/5817], Loss: 0.1905, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [60/5817], Loss: 0.1861, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [70/5817], Loss: 0.1789, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [80/5817], Loss: 0.1872, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [90/5817], Loss: 0.1864, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [100/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [110/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [120/5817], Loss: 0.1918, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [130/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [140/5817], Loss: 0.1758, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [150/5817], Loss: 0.1991, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [160/5817], Loss: 0.1879, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [170/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [180/5817], Loss: 0.1951, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [190/5817], Loss: 0.1812, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [200/5817], Loss: 0.1921, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [210/5817], Loss: 0.1857, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [220/5817], Loss: 0.1834, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [230/5817], Loss: 0.1968, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [240/5817], Loss: 0.1909, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [250/5817], Loss: 0.1906, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [260/5817], Loss: 0.1962, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [270/5817], Loss: 0.1975, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [280/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [290/5817], Loss: 0.2045, Accuracy: 0.7266\n",
      "Epoch [2/10], Step [300/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [310/5817], Loss: 0.1851, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [320/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [330/5817], Loss: 0.1878, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [340/5817], Loss: 0.1873, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [350/5817], Loss: 0.1933, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [360/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [370/5817], Loss: 0.1990, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [380/5817], Loss: 0.1920, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [390/5817], Loss: 0.2028, Accuracy: 0.7266\n",
      "Epoch [2/10], Step [400/5817], Loss: 0.2144, Accuracy: 0.6797\n",
      "Epoch [2/10], Step [410/5817], Loss: 0.1739, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [420/5817], Loss: 0.1924, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [430/5817], Loss: 0.1972, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [440/5817], Loss: 0.2071, Accuracy: 0.7188\n",
      "Epoch [2/10], Step [450/5817], Loss: 0.1987, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [460/5817], Loss: 0.1856, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [470/5817], Loss: 0.1776, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [480/5817], Loss: 0.1891, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [490/5817], Loss: 0.1822, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [500/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [510/5817], Loss: 0.1972, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [520/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [530/5817], Loss: 0.1809, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [540/5817], Loss: 0.1863, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [550/5817], Loss: 0.1823, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [560/5817], Loss: 0.1923, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [570/5817], Loss: 0.1836, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [580/5817], Loss: 0.1801, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [590/5817], Loss: 0.1757, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [600/5817], Loss: 0.1898, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [610/5817], Loss: 0.2052, Accuracy: 0.7031\n",
      "Epoch [2/10], Step [620/5817], Loss: 0.1970, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [630/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [640/5817], Loss: 0.1946, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [650/5817], Loss: 0.1920, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [660/5817], Loss: 0.1901, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [670/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [680/5817], Loss: 0.1912, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [690/5817], Loss: 0.1871, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [700/5817], Loss: 0.2019, Accuracy: 0.7344\n",
      "Epoch [2/10], Step [710/5817], Loss: 0.1911, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [720/5817], Loss: 0.1885, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [730/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [740/5817], Loss: 0.1791, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [750/5817], Loss: 0.1999, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [760/5817], Loss: 0.1897, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [770/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [780/5817], Loss: 0.1924, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [790/5817], Loss: 0.1878, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [800/5817], Loss: 0.1937, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [810/5817], Loss: 0.1940, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [820/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [830/5817], Loss: 0.1911, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [840/5817], Loss: 0.1897, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [850/5817], Loss: 0.1848, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [860/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [870/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [880/5817], Loss: 0.1856, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [890/5817], Loss: 0.1954, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [900/5817], Loss: 0.1896, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [910/5817], Loss: 0.1879, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [920/5817], Loss: 0.1928, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [930/5817], Loss: 0.1688, Accuracy: 0.8906\n",
      "Epoch [2/10], Step [940/5817], Loss: 0.1910, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [950/5817], Loss: 0.1934, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [960/5817], Loss: 0.1881, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [970/5817], Loss: 0.1964, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [980/5817], Loss: 0.1915, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [990/5817], Loss: 0.1820, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [1000/5817], Loss: 0.1770, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [1010/5817], Loss: 0.1858, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1020/5817], Loss: 0.1890, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [1030/5817], Loss: 0.1906, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1040/5817], Loss: 0.1981, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [1050/5817], Loss: 0.1950, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [1060/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1070/5817], Loss: 0.1965, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [1080/5817], Loss: 0.1866, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [1090/5817], Loss: 0.1739, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [1100/5817], Loss: 0.1950, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [1110/5817], Loss: 0.1846, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1120/5817], Loss: 0.1735, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [1130/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1140/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1150/5817], Loss: 0.1859, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1160/5817], Loss: 0.1892, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1170/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [1180/5817], Loss: 0.1929, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [1190/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1200/5817], Loss: 0.1936, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [1210/5817], Loss: 0.1873, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [1220/5817], Loss: 0.1899, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [1230/5817], Loss: 0.1896, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [1240/5817], Loss: 0.1847, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1250/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [1260/5817], Loss: 0.2105, Accuracy: 0.7109\n",
      "Epoch [2/10], Step [1270/5817], Loss: 0.1949, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [1280/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [1290/5817], Loss: 0.1959, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [1300/5817], Loss: 0.1927, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [1310/5817], Loss: 0.1895, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [1320/5817], Loss: 0.1965, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [1330/5817], Loss: 0.1833, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1340/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1350/5817], Loss: 0.1851, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1360/5817], Loss: 0.1782, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [1370/5817], Loss: 0.2051, Accuracy: 0.7266\n",
      "Epoch [2/10], Step [1380/5817], Loss: 0.1859, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1390/5817], Loss: 0.1761, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1400/5817], Loss: 0.1965, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [1410/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [1420/5817], Loss: 0.1922, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [1430/5817], Loss: 0.1829, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1440/5817], Loss: 0.1827, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [1450/5817], Loss: 0.1864, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1460/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1470/5817], Loss: 0.2014, Accuracy: 0.7266\n",
      "Epoch [2/10], Step [1480/5817], Loss: 0.1897, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [1490/5817], Loss: 0.1895, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [1500/5817], Loss: 0.1889, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [1510/5817], Loss: 0.1992, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [1520/5817], Loss: 0.1910, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [1530/5817], Loss: 0.1917, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [1540/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [1550/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1560/5817], Loss: 0.2011, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [1570/5817], Loss: 0.1917, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [1580/5817], Loss: 0.1910, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [1590/5817], Loss: 0.1964, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [1600/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [1610/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [1620/5817], Loss: 0.1890, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [1630/5817], Loss: 0.1837, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1640/5817], Loss: 0.1835, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1650/5817], Loss: 0.1934, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [1660/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1670/5817], Loss: 0.1856, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1680/5817], Loss: 0.1841, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1690/5817], Loss: 0.1784, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1700/5817], Loss: 0.1841, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1710/5817], Loss: 0.1821, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [1720/5817], Loss: 0.2100, Accuracy: 0.7031\n",
      "Epoch [2/10], Step [1730/5817], Loss: 0.1911, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [1740/5817], Loss: 0.2005, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [1750/5817], Loss: 0.1876, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1760/5817], Loss: 0.1868, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1770/5817], Loss: 0.1740, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [1780/5817], Loss: 0.2015, Accuracy: 0.7188\n",
      "Epoch [2/10], Step [1790/5817], Loss: 0.1937, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [1800/5817], Loss: 0.1901, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [1810/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1820/5817], Loss: 0.1898, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [1830/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [1840/5817], Loss: 0.1910, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [1850/5817], Loss: 0.1829, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1860/5817], Loss: 0.1866, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1870/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1880/5817], Loss: 0.1740, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [1890/5817], Loss: 0.1887, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [1900/5817], Loss: 0.1903, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [1910/5817], Loss: 0.1843, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1920/5817], Loss: 0.1940, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [1930/5817], Loss: 0.1807, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1940/5817], Loss: 0.1902, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [1950/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [1960/5817], Loss: 0.2004, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [1970/5817], Loss: 0.1759, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [1980/5817], Loss: 0.1881, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [1990/5817], Loss: 0.1906, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2000/5817], Loss: 0.2030, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [2010/5817], Loss: 0.1885, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2020/5817], Loss: 0.2031, Accuracy: 0.7266\n",
      "Epoch [2/10], Step [2030/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [2040/5817], Loss: 0.1873, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2050/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [2060/5817], Loss: 0.1838, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [2070/5817], Loss: 0.1871, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [2080/5817], Loss: 0.1971, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [2090/5817], Loss: 0.2093, Accuracy: 0.6875\n",
      "Epoch [2/10], Step [2100/5817], Loss: 0.1818, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2110/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2120/5817], Loss: 0.1730, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2130/5817], Loss: 0.1938, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [2140/5817], Loss: 0.1790, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2150/5817], Loss: 0.1849, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2160/5817], Loss: 0.1772, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [2170/5817], Loss: 0.1747, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [2180/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2190/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2200/5817], Loss: 0.1884, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2210/5817], Loss: 0.1958, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [2220/5817], Loss: 0.1911, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [2230/5817], Loss: 0.1928, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [2240/5817], Loss: 0.1844, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2250/5817], Loss: 0.1838, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2260/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [2270/5817], Loss: 0.1908, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2280/5817], Loss: 0.1782, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [2290/5817], Loss: 0.1866, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [2300/5817], Loss: 0.1922, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [2310/5817], Loss: 0.2009, Accuracy: 0.7266\n",
      "Epoch [2/10], Step [2320/5817], Loss: 0.1971, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [2330/5817], Loss: 0.1853, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2340/5817], Loss: 0.1938, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [2350/5817], Loss: 0.1882, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2360/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2370/5817], Loss: 0.2031, Accuracy: 0.7344\n",
      "Epoch [2/10], Step [2380/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [2390/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [2400/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2410/5817], Loss: 0.1842, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [2420/5817], Loss: 0.1952, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [2430/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [2440/5817], Loss: 0.1894, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [2450/5817], Loss: 0.1915, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [2460/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2470/5817], Loss: 0.1904, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2480/5817], Loss: 0.2037, Accuracy: 0.7344\n",
      "Epoch [2/10], Step [2490/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2500/5817], Loss: 0.1936, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [2510/5817], Loss: 0.1862, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2520/5817], Loss: 0.1950, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [2530/5817], Loss: 0.1966, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [2540/5817], Loss: 0.1944, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [2550/5817], Loss: 0.1921, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [2560/5817], Loss: 0.1918, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2570/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2580/5817], Loss: 0.1996, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [2590/5817], Loss: 0.1868, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [2600/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2610/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [2620/5817], Loss: 0.1805, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2630/5817], Loss: 0.1901, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [2640/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2650/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2660/5817], Loss: 0.1874, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2670/5817], Loss: 0.1849, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2680/5817], Loss: 0.1988, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [2690/5817], Loss: 0.1823, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2700/5817], Loss: 0.1809, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2710/5817], Loss: 0.1844, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2720/5817], Loss: 0.1904, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2730/5817], Loss: 0.1844, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2740/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2750/5817], Loss: 0.1847, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2760/5817], Loss: 0.1953, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [2770/5817], Loss: 0.1899, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2780/5817], Loss: 0.1867, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2790/5817], Loss: 0.1904, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [2800/5817], Loss: 0.1873, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [2810/5817], Loss: 0.1962, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [2820/5817], Loss: 0.1841, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2830/5817], Loss: 0.1872, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [2840/5817], Loss: 0.1840, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2850/5817], Loss: 0.1778, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2860/5817], Loss: 0.1835, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2870/5817], Loss: 0.1890, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2880/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2890/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2900/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [2910/5817], Loss: 0.1965, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [2920/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2930/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [2940/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2950/5817], Loss: 0.1954, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [2960/5817], Loss: 0.1921, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [2970/5817], Loss: 0.1925, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [2980/5817], Loss: 0.1868, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2990/5817], Loss: 0.2037, Accuracy: 0.7344\n",
      "Epoch [2/10], Step [3000/5817], Loss: 0.1752, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [3010/5817], Loss: 0.1814, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [3020/5817], Loss: 0.1836, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3030/5817], Loss: 0.1821, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [3040/5817], Loss: 0.1890, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [3050/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [3060/5817], Loss: 0.1942, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [3070/5817], Loss: 0.1904, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3080/5817], Loss: 0.1889, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3090/5817], Loss: 0.1955, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [3100/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [3110/5817], Loss: 0.2005, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [3120/5817], Loss: 0.2035, Accuracy: 0.7266\n",
      "Epoch [2/10], Step [3130/5817], Loss: 0.1944, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [3140/5817], Loss: 0.1849, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [3150/5817], Loss: 0.1856, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3160/5817], Loss: 0.1917, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [3170/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [3180/5817], Loss: 0.1982, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [3190/5817], Loss: 0.1871, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [3200/5817], Loss: 0.1833, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [3210/5817], Loss: 0.1948, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [3220/5817], Loss: 0.1749, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [3230/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [3240/5817], Loss: 0.1916, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3250/5817], Loss: 0.1899, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [3260/5817], Loss: 0.1853, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3270/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3280/5817], Loss: 0.1964, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [3290/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3300/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [3310/5817], Loss: 0.1918, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [3320/5817], Loss: 0.1964, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [3330/5817], Loss: 0.1945, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [3340/5817], Loss: 0.1897, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3350/5817], Loss: 0.1921, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [3360/5817], Loss: 0.1916, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [3370/5817], Loss: 0.1817, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [3380/5817], Loss: 0.1853, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3390/5817], Loss: 0.1849, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [3400/5817], Loss: 0.1823, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3410/5817], Loss: 0.1854, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3420/5817], Loss: 0.1878, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3430/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [3440/5817], Loss: 0.1908, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [3450/5817], Loss: 0.1830, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3460/5817], Loss: 0.2043, Accuracy: 0.7344\n",
      "Epoch [2/10], Step [3470/5817], Loss: 0.1948, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [3480/5817], Loss: 0.1943, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [3490/5817], Loss: 0.1843, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [3500/5817], Loss: 0.1880, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [3510/5817], Loss: 0.1844, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3520/5817], Loss: 0.1988, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [3530/5817], Loss: 0.1821, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3540/5817], Loss: 0.1997, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [3550/5817], Loss: 0.1920, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [3560/5817], Loss: 0.1869, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3570/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [3580/5817], Loss: 0.2012, Accuracy: 0.7188\n",
      "Epoch [2/10], Step [3590/5817], Loss: 0.1884, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [3600/5817], Loss: 0.1899, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [3610/5817], Loss: 0.1901, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3620/5817], Loss: 0.1934, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [3630/5817], Loss: 0.1898, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [3640/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3650/5817], Loss: 0.1856, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3660/5817], Loss: 0.2030, Accuracy: 0.7344\n",
      "Epoch [2/10], Step [3670/5817], Loss: 0.1865, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3680/5817], Loss: 0.1877, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3690/5817], Loss: 0.1989, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [3700/5817], Loss: 0.1838, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3710/5817], Loss: 0.1913, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [3720/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [3730/5817], Loss: 0.2103, Accuracy: 0.7031\n",
      "Epoch [2/10], Step [3740/5817], Loss: 0.1843, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3750/5817], Loss: 0.1800, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [3760/5817], Loss: 0.1900, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [3770/5817], Loss: 0.1961, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [3780/5817], Loss: 0.1992, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [3790/5817], Loss: 0.1945, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [3800/5817], Loss: 0.1873, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3810/5817], Loss: 0.1843, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [3820/5817], Loss: 0.1948, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [3830/5817], Loss: 0.1999, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [3840/5817], Loss: 0.1886, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3850/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [3860/5817], Loss: 0.1694, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [3870/5817], Loss: 0.1946, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [3880/5817], Loss: 0.2102, Accuracy: 0.7031\n",
      "Epoch [2/10], Step [3890/5817], Loss: 0.1907, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3900/5817], Loss: 0.2104, Accuracy: 0.6953\n",
      "Epoch [2/10], Step [3910/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [3920/5817], Loss: 0.1889, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3930/5817], Loss: 0.1975, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [3940/5817], Loss: 0.1961, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [3950/5817], Loss: 0.2117, Accuracy: 0.6953\n",
      "Epoch [2/10], Step [3960/5817], Loss: 0.2002, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [3970/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [3980/5817], Loss: 0.1921, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3990/5817], Loss: 0.2039, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [4000/5817], Loss: 0.1844, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4010/5817], Loss: 0.1842, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [4020/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4030/5817], Loss: 0.1908, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [4040/5817], Loss: 0.1855, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4050/5817], Loss: 0.1882, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4060/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [4070/5817], Loss: 0.1847, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4080/5817], Loss: 0.1661, Accuracy: 0.8906\n",
      "Epoch [2/10], Step [4090/5817], Loss: 0.1746, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [4100/5817], Loss: 0.1880, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4110/5817], Loss: 0.1895, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [4120/5817], Loss: 0.1721, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [4130/5817], Loss: 0.1927, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [4140/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [4150/5817], Loss: 0.1916, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [4160/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [4170/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [4180/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4190/5817], Loss: 0.1942, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [4200/5817], Loss: 0.1953, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [4210/5817], Loss: 0.1810, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4220/5817], Loss: 0.1701, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [4230/5817], Loss: 0.1808, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [4240/5817], Loss: 0.2005, Accuracy: 0.7344\n",
      "Epoch [2/10], Step [4250/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [4260/5817], Loss: 0.1946, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [4270/5817], Loss: 0.1884, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4280/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4290/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4300/5817], Loss: 0.1735, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [4310/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [4320/5817], Loss: 0.1865, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [4330/5817], Loss: 0.1929, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [4340/5817], Loss: 0.1997, Accuracy: 0.7344\n",
      "Epoch [2/10], Step [4350/5817], Loss: 0.1871, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4360/5817], Loss: 0.1892, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4370/5817], Loss: 0.1874, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4380/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4390/5817], Loss: 0.1892, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4400/5817], Loss: 0.1868, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4410/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4420/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [4430/5817], Loss: 0.1855, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [4440/5817], Loss: 0.1883, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4450/5817], Loss: 0.1964, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [4460/5817], Loss: 0.1898, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [4470/5817], Loss: 0.1967, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [4480/5817], Loss: 0.1993, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [4490/5817], Loss: 0.1832, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4500/5817], Loss: 0.1991, Accuracy: 0.7344\n",
      "Epoch [2/10], Step [4510/5817], Loss: 0.1734, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [4520/5817], Loss: 0.1956, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [4530/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [4540/5817], Loss: 0.1881, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4550/5817], Loss: 0.1936, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [4560/5817], Loss: 0.1849, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4570/5817], Loss: 0.1983, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [4580/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4590/5817], Loss: 0.1902, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [4600/5817], Loss: 0.1932, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4610/5817], Loss: 0.1948, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [4620/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4630/5817], Loss: 0.1784, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4640/5817], Loss: 0.1774, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [4650/5817], Loss: 0.1868, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4660/5817], Loss: 0.1953, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [4670/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [4680/5817], Loss: 0.1806, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4690/5817], Loss: 0.1890, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4700/5817], Loss: 0.1912, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [4710/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4720/5817], Loss: 0.1810, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [4730/5817], Loss: 0.1912, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4740/5817], Loss: 0.1897, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [4750/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4760/5817], Loss: 0.1825, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [4770/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4780/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4790/5817], Loss: 0.1817, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4800/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4810/5817], Loss: 0.1938, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [4820/5817], Loss: 0.1919, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [4830/5817], Loss: 0.1894, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4840/5817], Loss: 0.1858, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4850/5817], Loss: 0.1703, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [4860/5817], Loss: 0.1892, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [4870/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4880/5817], Loss: 0.2005, Accuracy: 0.7344\n",
      "Epoch [2/10], Step [4890/5817], Loss: 0.1716, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [4900/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [4910/5817], Loss: 0.1802, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [4920/5817], Loss: 0.1924, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [4930/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [4940/5817], Loss: 0.1857, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4950/5817], Loss: 0.2014, Accuracy: 0.7266\n",
      "Epoch [2/10], Step [4960/5817], Loss: 0.1880, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4970/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [4980/5817], Loss: 0.1939, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [4990/5817], Loss: 0.1620, Accuracy: 0.9062\n",
      "Epoch [2/10], Step [5000/5817], Loss: 0.1969, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [5010/5817], Loss: 0.1977, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [5020/5817], Loss: 0.1895, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [5030/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [5040/5817], Loss: 0.1901, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [5050/5817], Loss: 0.1850, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [5060/5817], Loss: 0.1839, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5070/5817], Loss: 0.1901, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [5080/5817], Loss: 0.1893, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [5090/5817], Loss: 0.1955, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [5100/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [5110/5817], Loss: 0.1880, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5120/5817], Loss: 0.1819, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5130/5817], Loss: 0.1951, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [5140/5817], Loss: 0.1949, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [5150/5817], Loss: 0.1967, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [5160/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [5170/5817], Loss: 0.1827, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5180/5817], Loss: 0.1895, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5190/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [5200/5817], Loss: 0.1895, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [5210/5817], Loss: 0.1950, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [5220/5817], Loss: 0.1880, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [5230/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5240/5817], Loss: 0.1872, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5250/5817], Loss: 0.1996, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [5260/5817], Loss: 0.1955, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [5270/5817], Loss: 0.1800, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [5280/5817], Loss: 0.1835, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5290/5817], Loss: 0.1926, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [5300/5817], Loss: 0.1826, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5310/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5320/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5330/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5340/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5350/5817], Loss: 0.1683, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [5360/5817], Loss: 0.1878, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [5370/5817], Loss: 0.1744, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [5380/5817], Loss: 0.1868, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5390/5817], Loss: 0.1814, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5400/5817], Loss: 0.1884, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [5410/5817], Loss: 0.1871, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [5420/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5430/5817], Loss: 0.1944, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [5440/5817], Loss: 0.1987, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [5450/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5460/5817], Loss: 0.1926, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [5470/5817], Loss: 0.1754, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [5480/5817], Loss: 0.1798, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5490/5817], Loss: 0.1896, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5500/5817], Loss: 0.2046, Accuracy: 0.7266\n",
      "Epoch [2/10], Step [5510/5817], Loss: 0.1897, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [5520/5817], Loss: 0.1827, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5530/5817], Loss: 0.1987, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [5540/5817], Loss: 0.1905, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [5550/5817], Loss: 0.1790, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5560/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5570/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [5580/5817], Loss: 0.1872, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [5590/5817], Loss: 0.1906, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [5600/5817], Loss: 0.1904, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [5610/5817], Loss: 0.1951, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [5620/5817], Loss: 0.1999, Accuracy: 0.7344\n",
      "Epoch [2/10], Step [5630/5817], Loss: 0.1885, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [5640/5817], Loss: 0.2014, Accuracy: 0.7266\n",
      "Epoch [2/10], Step [5650/5817], Loss: 0.1761, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [5660/5817], Loss: 0.1850, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5670/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5680/5817], Loss: 0.1945, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [5690/5817], Loss: 0.1818, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [5700/5817], Loss: 0.1932, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [5710/5817], Loss: 0.1845, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5720/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [5730/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [5740/5817], Loss: 0.1895, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [5750/5817], Loss: 0.1944, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [5760/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [5770/5817], Loss: 0.1937, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [5780/5817], Loss: 0.1909, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [5790/5817], Loss: 0.2028, Accuracy: 0.7266\n",
      "Epoch [2/10], Step [5800/5817], Loss: 0.2007, Accuracy: 0.7266\n",
      "Epoch [2/10], Step [5810/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [10/5817], Loss: 0.1810, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [20/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [30/5817], Loss: 0.1888, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [40/5817], Loss: 0.1947, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [50/5817], Loss: 0.1899, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [60/5817], Loss: 0.1798, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [70/5817], Loss: 0.1980, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [80/5817], Loss: 0.1980, Accuracy: 0.7344\n",
      "Epoch [3/10], Step [90/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [100/5817], Loss: 0.1882, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [110/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [120/5817], Loss: 0.1877, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [130/5817], Loss: 0.1792, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [140/5817], Loss: 0.1909, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [150/5817], Loss: 0.1871, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [160/5817], Loss: 0.1889, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [170/5817], Loss: 0.1787, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [180/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [190/5817], Loss: 0.1923, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [200/5817], Loss: 0.1737, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [210/5817], Loss: 0.1927, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [220/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [230/5817], Loss: 0.1761, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [240/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [250/5817], Loss: 0.1939, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [260/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [270/5817], Loss: 0.1928, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [280/5817], Loss: 0.1841, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [290/5817], Loss: 0.1756, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [300/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [310/5817], Loss: 0.1914, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [320/5817], Loss: 0.1975, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [330/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [340/5817], Loss: 0.1959, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [350/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [360/5817], Loss: 0.1808, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [370/5817], Loss: 0.1781, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [380/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [390/5817], Loss: 0.1917, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [400/5817], Loss: 0.1928, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [410/5817], Loss: 0.1788, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [420/5817], Loss: 0.1946, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [430/5817], Loss: 0.1784, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [440/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [450/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [460/5817], Loss: 0.1865, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [470/5817], Loss: 0.2097, Accuracy: 0.7031\n",
      "Epoch [3/10], Step [480/5817], Loss: 0.1908, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [490/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [500/5817], Loss: 0.1737, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [510/5817], Loss: 0.1818, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [520/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [530/5817], Loss: 0.1741, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [540/5817], Loss: 0.1946, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [550/5817], Loss: 0.1750, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [560/5817], Loss: 0.1899, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [570/5817], Loss: 0.1934, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [580/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [590/5817], Loss: 0.1893, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [600/5817], Loss: 0.1636, Accuracy: 0.8984\n",
      "Epoch [3/10], Step [610/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [620/5817], Loss: 0.1830, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [630/5817], Loss: 0.1986, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [640/5817], Loss: 0.1733, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [650/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [660/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [670/5817], Loss: 0.1804, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [680/5817], Loss: 0.1898, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [690/5817], Loss: 0.1937, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [700/5817], Loss: 0.1821, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [710/5817], Loss: 0.1712, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [720/5817], Loss: 0.1867, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [730/5817], Loss: 0.1787, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [740/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [750/5817], Loss: 0.1833, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [760/5817], Loss: 0.1658, Accuracy: 0.8984\n",
      "Epoch [3/10], Step [770/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [780/5817], Loss: 0.1937, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [790/5817], Loss: 0.1941, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [800/5817], Loss: 0.1811, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [810/5817], Loss: 0.1945, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [820/5817], Loss: 0.1891, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [830/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [840/5817], Loss: 0.1948, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [850/5817], Loss: 0.1938, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [860/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [870/5817], Loss: 0.1920, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [880/5817], Loss: 0.1936, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [890/5817], Loss: 0.1873, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [900/5817], Loss: 0.2026, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [910/5817], Loss: 0.1922, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [920/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [930/5817], Loss: 0.1776, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [940/5817], Loss: 0.1938, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [950/5817], Loss: 0.1922, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [960/5817], Loss: 0.2004, Accuracy: 0.7422\n",
      "Epoch [3/10], Step [970/5817], Loss: 0.1792, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [980/5817], Loss: 0.1715, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [990/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [1000/5817], Loss: 0.1840, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1010/5817], Loss: 0.1702, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [1020/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [1030/5817], Loss: 0.1946, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [1040/5817], Loss: 0.1826, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1050/5817], Loss: 0.1863, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1060/5817], Loss: 0.2020, Accuracy: 0.7422\n",
      "Epoch [3/10], Step [1070/5817], Loss: 0.1876, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1080/5817], Loss: 0.2078, Accuracy: 0.7109\n",
      "Epoch [3/10], Step [1090/5817], Loss: 0.2050, Accuracy: 0.7266\n",
      "Epoch [3/10], Step [1100/5817], Loss: 0.2049, Accuracy: 0.7266\n",
      "Epoch [3/10], Step [1110/5817], Loss: 0.2043, Accuracy: 0.7266\n",
      "Epoch [3/10], Step [1120/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1130/5817], Loss: 0.1868, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [1140/5817], Loss: 0.1872, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [1150/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1160/5817], Loss: 0.1825, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1170/5817], Loss: 0.1843, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1180/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1190/5817], Loss: 0.1878, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1200/5817], Loss: 0.1837, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1210/5817], Loss: 0.1779, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1220/5817], Loss: 0.1764, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1230/5817], Loss: 0.1862, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1240/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1250/5817], Loss: 0.1896, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [1260/5817], Loss: 0.1884, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [1270/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1280/5817], Loss: 0.1904, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [1290/5817], Loss: 0.1890, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [1300/5817], Loss: 0.1690, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [1310/5817], Loss: 0.1760, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1320/5817], Loss: 0.1901, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [1330/5817], Loss: 0.1868, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [1340/5817], Loss: 0.2016, Accuracy: 0.7344\n",
      "Epoch [3/10], Step [1350/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [1360/5817], Loss: 0.1601, Accuracy: 0.9141\n",
      "Epoch [3/10], Step [1370/5817], Loss: 0.1854, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1380/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1390/5817], Loss: 0.1927, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [1400/5817], Loss: 0.1651, Accuracy: 0.8906\n",
      "Epoch [3/10], Step [1410/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1420/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [1430/5817], Loss: 0.1825, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1440/5817], Loss: 0.1769, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1450/5817], Loss: 0.1924, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [1460/5817], Loss: 0.1849, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1470/5817], Loss: 0.1666, Accuracy: 0.9062\n",
      "Epoch [3/10], Step [1480/5817], Loss: 0.1952, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [1490/5817], Loss: 0.1883, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [1500/5817], Loss: 0.1754, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [1510/5817], Loss: 0.1827, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1520/5817], Loss: 0.1945, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [1530/5817], Loss: 0.1835, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1540/5817], Loss: 0.1740, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1550/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1560/5817], Loss: 0.1860, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [1570/5817], Loss: 0.1914, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [1580/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1590/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1600/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1610/5817], Loss: 0.1773, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1620/5817], Loss: 0.1904, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [1630/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [1640/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1650/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1660/5817], Loss: 0.1850, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1670/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1680/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [1690/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [1700/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1710/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1720/5817], Loss: 0.1883, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [1730/5817], Loss: 0.1753, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [1740/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [1750/5817], Loss: 0.1933, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [1760/5817], Loss: 0.1902, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [1770/5817], Loss: 0.1899, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [1780/5817], Loss: 0.1773, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1790/5817], Loss: 0.1943, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [1800/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1810/5817], Loss: 0.1651, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [1820/5817], Loss: 0.1849, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [1830/5817], Loss: 0.1931, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [1840/5817], Loss: 0.1836, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1850/5817], Loss: 0.1971, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [1860/5817], Loss: 0.1914, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [1870/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1880/5817], Loss: 0.1781, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1890/5817], Loss: 0.1891, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [1900/5817], Loss: 0.1889, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [1910/5817], Loss: 0.1976, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [1920/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1930/5817], Loss: 0.1963, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [1940/5817], Loss: 0.1976, Accuracy: 0.7422\n",
      "Epoch [3/10], Step [1950/5817], Loss: 0.1681, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [1960/5817], Loss: 0.1790, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1970/5817], Loss: 0.1901, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [1980/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1990/5817], Loss: 0.1985, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [2000/5817], Loss: 0.1824, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2010/5817], Loss: 0.1791, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2020/5817], Loss: 0.1807, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [2030/5817], Loss: 0.2037, Accuracy: 0.7344\n",
      "Epoch [3/10], Step [2040/5817], Loss: 0.1847, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2050/5817], Loss: 0.1925, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2060/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2070/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2080/5817], Loss: 0.1887, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [2090/5817], Loss: 0.1757, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2100/5817], Loss: 0.1959, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [2110/5817], Loss: 0.1797, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2120/5817], Loss: 0.1889, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [2130/5817], Loss: 0.1916, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2140/5817], Loss: 0.1822, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2150/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2160/5817], Loss: 0.1913, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [2170/5817], Loss: 0.1795, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2180/5817], Loss: 0.1876, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2190/5817], Loss: 0.1818, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2200/5817], Loss: 0.1917, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [2210/5817], Loss: 0.1961, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [2220/5817], Loss: 0.2041, Accuracy: 0.7188\n",
      "Epoch [3/10], Step [2230/5817], Loss: 0.2049, Accuracy: 0.7344\n",
      "Epoch [3/10], Step [2240/5817], Loss: 0.1784, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2250/5817], Loss: 0.1923, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [2260/5817], Loss: 0.1854, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2270/5817], Loss: 0.1887, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2280/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2290/5817], Loss: 0.1938, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [2300/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2310/5817], Loss: 0.1883, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2320/5817], Loss: 0.1924, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [2330/5817], Loss: 0.1760, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [2340/5817], Loss: 0.2006, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [2350/5817], Loss: 0.1771, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2360/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2370/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2380/5817], Loss: 0.1889, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2390/5817], Loss: 0.2020, Accuracy: 0.7344\n",
      "Epoch [3/10], Step [2400/5817], Loss: 0.1923, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [2410/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [2420/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2430/5817], Loss: 0.1889, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2440/5817], Loss: 0.1853, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2450/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [2460/5817], Loss: 0.1812, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [2470/5817], Loss: 0.1849, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2480/5817], Loss: 0.1713, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2490/5817], Loss: 0.1666, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [2500/5817], Loss: 0.1911, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2510/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2520/5817], Loss: 0.1854, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2530/5817], Loss: 0.1841, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2540/5817], Loss: 0.1820, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2550/5817], Loss: 0.1962, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [2560/5817], Loss: 0.1886, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [2570/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2580/5817], Loss: 0.1879, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2590/5817], Loss: 0.1878, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2600/5817], Loss: 0.1745, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [2610/5817], Loss: 0.1824, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2620/5817], Loss: 0.1898, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2630/5817], Loss: 0.1897, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [2640/5817], Loss: 0.1878, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2650/5817], Loss: 0.1962, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [2660/5817], Loss: 0.1881, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2670/5817], Loss: 0.1917, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [2680/5817], Loss: 0.1825, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2690/5817], Loss: 0.1875, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2700/5817], Loss: 0.1911, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [2710/5817], Loss: 0.1836, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2720/5817], Loss: 0.1896, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2730/5817], Loss: 0.1875, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2740/5817], Loss: 0.1911, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2750/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2760/5817], Loss: 0.1830, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2770/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [2780/5817], Loss: 0.1907, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [2790/5817], Loss: 0.1892, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2800/5817], Loss: 0.1923, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [2810/5817], Loss: 0.1933, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [2820/5817], Loss: 0.1919, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [2830/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2840/5817], Loss: 0.1960, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [2850/5817], Loss: 0.1985, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [2860/5817], Loss: 0.1928, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2870/5817], Loss: 0.1814, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2880/5817], Loss: 0.1855, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2890/5817], Loss: 0.1935, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [2900/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [2910/5817], Loss: 0.1839, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2920/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2930/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2940/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [2950/5817], Loss: 0.1835, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2960/5817], Loss: 0.1888, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2970/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2980/5817], Loss: 0.2008, Accuracy: 0.7266\n",
      "Epoch [3/10], Step [2990/5817], Loss: 0.1859, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [3000/5817], Loss: 0.1827, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3010/5817], Loss: 0.1945, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [3020/5817], Loss: 0.1932, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3030/5817], Loss: 0.1874, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3040/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3050/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [3060/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3070/5817], Loss: 0.1763, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [3080/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [3090/5817], Loss: 0.1848, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [3100/5817], Loss: 0.1941, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [3110/5817], Loss: 0.1881, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [3120/5817], Loss: 0.1728, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [3130/5817], Loss: 0.2036, Accuracy: 0.7266\n",
      "Epoch [3/10], Step [3140/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [3150/5817], Loss: 0.1807, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3160/5817], Loss: 0.1747, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [3170/5817], Loss: 0.1831, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3180/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [3190/5817], Loss: 0.1931, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3200/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3210/5817], Loss: 0.2011, Accuracy: 0.7422\n",
      "Epoch [3/10], Step [3220/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3230/5817], Loss: 0.1887, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3240/5817], Loss: 0.1969, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [3250/5817], Loss: 0.1704, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [3260/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3270/5817], Loss: 0.1886, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [3280/5817], Loss: 0.1991, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [3290/5817], Loss: 0.1867, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [3300/5817], Loss: 0.1854, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [3310/5817], Loss: 0.1875, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3320/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [3330/5817], Loss: 0.1814, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [3340/5817], Loss: 0.1797, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [3350/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3360/5817], Loss: 0.1776, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3370/5817], Loss: 0.1990, Accuracy: 0.7266\n",
      "Epoch [3/10], Step [3380/5817], Loss: 0.1884, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3390/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3400/5817], Loss: 0.1951, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [3410/5817], Loss: 0.1932, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [3420/5817], Loss: 0.1890, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3430/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [3440/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3450/5817], Loss: 0.1903, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [3460/5817], Loss: 0.1886, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3470/5817], Loss: 0.1761, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [3480/5817], Loss: 0.1944, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [3490/5817], Loss: 0.2026, Accuracy: 0.7266\n",
      "Epoch [3/10], Step [3500/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [3510/5817], Loss: 0.1835, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3520/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [3530/5817], Loss: 0.1973, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [3540/5817], Loss: 0.1816, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3550/5817], Loss: 0.1722, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [3560/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3570/5817], Loss: 0.1907, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [3580/5817], Loss: 0.2003, Accuracy: 0.7344\n",
      "Epoch [3/10], Step [3590/5817], Loss: 0.1962, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [3600/5817], Loss: 0.1891, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [3610/5817], Loss: 0.1954, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [3620/5817], Loss: 0.1843, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3630/5817], Loss: 0.1839, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [3640/5817], Loss: 0.1928, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [3650/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [3660/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [3670/5817], Loss: 0.1966, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [3680/5817], Loss: 0.1868, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [3690/5817], Loss: 0.1746, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [3700/5817], Loss: 0.1909, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [3710/5817], Loss: 0.1951, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [3720/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3730/5817], Loss: 0.1975, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [3740/5817], Loss: 0.1794, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3750/5817], Loss: 0.1750, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [3760/5817], Loss: 0.1943, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [3770/5817], Loss: 0.1873, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [3780/5817], Loss: 0.1902, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3790/5817], Loss: 0.1725, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [3800/5817], Loss: 0.1915, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [3810/5817], Loss: 0.1875, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [3820/5817], Loss: 0.2046, Accuracy: 0.7188\n",
      "Epoch [3/10], Step [3830/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [3840/5817], Loss: 0.1787, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [3850/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3860/5817], Loss: 0.2045, Accuracy: 0.7188\n",
      "Epoch [3/10], Step [3870/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [3880/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [3890/5817], Loss: 0.1752, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [3900/5817], Loss: 0.1856, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [3910/5817], Loss: 0.1785, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3920/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3930/5817], Loss: 0.1874, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [3940/5817], Loss: 0.1876, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3950/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [3960/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [3970/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [3980/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3990/5817], Loss: 0.1931, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [4000/5817], Loss: 0.1901, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4010/5817], Loss: 0.1773, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4020/5817], Loss: 0.1887, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [4030/5817], Loss: 0.1901, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [4040/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4050/5817], Loss: 0.1861, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4060/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4070/5817], Loss: 0.1914, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [4080/5817], Loss: 0.1808, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4090/5817], Loss: 0.1958, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [4100/5817], Loss: 0.1863, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4110/5817], Loss: 0.1994, Accuracy: 0.7422\n",
      "Epoch [3/10], Step [4120/5817], Loss: 0.1915, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [4130/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4140/5817], Loss: 0.2077, Accuracy: 0.7109\n",
      "Epoch [3/10], Step [4150/5817], Loss: 0.1873, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4160/5817], Loss: 0.1848, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4170/5817], Loss: 0.1722, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [4180/5817], Loss: 0.1872, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4190/5817], Loss: 0.1930, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [4200/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [4210/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [4220/5817], Loss: 0.1783, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4230/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4240/5817], Loss: 0.1830, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4250/5817], Loss: 0.2009, Accuracy: 0.7422\n",
      "Epoch [3/10], Step [4260/5817], Loss: 0.1920, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [4270/5817], Loss: 0.1929, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [4280/5817], Loss: 0.1832, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [4290/5817], Loss: 0.1858, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [4300/5817], Loss: 0.1775, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4310/5817], Loss: 0.1868, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4320/5817], Loss: 0.2031, Accuracy: 0.7188\n",
      "Epoch [3/10], Step [4330/5817], Loss: 0.1718, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [4340/5817], Loss: 0.1798, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [4350/5817], Loss: 0.1752, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4360/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4370/5817], Loss: 0.1754, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [4380/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4390/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4400/5817], Loss: 0.1871, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [4410/5817], Loss: 0.1871, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4420/5817], Loss: 0.1925, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [4430/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4440/5817], Loss: 0.1880, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4450/5817], Loss: 0.1903, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4460/5817], Loss: 0.1935, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [4470/5817], Loss: 0.1838, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4480/5817], Loss: 0.1852, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4490/5817], Loss: 0.1892, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [4500/5817], Loss: 0.1972, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [4510/5817], Loss: 0.1968, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [4520/5817], Loss: 0.1886, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [4530/5817], Loss: 0.1957, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [4540/5817], Loss: 0.1889, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [4550/5817], Loss: 0.1857, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4560/5817], Loss: 0.1779, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [4570/5817], Loss: 0.1833, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4580/5817], Loss: 0.1867, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [4590/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4600/5817], Loss: 0.1934, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [4610/5817], Loss: 0.1927, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [4620/5817], Loss: 0.1739, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [4630/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4640/5817], Loss: 0.1969, Accuracy: 0.7422\n",
      "Epoch [3/10], Step [4650/5817], Loss: 0.1833, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4660/5817], Loss: 0.1889, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [4670/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4680/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4690/5817], Loss: 0.1844, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4700/5817], Loss: 0.1913, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [4710/5817], Loss: 0.1797, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [4720/5817], Loss: 0.1970, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [4730/5817], Loss: 0.1719, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [4740/5817], Loss: 0.1975, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [4750/5817], Loss: 0.1901, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [4760/5817], Loss: 0.1899, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4770/5817], Loss: 0.1702, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [4780/5817], Loss: 0.1979, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [4790/5817], Loss: 0.1763, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [4800/5817], Loss: 0.1920, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [4810/5817], Loss: 0.1625, Accuracy: 0.9141\n",
      "Epoch [3/10], Step [4820/5817], Loss: 0.1895, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [4830/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4840/5817], Loss: 0.1987, Accuracy: 0.7422\n",
      "Epoch [3/10], Step [4850/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4860/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4870/5817], Loss: 0.1815, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4880/5817], Loss: 0.1895, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4890/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4900/5817], Loss: 0.1896, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [4910/5817], Loss: 0.1906, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4920/5817], Loss: 0.1811, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4930/5817], Loss: 0.1835, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4940/5817], Loss: 0.1710, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [4950/5817], Loss: 0.1819, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4960/5817], Loss: 0.1768, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4970/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4980/5817], Loss: 0.1856, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4990/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [5000/5817], Loss: 0.1911, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [5010/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [5020/5817], Loss: 0.2048, Accuracy: 0.7266\n",
      "Epoch [3/10], Step [5030/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [5040/5817], Loss: 0.1839, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [5050/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [5060/5817], Loss: 0.1860, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [5070/5817], Loss: 0.1709, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [5080/5817], Loss: 0.1788, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5090/5817], Loss: 0.1643, Accuracy: 0.8906\n",
      "Epoch [3/10], Step [5100/5817], Loss: 0.1784, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5110/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5120/5817], Loss: 0.1895, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [5130/5817], Loss: 0.1867, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [5140/5817], Loss: 0.1983, Accuracy: 0.7422\n",
      "Epoch [3/10], Step [5150/5817], Loss: 0.1908, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [5160/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [5170/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [5180/5817], Loss: 0.1932, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [5190/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [5200/5817], Loss: 0.1789, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5210/5817], Loss: 0.1936, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [5220/5817], Loss: 0.1819, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [5230/5817], Loss: 0.1851, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [5240/5817], Loss: 0.1909, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [5250/5817], Loss: 0.1893, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [5260/5817], Loss: 0.1924, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [5270/5817], Loss: 0.1857, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [5280/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5290/5817], Loss: 0.1864, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [5300/5817], Loss: 0.1822, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [5310/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [5320/5817], Loss: 0.1791, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5330/5817], Loss: 0.2001, Accuracy: 0.7422\n",
      "Epoch [3/10], Step [5340/5817], Loss: 0.1795, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [5350/5817], Loss: 0.1885, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [5360/5817], Loss: 0.1895, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [5370/5817], Loss: 0.1791, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5380/5817], Loss: 0.1885, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [5390/5817], Loss: 0.1840, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [5400/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5410/5817], Loss: 0.1904, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [5420/5817], Loss: 0.1932, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [5430/5817], Loss: 0.1905, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [5440/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5450/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [5460/5817], Loss: 0.1911, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [5470/5817], Loss: 0.1834, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [5480/5817], Loss: 0.1945, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [5490/5817], Loss: 0.1856, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [5500/5817], Loss: 0.1809, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [5510/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [5520/5817], Loss: 0.1846, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [5530/5817], Loss: 0.1907, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [5540/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [5550/5817], Loss: 0.1930, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [5560/5817], Loss: 0.1914, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [5570/5817], Loss: 0.1764, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [5580/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [5590/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [5600/5817], Loss: 0.1826, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [5610/5817], Loss: 0.1836, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [5620/5817], Loss: 0.1806, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [5630/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [5640/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [5650/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [5660/5817], Loss: 0.1901, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [5670/5817], Loss: 0.1841, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [5680/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5690/5817], Loss: 0.1854, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [5700/5817], Loss: 0.1855, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [5710/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [5720/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [5730/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5740/5817], Loss: 0.1980, Accuracy: 0.7344\n",
      "Epoch [3/10], Step [5750/5817], Loss: 0.1960, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [5760/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [5770/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [5780/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [5790/5817], Loss: 0.1896, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [5800/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5810/5817], Loss: 0.1933, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [10/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [20/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [30/5817], Loss: 0.1921, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [40/5817], Loss: 0.1754, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [50/5817], Loss: 0.1885, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [60/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [70/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [80/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [90/5817], Loss: 0.1902, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [100/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [110/5817], Loss: 0.1831, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [120/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [130/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [140/5817], Loss: 0.1958, Accuracy: 0.7500\n",
      "Epoch [4/10], Step [150/5817], Loss: 0.1825, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [160/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [170/5817], Loss: 0.1753, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [180/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [190/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [200/5817], Loss: 0.2080, Accuracy: 0.7109\n",
      "Epoch [4/10], Step [210/5817], Loss: 0.1855, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [220/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [230/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [240/5817], Loss: 0.1757, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [250/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [260/5817], Loss: 0.1837, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [270/5817], Loss: 0.1834, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [280/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [290/5817], Loss: 0.1702, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [300/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [310/5817], Loss: 0.1872, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [320/5817], Loss: 0.1841, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [330/5817], Loss: 0.1816, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [340/5817], Loss: 0.1700, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [350/5817], Loss: 0.1793, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [360/5817], Loss: 0.1860, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [370/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [380/5817], Loss: 0.1833, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [390/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [400/5817], Loss: 0.1805, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [410/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [420/5817], Loss: 0.1847, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [430/5817], Loss: 0.1927, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [440/5817], Loss: 0.1882, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [450/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [460/5817], Loss: 0.1959, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [470/5817], Loss: 0.1812, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [480/5817], Loss: 0.1914, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [490/5817], Loss: 0.1889, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [500/5817], Loss: 0.1709, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [510/5817], Loss: 0.1795, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [520/5817], Loss: 0.1852, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [530/5817], Loss: 0.1776, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [540/5817], Loss: 0.1822, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [550/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [560/5817], Loss: 0.1835, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [570/5817], Loss: 0.1957, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [580/5817], Loss: 0.1884, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [590/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [600/5817], Loss: 0.1919, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [610/5817], Loss: 0.1802, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [620/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [630/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [640/5817], Loss: 0.1826, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [650/5817], Loss: 0.1838, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [660/5817], Loss: 0.1782, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [670/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [680/5817], Loss: 0.1893, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [690/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [700/5817], Loss: 0.1867, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [710/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [720/5817], Loss: 0.1908, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [730/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [740/5817], Loss: 0.1890, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [750/5817], Loss: 0.1838, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [760/5817], Loss: 0.1898, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [770/5817], Loss: 0.1745, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [780/5817], Loss: 0.1834, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [790/5817], Loss: 0.1853, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [800/5817], Loss: 0.1861, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [810/5817], Loss: 0.1970, Accuracy: 0.7500\n",
      "Epoch [4/10], Step [820/5817], Loss: 0.1864, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [830/5817], Loss: 0.1950, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [840/5817], Loss: 0.1813, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [850/5817], Loss: 0.1849, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [860/5817], Loss: 0.1871, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [870/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [880/5817], Loss: 0.1736, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [890/5817], Loss: 0.1744, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [900/5817], Loss: 0.1852, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [910/5817], Loss: 0.1931, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [920/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [930/5817], Loss: 0.1689, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [940/5817], Loss: 0.1905, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [950/5817], Loss: 0.1911, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [960/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [970/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [980/5817], Loss: 0.1890, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [990/5817], Loss: 0.1872, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [1000/5817], Loss: 0.2031, Accuracy: 0.7188\n",
      "Epoch [4/10], Step [1010/5817], Loss: 0.1933, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [1020/5817], Loss: 0.1784, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1030/5817], Loss: 0.1815, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [1040/5817], Loss: 0.1860, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [1050/5817], Loss: 0.1921, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [1060/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1070/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1080/5817], Loss: 0.1951, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [1090/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1100/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [1110/5817], Loss: 0.1876, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [1120/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [1130/5817], Loss: 0.1634, Accuracy: 0.8984\n",
      "Epoch [4/10], Step [1140/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [1150/5817], Loss: 0.1845, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [1160/5817], Loss: 0.1774, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1170/5817], Loss: 0.1688, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [1180/5817], Loss: 0.1941, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [1190/5817], Loss: 0.1787, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1200/5817], Loss: 0.1702, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1210/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [1220/5817], Loss: 0.1860, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [1230/5817], Loss: 0.1929, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [1240/5817], Loss: 0.1881, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [1250/5817], Loss: 0.1854, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [1260/5817], Loss: 0.1807, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [1270/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1280/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [1290/5817], Loss: 0.1806, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1300/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1310/5817], Loss: 0.1851, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1320/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1330/5817], Loss: 0.1787, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1340/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1350/5817], Loss: 0.1887, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [1360/5817], Loss: 0.1800, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1370/5817], Loss: 0.1912, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [1380/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1390/5817], Loss: 0.1871, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [1400/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1410/5817], Loss: 0.1891, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [1420/5817], Loss: 0.2075, Accuracy: 0.7266\n",
      "Epoch [4/10], Step [1430/5817], Loss: 0.1962, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [1440/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1450/5817], Loss: 0.1887, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [1460/5817], Loss: 0.1738, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [1470/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [1480/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [1490/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1500/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1510/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1520/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1530/5817], Loss: 0.1937, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [1540/5817], Loss: 0.1689, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [1550/5817], Loss: 0.1716, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [1560/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1570/5817], Loss: 0.1803, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1580/5817], Loss: 0.2040, Accuracy: 0.7344\n",
      "Epoch [4/10], Step [1590/5817], Loss: 0.1915, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [1600/5817], Loss: 0.1930, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [1610/5817], Loss: 0.1816, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1620/5817], Loss: 0.1800, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1630/5817], Loss: 0.1819, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1640/5817], Loss: 0.1788, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [1650/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1660/5817], Loss: 0.1909, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [1670/5817], Loss: 0.1804, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1680/5817], Loss: 0.1737, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1690/5817], Loss: 0.1940, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [1700/5817], Loss: 0.1780, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1710/5817], Loss: 0.1902, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [1720/5817], Loss: 0.1726, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1730/5817], Loss: 0.1896, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [1740/5817], Loss: 0.1782, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1750/5817], Loss: 0.1835, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [1760/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1770/5817], Loss: 0.1868, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [1780/5817], Loss: 0.1789, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1790/5817], Loss: 0.1741, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [1800/5817], Loss: 0.1839, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1810/5817], Loss: 0.1890, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [1820/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1830/5817], Loss: 0.1775, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1840/5817], Loss: 0.1879, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [1850/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [1860/5817], Loss: 0.1907, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [1870/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1880/5817], Loss: 0.1906, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [1890/5817], Loss: 0.1927, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [1900/5817], Loss: 0.1791, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1910/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [1920/5817], Loss: 0.1940, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [1930/5817], Loss: 0.1845, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [1940/5817], Loss: 0.1851, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [1950/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [1960/5817], Loss: 0.1805, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1970/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1980/5817], Loss: 0.1875, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [1990/5817], Loss: 0.1807, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [2000/5817], Loss: 0.1692, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [2010/5817], Loss: 0.1805, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2020/5817], Loss: 0.1893, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [2030/5817], Loss: 0.1932, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [2040/5817], Loss: 0.1915, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [2050/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [2060/5817], Loss: 0.1764, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [2070/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [2080/5817], Loss: 0.1883, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [2090/5817], Loss: 0.1823, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2100/5817], Loss: 0.1763, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2110/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [2120/5817], Loss: 0.1935, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [2130/5817], Loss: 0.1618, Accuracy: 0.8984\n",
      "Epoch [4/10], Step [2140/5817], Loss: 0.1843, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [2150/5817], Loss: 0.1903, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [2160/5817], Loss: 0.1841, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [2170/5817], Loss: 0.1701, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [2180/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [2190/5817], Loss: 0.1920, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [2200/5817], Loss: 0.1969, Accuracy: 0.7500\n",
      "Epoch [4/10], Step [2210/5817], Loss: 0.1985, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [2220/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2230/5817], Loss: 0.1853, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [2240/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [2250/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2260/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2270/5817], Loss: 0.1829, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [2280/5817], Loss: 0.1887, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [2290/5817], Loss: 0.1750, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [2300/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2310/5817], Loss: 0.1955, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [2320/5817], Loss: 0.1981, Accuracy: 0.7344\n",
      "Epoch [4/10], Step [2330/5817], Loss: 0.1747, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [2340/5817], Loss: 0.1737, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2350/5817], Loss: 0.1861, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [2360/5817], Loss: 0.1931, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [2370/5817], Loss: 0.1863, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2380/5817], Loss: 0.1935, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [2390/5817], Loss: 0.1861, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [2400/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2410/5817], Loss: 0.1798, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [2420/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2430/5817], Loss: 0.1856, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2440/5817], Loss: 0.1833, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2450/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [2460/5817], Loss: 0.1686, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [2470/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [2480/5817], Loss: 0.1789, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [2490/5817], Loss: 0.1999, Accuracy: 0.7500\n",
      "Epoch [4/10], Step [2500/5817], Loss: 0.1845, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [2510/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2520/5817], Loss: 0.1802, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [2530/5817], Loss: 0.1939, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [2540/5817], Loss: 0.1895, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [2550/5817], Loss: 0.1843, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [2560/5817], Loss: 0.1710, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [2570/5817], Loss: 0.1903, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [2580/5817], Loss: 0.1892, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [2590/5817], Loss: 0.1888, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [2600/5817], Loss: 0.1850, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2610/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2620/5817], Loss: 0.1838, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [2630/5817], Loss: 0.1963, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [2640/5817], Loss: 0.1860, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [2650/5817], Loss: 0.1912, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [2660/5817], Loss: 0.1721, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2670/5817], Loss: 0.1845, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [2680/5817], Loss: 0.1991, Accuracy: 0.7422\n",
      "Epoch [4/10], Step [2690/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2700/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [2710/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2720/5817], Loss: 0.1952, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [2730/5817], Loss: 0.1769, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2740/5817], Loss: 0.1955, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [2750/5817], Loss: 0.1723, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [2760/5817], Loss: 0.1827, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [2770/5817], Loss: 0.1885, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [2780/5817], Loss: 0.1708, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [2790/5817], Loss: 0.1949, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [2800/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2810/5817], Loss: 0.1832, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [2820/5817], Loss: 0.1912, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [2830/5817], Loss: 0.1883, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [2840/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [2850/5817], Loss: 0.1992, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [2860/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2870/5817], Loss: 0.1970, Accuracy: 0.7422\n",
      "Epoch [4/10], Step [2880/5817], Loss: 0.1887, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [2890/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2900/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2910/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [2920/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [2930/5817], Loss: 0.1872, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [2940/5817], Loss: 0.1853, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [2950/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2960/5817], Loss: 0.1840, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2970/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [2980/5817], Loss: 0.1896, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [2990/5817], Loss: 0.1955, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [3000/5817], Loss: 0.1850, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [3010/5817], Loss: 0.1920, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [3020/5817], Loss: 0.1971, Accuracy: 0.7422\n",
      "Epoch [4/10], Step [3030/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3040/5817], Loss: 0.1757, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3050/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [3060/5817], Loss: 0.2006, Accuracy: 0.7500\n",
      "Epoch [4/10], Step [3070/5817], Loss: 0.1912, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [3080/5817], Loss: 0.1816, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3090/5817], Loss: 0.1707, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3100/5817], Loss: 0.1914, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [3110/5817], Loss: 0.1894, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [3120/5817], Loss: 0.1928, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [3130/5817], Loss: 0.1755, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3140/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3150/5817], Loss: 0.1901, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [3160/5817], Loss: 0.1882, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [3170/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3180/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [3190/5817], Loss: 0.1981, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [3200/5817], Loss: 0.1911, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [3210/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3220/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [3230/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3240/5817], Loss: 0.1631, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [3250/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3260/5817], Loss: 0.1729, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [3270/5817], Loss: 0.1700, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3280/5817], Loss: 0.1823, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3290/5817], Loss: 0.1889, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [3300/5817], Loss: 0.1801, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [3310/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [3320/5817], Loss: 0.1882, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [3330/5817], Loss: 0.1710, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3340/5817], Loss: 0.1885, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [3350/5817], Loss: 0.1764, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [3360/5817], Loss: 0.1830, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3370/5817], Loss: 0.1680, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [3380/5817], Loss: 0.1867, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [3390/5817], Loss: 0.1672, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [3400/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3410/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3420/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3430/5817], Loss: 0.1917, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [3440/5817], Loss: 0.1745, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3450/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [3460/5817], Loss: 0.1928, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [3470/5817], Loss: 0.2009, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [3480/5817], Loss: 0.1907, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [3490/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3500/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3510/5817], Loss: 0.1916, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [3520/5817], Loss: 0.1870, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [3530/5817], Loss: 0.1744, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3540/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3550/5817], Loss: 0.1837, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [3560/5817], Loss: 0.1844, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [3570/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3580/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [3590/5817], Loss: 0.1887, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [3600/5817], Loss: 0.1933, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [3610/5817], Loss: 0.1904, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [3620/5817], Loss: 0.1969, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [3630/5817], Loss: 0.1954, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [3640/5817], Loss: 0.1757, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3650/5817], Loss: 0.1853, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [3660/5817], Loss: 0.2003, Accuracy: 0.7422\n",
      "Epoch [4/10], Step [3670/5817], Loss: 0.1741, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [3680/5817], Loss: 0.1785, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3690/5817], Loss: 0.1885, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [3700/5817], Loss: 0.1843, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [3710/5817], Loss: 0.1774, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3720/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3730/5817], Loss: 0.1765, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3740/5817], Loss: 0.1748, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3750/5817], Loss: 0.1888, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [3760/5817], Loss: 0.1879, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [3770/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3780/5817], Loss: 0.2011, Accuracy: 0.7344\n",
      "Epoch [4/10], Step [3790/5817], Loss: 0.1738, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3800/5817], Loss: 0.1814, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [3810/5817], Loss: 0.1725, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [3820/5817], Loss: 0.1984, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [3830/5817], Loss: 0.1940, Accuracy: 0.7422\n",
      "Epoch [4/10], Step [3840/5817], Loss: 0.1916, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [3850/5817], Loss: 0.1782, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3860/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3870/5817], Loss: 0.1750, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3880/5817], Loss: 0.1843, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3890/5817], Loss: 0.1886, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [3900/5817], Loss: 0.1744, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3910/5817], Loss: 0.1844, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3920/5817], Loss: 0.1726, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3930/5817], Loss: 0.1822, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3940/5817], Loss: 0.1845, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [3950/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3960/5817], Loss: 0.1779, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3970/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3980/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3990/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [4000/5817], Loss: 0.1727, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [4010/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [4020/5817], Loss: 0.2002, Accuracy: 0.7422\n",
      "Epoch [4/10], Step [4030/5817], Loss: 0.2052, Accuracy: 0.7188\n",
      "Epoch [4/10], Step [4040/5817], Loss: 0.1806, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4050/5817], Loss: 0.1875, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [4060/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4070/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [4080/5817], Loss: 0.1873, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [4090/5817], Loss: 0.1696, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [4100/5817], Loss: 0.1886, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [4110/5817], Loss: 0.1871, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [4120/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4130/5817], Loss: 0.1693, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [4140/5817], Loss: 0.1985, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [4150/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4160/5817], Loss: 0.1921, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [4170/5817], Loss: 0.1785, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4180/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [4190/5817], Loss: 0.1917, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [4200/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [4210/5817], Loss: 0.1982, Accuracy: 0.7422\n",
      "Epoch [4/10], Step [4220/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [4230/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4240/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4250/5817], Loss: 0.1868, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [4260/5817], Loss: 0.1883, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [4270/5817], Loss: 0.1919, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [4280/5817], Loss: 0.1821, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4290/5817], Loss: 0.1930, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [4300/5817], Loss: 0.1813, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4310/5817], Loss: 0.1752, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [4320/5817], Loss: 0.1841, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4330/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [4340/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [4350/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [4360/5817], Loss: 0.1748, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [4370/5817], Loss: 0.1801, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4380/5817], Loss: 0.2026, Accuracy: 0.7422\n",
      "Epoch [4/10], Step [4390/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4400/5817], Loss: 0.1871, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [4410/5817], Loss: 0.1754, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [4420/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [4430/5817], Loss: 0.1692, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [4440/5817], Loss: 0.1834, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4450/5817], Loss: 0.1870, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [4460/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [4470/5817], Loss: 0.1839, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [4480/5817], Loss: 0.1852, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [4490/5817], Loss: 0.1796, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4500/5817], Loss: 0.1708, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [4510/5817], Loss: 0.1682, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [4520/5817], Loss: 0.1867, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [4530/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4540/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [4550/5817], Loss: 0.1878, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [4560/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [4570/5817], Loss: 0.1918, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [4580/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4590/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4600/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4610/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4620/5817], Loss: 0.1818, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [4630/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4640/5817], Loss: 0.1830, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [4650/5817], Loss: 0.1822, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [4660/5817], Loss: 0.1860, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [4670/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4680/5817], Loss: 0.1845, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4690/5817], Loss: 0.1863, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [4700/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4710/5817], Loss: 0.1838, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4720/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4730/5817], Loss: 0.1954, Accuracy: 0.7500\n",
      "Epoch [4/10], Step [4740/5817], Loss: 0.1942, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [4750/5817], Loss: 0.1879, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [4760/5817], Loss: 0.1769, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4770/5817], Loss: 0.1746, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4780/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [4790/5817], Loss: 0.1909, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [4800/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4810/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4820/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [4830/5817], Loss: 0.1956, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [4840/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4850/5817], Loss: 0.1714, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [4860/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4870/5817], Loss: 0.1940, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [4880/5817], Loss: 0.1954, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [4890/5817], Loss: 0.1944, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [4900/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4910/5817], Loss: 0.1951, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [4920/5817], Loss: 0.1851, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [4930/5817], Loss: 0.1797, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4940/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4950/5817], Loss: 0.2085, Accuracy: 0.7031\n",
      "Epoch [4/10], Step [4960/5817], Loss: 0.1774, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4970/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [4980/5817], Loss: 0.1810, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [4990/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5000/5817], Loss: 0.1955, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [5010/5817], Loss: 0.1906, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [5020/5817], Loss: 0.1841, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [5030/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [5040/5817], Loss: 0.1787, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [5050/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [5060/5817], Loss: 0.1847, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5070/5817], Loss: 0.1944, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [5080/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5090/5817], Loss: 0.1863, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5100/5817], Loss: 0.1691, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [5110/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [5120/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [5130/5817], Loss: 0.1865, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5140/5817], Loss: 0.1702, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [5150/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [5160/5817], Loss: 0.1818, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5170/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5180/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [5190/5817], Loss: 0.2047, Accuracy: 0.7266\n",
      "Epoch [4/10], Step [5200/5817], Loss: 0.1909, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [5210/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [5220/5817], Loss: 0.1883, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5230/5817], Loss: 0.1892, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [5240/5817], Loss: 0.1942, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [5250/5817], Loss: 0.1914, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [5260/5817], Loss: 0.1774, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5270/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5280/5817], Loss: 0.1899, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [5290/5817], Loss: 0.1673, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [5300/5817], Loss: 0.1812, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [5310/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [5320/5817], Loss: 0.1888, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [5330/5817], Loss: 0.1839, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [5340/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5350/5817], Loss: 0.1729, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [5360/5817], Loss: 0.1819, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5370/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [5380/5817], Loss: 0.1936, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [5390/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [5400/5817], Loss: 0.1875, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5410/5817], Loss: 0.1735, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [5420/5817], Loss: 0.1843, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5430/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [5440/5817], Loss: 0.1921, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [5450/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [5460/5817], Loss: 0.1745, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [5470/5817], Loss: 0.1638, Accuracy: 0.8984\n",
      "Epoch [4/10], Step [5480/5817], Loss: 0.1879, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [5490/5817], Loss: 0.1912, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [5500/5817], Loss: 0.1653, Accuracy: 0.8984\n",
      "Epoch [4/10], Step [5510/5817], Loss: 0.1947, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [5520/5817], Loss: 0.1761, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [5530/5817], Loss: 0.1872, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [5540/5817], Loss: 0.2031, Accuracy: 0.7266\n",
      "Epoch [4/10], Step [5550/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [5560/5817], Loss: 0.2001, Accuracy: 0.7344\n",
      "Epoch [4/10], Step [5570/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [5580/5817], Loss: 0.1902, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [5590/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [5600/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [5610/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [5620/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [5630/5817], Loss: 0.1693, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [5640/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5650/5817], Loss: 0.1869, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5660/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [5670/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [5680/5817], Loss: 0.1807, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [5690/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5700/5817], Loss: 0.1811, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5710/5817], Loss: 0.1816, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5720/5817], Loss: 0.1704, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [5730/5817], Loss: 0.1834, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5740/5817], Loss: 0.1858, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [5750/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [5760/5817], Loss: 0.1798, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5770/5817], Loss: 0.1838, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5780/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [5790/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [5800/5817], Loss: 0.1843, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5810/5817], Loss: 0.1871, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [10/5817], Loss: 0.1895, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [20/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [30/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [40/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [50/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [60/5817], Loss: 0.1873, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [70/5817], Loss: 0.1879, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [80/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [90/5817], Loss: 0.1874, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [100/5817], Loss: 0.1867, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [110/5817], Loss: 0.1913, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [120/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [130/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [140/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [150/5817], Loss: 0.2033, Accuracy: 0.7422\n",
      "Epoch [5/10], Step [160/5817], Loss: 0.1785, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [170/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [180/5817], Loss: 0.1892, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [190/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [200/5817], Loss: 0.1668, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [210/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [220/5817], Loss: 0.1831, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [230/5817], Loss: 0.1801, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [240/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [250/5817], Loss: 0.1798, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [260/5817], Loss: 0.1850, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [270/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [280/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [290/5817], Loss: 0.1746, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [300/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [310/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [320/5817], Loss: 0.1853, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [330/5817], Loss: 0.1666, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [340/5817], Loss: 0.1719, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [350/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [360/5817], Loss: 0.1659, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [370/5817], Loss: 0.1817, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [380/5817], Loss: 0.1801, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [390/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [400/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [410/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [420/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [430/5817], Loss: 0.1909, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [440/5817], Loss: 0.1898, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [450/5817], Loss: 0.1832, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [460/5817], Loss: 0.1768, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [470/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [480/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [490/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [500/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [510/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [520/5817], Loss: 0.2021, Accuracy: 0.7188\n",
      "Epoch [5/10], Step [530/5817], Loss: 0.1735, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [540/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [550/5817], Loss: 0.1862, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [560/5817], Loss: 0.1908, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [570/5817], Loss: 0.1811, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [580/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [590/5817], Loss: 0.1889, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [600/5817], Loss: 0.1909, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [610/5817], Loss: 0.1865, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [620/5817], Loss: 0.1919, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [630/5817], Loss: 0.1956, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [640/5817], Loss: 0.1905, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [650/5817], Loss: 0.1811, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [660/5817], Loss: 0.1904, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [670/5817], Loss: 0.1878, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [680/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [690/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [700/5817], Loss: 0.1764, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [710/5817], Loss: 0.1823, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [720/5817], Loss: 0.1803, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [730/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [740/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [750/5817], Loss: 0.1811, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [760/5817], Loss: 0.1867, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [770/5817], Loss: 0.1834, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [780/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [790/5817], Loss: 0.1916, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [800/5817], Loss: 0.1881, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [810/5817], Loss: 0.1816, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [820/5817], Loss: 0.2051, Accuracy: 0.7109\n",
      "Epoch [5/10], Step [830/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [840/5817], Loss: 0.1726, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [850/5817], Loss: 0.1864, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [860/5817], Loss: 0.1874, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [870/5817], Loss: 0.1707, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [880/5817], Loss: 0.1753, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [890/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [900/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [910/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [920/5817], Loss: 0.1601, Accuracy: 0.9141\n",
      "Epoch [5/10], Step [930/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [940/5817], Loss: 0.1895, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [950/5817], Loss: 0.1723, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [960/5817], Loss: 0.1847, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [970/5817], Loss: 0.2067, Accuracy: 0.7266\n",
      "Epoch [5/10], Step [980/5817], Loss: 0.1710, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [990/5817], Loss: 0.1856, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [1000/5817], Loss: 0.1858, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [1010/5817], Loss: 0.1925, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [1020/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [1030/5817], Loss: 0.1930, Accuracy: 0.7656\n",
      "Epoch [5/10], Step [1040/5817], Loss: 0.1656, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [1050/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [1060/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1070/5817], Loss: 0.1955, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [1080/5817], Loss: 0.1852, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1090/5817], Loss: 0.1812, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1100/5817], Loss: 0.1865, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [1110/5817], Loss: 0.1844, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1120/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [1130/5817], Loss: 0.1818, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1140/5817], Loss: 0.1880, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [1150/5817], Loss: 0.1771, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1160/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1170/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1180/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [1190/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1200/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1210/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [1220/5817], Loss: 0.1884, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [1230/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1240/5817], Loss: 0.1960, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [1250/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1260/5817], Loss: 0.1814, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1270/5817], Loss: 0.1727, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [1280/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [1290/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1300/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1310/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1320/5817], Loss: 0.1859, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1330/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1340/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1350/5817], Loss: 0.1676, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [1360/5817], Loss: 0.1665, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [1370/5817], Loss: 0.1877, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [1380/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [1390/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1400/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1410/5817], Loss: 0.1782, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [1420/5817], Loss: 0.1716, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [1430/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1440/5817], Loss: 0.1829, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [1450/5817], Loss: 0.1717, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [1460/5817], Loss: 0.1844, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [1470/5817], Loss: 0.1969, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [1480/5817], Loss: 0.1729, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [1490/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [1500/5817], Loss: 0.1877, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [1510/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1520/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [1530/5817], Loss: 0.1805, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1540/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1550/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [1560/5817], Loss: 0.1900, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [1570/5817], Loss: 0.1762, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [1580/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [1590/5817], Loss: 0.1850, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [1600/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [1610/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [1620/5817], Loss: 0.1872, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [1630/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1640/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1650/5817], Loss: 0.1905, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [1660/5817], Loss: 0.1645, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [1670/5817], Loss: 0.1807, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [1680/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1690/5817], Loss: 0.1872, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [1700/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [1710/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1720/5817], Loss: 0.1913, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [1730/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [1740/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1750/5817], Loss: 0.1856, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1760/5817], Loss: 0.1905, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [1770/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [1780/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1790/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [1800/5817], Loss: 0.1748, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [1810/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1820/5817], Loss: 0.1674, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [1830/5817], Loss: 0.1744, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1840/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1850/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1860/5817], Loss: 0.1686, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [1870/5817], Loss: 0.1860, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [1880/5817], Loss: 0.1875, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [1890/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1900/5817], Loss: 0.1893, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [1910/5817], Loss: 0.1697, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [1920/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [1930/5817], Loss: 0.1942, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [1940/5817], Loss: 0.1901, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [1950/5817], Loss: 0.1776, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [1960/5817], Loss: 0.1844, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [1970/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [1980/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1990/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [2000/5817], Loss: 0.1805, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [2010/5817], Loss: 0.1722, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2020/5817], Loss: 0.1682, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2030/5817], Loss: 0.1793, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [2040/5817], Loss: 0.1697, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [2050/5817], Loss: 0.1706, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [2060/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2070/5817], Loss: 0.1860, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [2080/5817], Loss: 0.1915, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [2090/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2100/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2110/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2120/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2130/5817], Loss: 0.1877, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [2140/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [2150/5817], Loss: 0.1870, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [2160/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [2170/5817], Loss: 0.1980, Accuracy: 0.7500\n",
      "Epoch [5/10], Step [2180/5817], Loss: 0.1816, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2190/5817], Loss: 0.1785, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2200/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2210/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2220/5817], Loss: 0.1673, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [2230/5817], Loss: 0.1764, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2240/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2250/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2260/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2270/5817], Loss: 0.1829, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2280/5817], Loss: 0.1884, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [2290/5817], Loss: 0.1860, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [2300/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [2310/5817], Loss: 0.1881, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [2320/5817], Loss: 0.1952, Accuracy: 0.7656\n",
      "Epoch [5/10], Step [2330/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2340/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [2350/5817], Loss: 0.1903, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [2360/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [2370/5817], Loss: 0.1893, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [2380/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [2390/5817], Loss: 0.1695, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [2400/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2410/5817], Loss: 0.1764, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2420/5817], Loss: 0.1679, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [2430/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [2440/5817], Loss: 0.1858, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [2450/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2460/5817], Loss: 0.1764, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2470/5817], Loss: 0.1819, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2480/5817], Loss: 0.1832, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [2490/5817], Loss: 0.1780, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2500/5817], Loss: 0.1742, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2510/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2520/5817], Loss: 0.1811, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2530/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2540/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2550/5817], Loss: 0.1847, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [2560/5817], Loss: 0.1963, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [2570/5817], Loss: 0.1991, Accuracy: 0.7422\n",
      "Epoch [5/10], Step [2580/5817], Loss: 0.1997, Accuracy: 0.7500\n",
      "Epoch [5/10], Step [2590/5817], Loss: 0.1866, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [2600/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [2610/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [2620/5817], Loss: 0.1778, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2630/5817], Loss: 0.1761, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2640/5817], Loss: 0.1909, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [2650/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2660/5817], Loss: 0.1688, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [2670/5817], Loss: 0.1705, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2680/5817], Loss: 0.1693, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [2690/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2700/5817], Loss: 0.1781, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2710/5817], Loss: 0.1674, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [2720/5817], Loss: 0.1714, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [2730/5817], Loss: 0.1831, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [2740/5817], Loss: 0.1886, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [2750/5817], Loss: 0.1967, Accuracy: 0.7344\n",
      "Epoch [5/10], Step [2760/5817], Loss: 0.1844, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [2770/5817], Loss: 0.1766, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2780/5817], Loss: 0.1767, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2790/5817], Loss: 0.1865, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [2800/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2810/5817], Loss: 0.1952, Accuracy: 0.7500\n",
      "Epoch [5/10], Step [2820/5817], Loss: 0.1909, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [2830/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2840/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2850/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2860/5817], Loss: 0.1838, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [2870/5817], Loss: 0.1976, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [2880/5817], Loss: 0.1864, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [2890/5817], Loss: 0.1763, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2900/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2910/5817], Loss: 0.1864, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [2920/5817], Loss: 0.1847, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [2930/5817], Loss: 0.1873, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [2940/5817], Loss: 0.1870, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [2950/5817], Loss: 0.1832, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2960/5817], Loss: 0.1924, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [2970/5817], Loss: 0.1932, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [2980/5817], Loss: 0.1822, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [2990/5817], Loss: 0.1780, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3000/5817], Loss: 0.1893, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [3010/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [3020/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3030/5817], Loss: 0.1727, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [3040/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3050/5817], Loss: 0.1921, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [3060/5817], Loss: 0.1758, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3070/5817], Loss: 0.1689, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [3080/5817], Loss: 0.1883, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [3090/5817], Loss: 0.1745, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [3100/5817], Loss: 0.1894, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [3110/5817], Loss: 0.1674, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [3120/5817], Loss: 0.1859, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [3130/5817], Loss: 0.1843, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [3140/5817], Loss: 0.1835, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [3150/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3160/5817], Loss: 0.1819, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3170/5817], Loss: 0.1792, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3180/5817], Loss: 0.1905, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [3190/5817], Loss: 0.1920, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [3200/5817], Loss: 0.1925, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [3210/5817], Loss: 0.1979, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [3220/5817], Loss: 0.1728, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [3230/5817], Loss: 0.1711, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [3240/5817], Loss: 0.1830, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3250/5817], Loss: 0.1765, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [3260/5817], Loss: 0.1954, Accuracy: 0.7500\n",
      "Epoch [5/10], Step [3270/5817], Loss: 0.1878, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [3280/5817], Loss: 0.1644, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [3290/5817], Loss: 0.1759, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [3300/5817], Loss: 0.1887, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [3310/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3320/5817], Loss: 0.1927, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [3330/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3340/5817], Loss: 0.1823, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [3350/5817], Loss: 0.1804, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [3360/5817], Loss: 0.1818, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3370/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [3380/5817], Loss: 0.1628, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [3390/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [3400/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3410/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [3420/5817], Loss: 0.1649, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [3430/5817], Loss: 0.1712, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [3440/5817], Loss: 0.1690, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [3450/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3460/5817], Loss: 0.1674, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [3470/5817], Loss: 0.1912, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [3480/5817], Loss: 0.1729, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [3490/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [3500/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [3510/5817], Loss: 0.1757, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [3520/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3530/5817], Loss: 0.1822, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [3540/5817], Loss: 0.1814, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [3550/5817], Loss: 0.1931, Accuracy: 0.7656\n",
      "Epoch [5/10], Step [3560/5817], Loss: 0.1944, Accuracy: 0.7656\n",
      "Epoch [5/10], Step [3570/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3580/5817], Loss: 0.1714, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [3590/5817], Loss: 0.1709, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [3600/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3610/5817], Loss: 0.1874, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [3620/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3630/5817], Loss: 0.1902, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [3640/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3650/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3660/5817], Loss: 0.1883, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [3670/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [3680/5817], Loss: 0.1874, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [3690/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3700/5817], Loss: 0.1900, Accuracy: 0.7656\n",
      "Epoch [5/10], Step [3710/5817], Loss: 0.1802, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [3720/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3730/5817], Loss: 0.1896, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [3740/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3750/5817], Loss: 0.1865, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [3760/5817], Loss: 0.1883, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [3770/5817], Loss: 0.1787, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3780/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3790/5817], Loss: 0.1784, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3800/5817], Loss: 0.1851, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [3810/5817], Loss: 0.1918, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [3820/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [3830/5817], Loss: 0.1919, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [3840/5817], Loss: 0.1827, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [3850/5817], Loss: 0.1855, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3860/5817], Loss: 0.1847, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [3870/5817], Loss: 0.1819, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3880/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3890/5817], Loss: 0.1931, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [3900/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3910/5817], Loss: 0.1724, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3920/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3930/5817], Loss: 0.1737, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [3940/5817], Loss: 0.1853, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [3950/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3960/5817], Loss: 0.1906, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [3970/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3980/5817], Loss: 0.1780, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3990/5817], Loss: 0.1897, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [4000/5817], Loss: 0.1814, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4010/5817], Loss: 0.1908, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4020/5817], Loss: 0.1809, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4030/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4040/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4050/5817], Loss: 0.1923, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [4060/5817], Loss: 0.1855, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4070/5817], Loss: 0.1872, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4080/5817], Loss: 0.1696, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [4090/5817], Loss: 0.1817, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4100/5817], Loss: 0.1724, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [4110/5817], Loss: 0.1863, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [4120/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4130/5817], Loss: 0.1792, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4140/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4150/5817], Loss: 0.1911, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [4160/5817], Loss: 0.1736, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [4170/5817], Loss: 0.1871, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4180/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [4190/5817], Loss: 0.1868, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [4200/5817], Loss: 0.1940, Accuracy: 0.7656\n",
      "Epoch [5/10], Step [4210/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4220/5817], Loss: 0.1688, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [4230/5817], Loss: 0.1892, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [4240/5817], Loss: 0.1800, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4250/5817], Loss: 0.1875, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [4260/5817], Loss: 0.1911, Accuracy: 0.7656\n",
      "Epoch [5/10], Step [4270/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4280/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4290/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [4300/5817], Loss: 0.1783, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4310/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4320/5817], Loss: 0.1879, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4330/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4340/5817], Loss: 0.1872, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4350/5817], Loss: 0.1848, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4360/5817], Loss: 0.1830, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [4370/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4380/5817], Loss: 0.1855, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [4390/5817], Loss: 0.1739, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [4400/5817], Loss: 0.1964, Accuracy: 0.7500\n",
      "Epoch [5/10], Step [4410/5817], Loss: 0.1945, Accuracy: 0.7656\n",
      "Epoch [5/10], Step [4420/5817], Loss: 0.1872, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4430/5817], Loss: 0.1884, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4440/5817], Loss: 0.1867, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [4450/5817], Loss: 0.1834, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [4460/5817], Loss: 0.1967, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [4470/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4480/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4490/5817], Loss: 0.1920, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [4500/5817], Loss: 0.1868, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4510/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4520/5817], Loss: 0.1917, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [4530/5817], Loss: 0.1950, Accuracy: 0.7656\n",
      "Epoch [5/10], Step [4540/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4550/5817], Loss: 0.1774, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [4560/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [4570/5817], Loss: 0.1748, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [4580/5817], Loss: 0.1856, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [4590/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4600/5817], Loss: 0.1902, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [4610/5817], Loss: 0.1884, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [4620/5817], Loss: 0.1797, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4630/5817], Loss: 0.1930, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [4640/5817], Loss: 0.1855, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4650/5817], Loss: 0.1916, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [4660/5817], Loss: 0.1886, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4670/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [4680/5817], Loss: 0.1855, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4690/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [4700/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4710/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4720/5817], Loss: 0.1723, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [4730/5817], Loss: 0.1848, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4740/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4750/5817], Loss: 0.1915, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [4760/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4770/5817], Loss: 0.1850, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4780/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [4790/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4800/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4810/5817], Loss: 0.2064, Accuracy: 0.7266\n",
      "Epoch [5/10], Step [4820/5817], Loss: 0.1850, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4830/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [4840/5817], Loss: 0.1954, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [4850/5817], Loss: 0.1787, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [4860/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4870/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4880/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [4890/5817], Loss: 0.1914, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [4900/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4910/5817], Loss: 0.2066, Accuracy: 0.7266\n",
      "Epoch [5/10], Step [4920/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [4930/5817], Loss: 0.1663, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [4940/5817], Loss: 0.1855, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4950/5817], Loss: 0.1848, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4960/5817], Loss: 0.1890, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [4970/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4980/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4990/5817], Loss: 0.1966, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [5000/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5010/5817], Loss: 0.1833, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [5020/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [5030/5817], Loss: 0.1765, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [5040/5817], Loss: 0.1937, Accuracy: 0.7500\n",
      "Epoch [5/10], Step [5050/5817], Loss: 0.1818, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5060/5817], Loss: 0.1804, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [5070/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5080/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [5090/5817], Loss: 0.1744, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5100/5817], Loss: 0.1845, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5110/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5120/5817], Loss: 0.1796, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5130/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5140/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [5150/5817], Loss: 0.1847, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [5160/5817], Loss: 0.1827, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5170/5817], Loss: 0.1864, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5180/5817], Loss: 0.1981, Accuracy: 0.7500\n",
      "Epoch [5/10], Step [5190/5817], Loss: 0.1892, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5200/5817], Loss: 0.1836, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5210/5817], Loss: 0.2097, Accuracy: 0.7031\n",
      "Epoch [5/10], Step [5220/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5230/5817], Loss: 0.1844, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5240/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [5250/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [5260/5817], Loss: 0.1874, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [5270/5817], Loss: 0.1725, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [5280/5817], Loss: 0.2005, Accuracy: 0.7266\n",
      "Epoch [5/10], Step [5290/5817], Loss: 0.1921, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [5300/5817], Loss: 0.1832, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [5310/5817], Loss: 0.1863, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [5320/5817], Loss: 0.1878, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [5330/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5340/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5350/5817], Loss: 0.1870, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5360/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5370/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5380/5817], Loss: 0.2010, Accuracy: 0.7422\n",
      "Epoch [5/10], Step [5390/5817], Loss: 0.1800, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5400/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5410/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5420/5817], Loss: 0.1923, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [5430/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5440/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [5450/5817], Loss: 0.1714, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [5460/5817], Loss: 0.1835, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [5470/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [5480/5817], Loss: 0.1713, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [5490/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [5500/5817], Loss: 0.1813, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5510/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5520/5817], Loss: 0.1836, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [5530/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5540/5817], Loss: 0.1859, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5550/5817], Loss: 0.1959, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [5560/5817], Loss: 0.1663, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [5570/5817], Loss: 0.1814, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [5580/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [5590/5817], Loss: 0.1776, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5600/5817], Loss: 0.1878, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5610/5817], Loss: 0.1952, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [5620/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5630/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5640/5817], Loss: 0.1720, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [5650/5817], Loss: 0.1861, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [5660/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5670/5817], Loss: 0.1879, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [5680/5817], Loss: 0.1823, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5690/5817], Loss: 0.1855, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [5700/5817], Loss: 0.1888, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [5710/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5720/5817], Loss: 0.1899, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [5730/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5740/5817], Loss: 0.1810, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5750/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [5760/5817], Loss: 0.1804, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5770/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5780/5817], Loss: 0.1829, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [5790/5817], Loss: 0.1860, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5800/5817], Loss: 0.1617, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [5810/5817], Loss: 0.1864, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [10/5817], Loss: 0.1850, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [20/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [30/5817], Loss: 0.1723, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [40/5817], Loss: 0.1845, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [50/5817], Loss: 0.1710, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [60/5817], Loss: 0.1901, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [70/5817], Loss: 0.1881, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [80/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [90/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [100/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [110/5817], Loss: 0.1667, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [120/5817], Loss: 0.1878, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [130/5817], Loss: 0.1859, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [140/5817], Loss: 0.1799, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [150/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [160/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [170/5817], Loss: 0.1692, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [180/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [190/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [200/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [210/5817], Loss: 0.1872, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [220/5817], Loss: 0.1823, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [230/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [240/5817], Loss: 0.1745, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [250/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [260/5817], Loss: 0.1631, Accuracy: 0.9062\n",
      "Epoch [6/10], Step [270/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [280/5817], Loss: 0.1747, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [290/5817], Loss: 0.1928, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [300/5817], Loss: 0.1882, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [310/5817], Loss: 0.1750, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [320/5817], Loss: 0.1828, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [330/5817], Loss: 0.1707, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [340/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [350/5817], Loss: 0.1914, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [360/5817], Loss: 0.1817, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [370/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [380/5817], Loss: 0.1794, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [390/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [400/5817], Loss: 0.1782, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [410/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [420/5817], Loss: 0.1703, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [430/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [440/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [450/5817], Loss: 0.1780, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [460/5817], Loss: 0.1814, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [470/5817], Loss: 0.1757, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [480/5817], Loss: 0.1780, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [490/5817], Loss: 0.1727, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [500/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [510/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [520/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [530/5817], Loss: 0.1750, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [540/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [550/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [560/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [570/5817], Loss: 0.1684, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [580/5817], Loss: 0.1676, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [590/5817], Loss: 0.1913, Accuracy: 0.7734\n",
      "Epoch [6/10], Step [600/5817], Loss: 0.1796, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [610/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [620/5817], Loss: 0.1907, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [630/5817], Loss: 0.1780, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [640/5817], Loss: 0.1719, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [650/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [660/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [670/5817], Loss: 0.1691, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [680/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [690/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [700/5817], Loss: 0.1748, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [710/5817], Loss: 0.1954, Accuracy: 0.7578\n",
      "Epoch [6/10], Step [720/5817], Loss: 0.1897, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [730/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [740/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [750/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [760/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [770/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [780/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [790/5817], Loss: 0.1954, Accuracy: 0.7656\n",
      "Epoch [6/10], Step [800/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [810/5817], Loss: 0.1774, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [820/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [830/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [840/5817], Loss: 0.1847, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [850/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [860/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [870/5817], Loss: 0.1728, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [880/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [890/5817], Loss: 0.1609, Accuracy: 0.9062\n",
      "Epoch [6/10], Step [900/5817], Loss: 0.1744, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [910/5817], Loss: 0.1788, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [920/5817], Loss: 0.1816, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [930/5817], Loss: 0.1826, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [940/5817], Loss: 0.1886, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [950/5817], Loss: 0.1830, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [960/5817], Loss: 0.1873, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [970/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [980/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [990/5817], Loss: 0.1860, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1000/5817], Loss: 0.1815, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1010/5817], Loss: 0.1850, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1020/5817], Loss: 0.1859, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1030/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [1040/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [1050/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [1060/5817], Loss: 0.1859, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1070/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1080/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1090/5817], Loss: 0.1825, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1100/5817], Loss: 0.1758, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1110/5817], Loss: 0.1864, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [1120/5817], Loss: 0.1842, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [1130/5817], Loss: 0.1756, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1140/5817], Loss: 0.1754, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1150/5817], Loss: 0.1705, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1160/5817], Loss: 0.1747, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [1170/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [1180/5817], Loss: 0.1709, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [1190/5817], Loss: 0.1924, Accuracy: 0.7734\n",
      "Epoch [6/10], Step [1200/5817], Loss: 0.1766, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1210/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1220/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1230/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1240/5817], Loss: 0.1766, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1250/5817], Loss: 0.1883, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [1260/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1270/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [1280/5817], Loss: 0.1918, Accuracy: 0.7734\n",
      "Epoch [6/10], Step [1290/5817], Loss: 0.1714, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1300/5817], Loss: 0.1733, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1310/5817], Loss: 0.1650, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [1320/5817], Loss: 0.1807, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1330/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1340/5817], Loss: 0.1790, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [1350/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [1360/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [1370/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [1380/5817], Loss: 0.1797, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [1390/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [1400/5817], Loss: 0.1897, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [1410/5817], Loss: 0.1713, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1420/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1430/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1440/5817], Loss: 0.1920, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [1450/5817], Loss: 0.1807, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [1460/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [1470/5817], Loss: 0.1695, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1480/5817], Loss: 0.1861, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1490/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [1500/5817], Loss: 0.1849, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1510/5817], Loss: 0.1867, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [1520/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1530/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [1540/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1550/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1560/5817], Loss: 0.1828, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [1570/5817], Loss: 0.1878, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [1580/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1590/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1600/5817], Loss: 0.1852, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1610/5817], Loss: 0.1814, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1620/5817], Loss: 0.1711, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1630/5817], Loss: 0.1808, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1640/5817], Loss: 0.1896, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [1650/5817], Loss: 0.1667, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [1660/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [1670/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1680/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [1690/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [1700/5817], Loss: 0.1871, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [1710/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [1720/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1730/5817], Loss: 0.1823, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [1740/5817], Loss: 0.1737, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1750/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [1760/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [1770/5817], Loss: 0.2032, Accuracy: 0.7422\n",
      "Epoch [6/10], Step [1780/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [1790/5817], Loss: 0.1809, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1800/5817], Loss: 0.1884, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [1810/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1820/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1830/5817], Loss: 0.1841, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [1840/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [1850/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [1860/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1870/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1880/5817], Loss: 0.1835, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [1890/5817], Loss: 0.1742, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [1900/5817], Loss: 0.1754, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1910/5817], Loss: 0.1741, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1920/5817], Loss: 0.1834, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1930/5817], Loss: 0.1886, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [1940/5817], Loss: 0.1785, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [1950/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1960/5817], Loss: 0.1893, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [1970/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [1980/5817], Loss: 0.1810, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [1990/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2000/5817], Loss: 0.1903, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [2010/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [2020/5817], Loss: 0.1927, Accuracy: 0.7656\n",
      "Epoch [6/10], Step [2030/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [2040/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [2050/5817], Loss: 0.1914, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2060/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [2070/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2080/5817], Loss: 0.1960, Accuracy: 0.7500\n",
      "Epoch [6/10], Step [2090/5817], Loss: 0.1813, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2100/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [2110/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [2120/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [2130/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2140/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [2150/5817], Loss: 0.2027, Accuracy: 0.7266\n",
      "Epoch [6/10], Step [2160/5817], Loss: 0.1801, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2170/5817], Loss: 0.1681, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [2180/5817], Loss: 0.1917, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [2190/5817], Loss: 0.1740, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2200/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2210/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2220/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2230/5817], Loss: 0.1710, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [2240/5817], Loss: 0.1855, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [2250/5817], Loss: 0.1710, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [2260/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2270/5817], Loss: 0.1858, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [2280/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2290/5817], Loss: 0.1826, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [2300/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2310/5817], Loss: 0.1873, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [2320/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2330/5817], Loss: 0.1886, Accuracy: 0.7656\n",
      "Epoch [6/10], Step [2340/5817], Loss: 0.1765, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [2350/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2360/5817], Loss: 0.1767, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [2370/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2380/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2390/5817], Loss: 0.1860, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [2400/5817], Loss: 0.1786, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2410/5817], Loss: 0.1823, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2420/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2430/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2440/5817], Loss: 0.1691, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [2450/5817], Loss: 0.1784, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2460/5817], Loss: 0.1829, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2470/5817], Loss: 0.1688, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [2480/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [2490/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2500/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2510/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2520/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2530/5817], Loss: 0.1714, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [2540/5817], Loss: 0.1958, Accuracy: 0.7734\n",
      "Epoch [6/10], Step [2550/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2560/5817], Loss: 0.1755, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2570/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [2580/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [2590/5817], Loss: 0.1776, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2600/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2610/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2620/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2630/5817], Loss: 0.1802, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2640/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [2650/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2660/5817], Loss: 0.1700, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [2670/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2680/5817], Loss: 0.1863, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [2690/5817], Loss: 0.1723, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [2700/5817], Loss: 0.1977, Accuracy: 0.7578\n",
      "Epoch [6/10], Step [2710/5817], Loss: 0.1808, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [2720/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [2730/5817], Loss: 0.1838, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [2740/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2750/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2760/5817], Loss: 0.1853, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2770/5817], Loss: 0.1828, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2780/5817], Loss: 0.1838, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2790/5817], Loss: 0.1900, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [2800/5817], Loss: 0.1760, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [2810/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2820/5817], Loss: 0.1823, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [2830/5817], Loss: 0.1693, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [2840/5817], Loss: 0.1863, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2850/5817], Loss: 0.1875, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2860/5817], Loss: 0.1905, Accuracy: 0.7656\n",
      "Epoch [6/10], Step [2870/5817], Loss: 0.1738, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2880/5817], Loss: 0.1924, Accuracy: 0.7734\n",
      "Epoch [6/10], Step [2890/5817], Loss: 0.1882, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [2900/5817], Loss: 0.1851, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [2910/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [2920/5817], Loss: 0.2042, Accuracy: 0.7344\n",
      "Epoch [6/10], Step [2930/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [2940/5817], Loss: 0.1847, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [2950/5817], Loss: 0.1742, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [2960/5817], Loss: 0.1954, Accuracy: 0.7734\n",
      "Epoch [6/10], Step [2970/5817], Loss: 0.1872, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2980/5817], Loss: 0.1833, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2990/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [3000/5817], Loss: 0.1755, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3010/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3020/5817], Loss: 0.1814, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [3030/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3040/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3050/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [3060/5817], Loss: 0.1709, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [3070/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3080/5817], Loss: 0.1761, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3090/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3100/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3110/5817], Loss: 0.1846, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [3120/5817], Loss: 0.1803, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [3130/5817], Loss: 0.1702, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [3140/5817], Loss: 0.1704, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [3150/5817], Loss: 0.1844, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [3160/5817], Loss: 0.1819, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3170/5817], Loss: 0.1827, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [3180/5817], Loss: 0.1942, Accuracy: 0.7578\n",
      "Epoch [6/10], Step [3190/5817], Loss: 0.1704, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [3200/5817], Loss: 0.1731, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [3210/5817], Loss: 0.1782, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3220/5817], Loss: 0.1814, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [3230/5817], Loss: 0.1936, Accuracy: 0.7578\n",
      "Epoch [6/10], Step [3240/5817], Loss: 0.1678, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [3250/5817], Loss: 0.1880, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [3260/5817], Loss: 0.1802, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3270/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3280/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [3290/5817], Loss: 0.1731, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [3300/5817], Loss: 0.1943, Accuracy: 0.7578\n",
      "Epoch [6/10], Step [3310/5817], Loss: 0.1847, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [3320/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [3330/5817], Loss: 0.1822, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3340/5817], Loss: 0.2012, Accuracy: 0.7344\n",
      "Epoch [6/10], Step [3350/5817], Loss: 0.1828, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [3360/5817], Loss: 0.1728, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [3370/5817], Loss: 0.1899, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [3380/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [3390/5817], Loss: 0.1911, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [3400/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3410/5817], Loss: 0.1822, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3420/5817], Loss: 0.1955, Accuracy: 0.7656\n",
      "Epoch [6/10], Step [3430/5817], Loss: 0.1948, Accuracy: 0.7578\n",
      "Epoch [6/10], Step [3440/5817], Loss: 0.1738, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [3450/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [3460/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3470/5817], Loss: 0.1833, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [3480/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3490/5817], Loss: 0.1854, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [3500/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3510/5817], Loss: 0.1935, Accuracy: 0.7656\n",
      "Epoch [6/10], Step [3520/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3530/5817], Loss: 0.1888, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [3540/5817], Loss: 0.1901, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [3550/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3560/5817], Loss: 0.1743, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3570/5817], Loss: 0.1698, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [3580/5817], Loss: 0.1906, Accuracy: 0.7734\n",
      "Epoch [6/10], Step [3590/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [3600/5817], Loss: 0.1879, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [3610/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3620/5817], Loss: 0.1801, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3630/5817], Loss: 0.1845, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [3640/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [3650/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3660/5817], Loss: 0.1903, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [3670/5817], Loss: 0.1917, Accuracy: 0.7656\n",
      "Epoch [6/10], Step [3680/5817], Loss: 0.1774, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3690/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3700/5817], Loss: 0.1902, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [3710/5817], Loss: 0.1760, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3720/5817], Loss: 0.1681, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [3730/5817], Loss: 0.1874, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [3740/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3750/5817], Loss: 0.1900, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [3760/5817], Loss: 0.1954, Accuracy: 0.7656\n",
      "Epoch [6/10], Step [3770/5817], Loss: 0.1795, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [3780/5817], Loss: 0.1906, Accuracy: 0.7734\n",
      "Epoch [6/10], Step [3790/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [3800/5817], Loss: 0.1738, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [3810/5817], Loss: 0.1879, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [3820/5817], Loss: 0.1812, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [3830/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [3840/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [3850/5817], Loss: 0.1929, Accuracy: 0.7734\n",
      "Epoch [6/10], Step [3860/5817], Loss: 0.1966, Accuracy: 0.7656\n",
      "Epoch [6/10], Step [3870/5817], Loss: 0.1780, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3880/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [3890/5817], Loss: 0.1937, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [3900/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3910/5817], Loss: 0.1897, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [3920/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [3930/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3940/5817], Loss: 0.1845, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [3950/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [3960/5817], Loss: 0.1892, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [3970/5817], Loss: 0.1797, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [3980/5817], Loss: 0.1835, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [3990/5817], Loss: 0.1980, Accuracy: 0.7500\n",
      "Epoch [6/10], Step [4000/5817], Loss: 0.1843, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [4010/5817], Loss: 0.1833, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [4020/5817], Loss: 0.1981, Accuracy: 0.7578\n",
      "Epoch [6/10], Step [4030/5817], Loss: 0.1654, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [4040/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4050/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4060/5817], Loss: 0.1686, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [4070/5817], Loss: 0.1883, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [4080/5817], Loss: 0.1950, Accuracy: 0.7578\n",
      "Epoch [6/10], Step [4090/5817], Loss: 0.1820, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4100/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4110/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4120/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4130/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [4140/5817], Loss: 0.1912, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [4150/5817], Loss: 0.1887, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [4160/5817], Loss: 0.1811, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [4170/5817], Loss: 0.1771, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4180/5817], Loss: 0.1732, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4190/5817], Loss: 0.1900, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [4200/5817], Loss: 0.1864, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [4210/5817], Loss: 0.1732, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4220/5817], Loss: 0.1885, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [4230/5817], Loss: 0.1779, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4240/5817], Loss: 0.1809, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4250/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4260/5817], Loss: 0.1749, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4270/5817], Loss: 0.1914, Accuracy: 0.7656\n",
      "Epoch [6/10], Step [4280/5817], Loss: 0.1911, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [4290/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4300/5817], Loss: 0.1686, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [4310/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4320/5817], Loss: 0.1948, Accuracy: 0.7656\n",
      "Epoch [6/10], Step [4330/5817], Loss: 0.1647, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [4340/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4350/5817], Loss: 0.1879, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [4360/5817], Loss: 0.1718, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4370/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [4380/5817], Loss: 0.1791, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4390/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4400/5817], Loss: 0.1846, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [4410/5817], Loss: 0.1910, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [4420/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4430/5817], Loss: 0.1971, Accuracy: 0.7578\n",
      "Epoch [6/10], Step [4440/5817], Loss: 0.1767, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4450/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4460/5817], Loss: 0.1720, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4470/5817], Loss: 0.1809, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4480/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4490/5817], Loss: 0.1800, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [4500/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4510/5817], Loss: 0.1704, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [4520/5817], Loss: 0.1874, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [4530/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [4540/5817], Loss: 0.1780, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4550/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4560/5817], Loss: 0.1880, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [4570/5817], Loss: 0.1809, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4580/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [4590/5817], Loss: 0.1815, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [4600/5817], Loss: 0.1996, Accuracy: 0.7422\n",
      "Epoch [6/10], Step [4610/5817], Loss: 0.1693, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [4620/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4630/5817], Loss: 0.1851, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [4640/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [4650/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4660/5817], Loss: 0.1913, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [4670/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4680/5817], Loss: 0.1669, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [4690/5817], Loss: 0.1706, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4700/5817], Loss: 0.1655, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [4710/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4720/5817], Loss: 0.1843, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [4730/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [4740/5817], Loss: 0.1788, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4750/5817], Loss: 0.1840, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [4760/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [4770/5817], Loss: 0.1665, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [4780/5817], Loss: 0.1878, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [4790/5817], Loss: 0.1743, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4800/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4810/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4820/5817], Loss: 0.1623, Accuracy: 0.9062\n",
      "Epoch [6/10], Step [4830/5817], Loss: 0.1911, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [4840/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4850/5817], Loss: 0.1863, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [4860/5817], Loss: 0.1630, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [4870/5817], Loss: 0.1819, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [4880/5817], Loss: 0.1703, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [4890/5817], Loss: 0.1870, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [4900/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4910/5817], Loss: 0.1682, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4920/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4930/5817], Loss: 0.1744, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [4940/5817], Loss: 0.1871, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [4950/5817], Loss: 0.1788, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [4960/5817], Loss: 0.1677, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [4970/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4980/5817], Loss: 0.1701, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4990/5817], Loss: 0.1982, Accuracy: 0.7656\n",
      "Epoch [6/10], Step [5000/5817], Loss: 0.1880, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [5010/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5020/5817], Loss: 0.1973, Accuracy: 0.7500\n",
      "Epoch [6/10], Step [5030/5817], Loss: 0.1780, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5040/5817], Loss: 0.1898, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [5050/5817], Loss: 0.1715, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [5060/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [5070/5817], Loss: 0.1850, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [5080/5817], Loss: 0.1772, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5090/5817], Loss: 0.1876, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [5100/5817], Loss: 0.1718, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [5110/5817], Loss: 0.1868, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [5120/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [5130/5817], Loss: 0.1647, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [5140/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5150/5817], Loss: 0.1825, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5160/5817], Loss: 0.1830, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5170/5817], Loss: 0.1838, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5180/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5190/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [5200/5817], Loss: 0.1800, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5210/5817], Loss: 0.1880, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [5220/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [5230/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5240/5817], Loss: 0.1794, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5250/5817], Loss: 0.1834, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5260/5817], Loss: 0.1745, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [5270/5817], Loss: 0.1834, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [5280/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [5290/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5300/5817], Loss: 0.1796, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5310/5817], Loss: 0.1780, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [5320/5817], Loss: 0.1857, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [5330/5817], Loss: 0.1852, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5340/5817], Loss: 0.1843, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [5350/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [5360/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5370/5817], Loss: 0.1905, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [5380/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5390/5817], Loss: 0.1851, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [5400/5817], Loss: 0.1936, Accuracy: 0.7734\n",
      "Epoch [6/10], Step [5410/5817], Loss: 0.1877, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [5420/5817], Loss: 0.1872, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [5430/5817], Loss: 0.1797, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5440/5817], Loss: 0.1661, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [5450/5817], Loss: 0.1676, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [5460/5817], Loss: 0.1886, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [5470/5817], Loss: 0.1915, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [5480/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5490/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5500/5817], Loss: 0.1700, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [5510/5817], Loss: 0.1823, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5520/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5530/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5540/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [5550/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5560/5817], Loss: 0.1722, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [5570/5817], Loss: 0.1747, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5580/5817], Loss: 0.1732, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [5590/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [5600/5817], Loss: 0.1692, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [5610/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [5620/5817], Loss: 0.1880, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [5630/5817], Loss: 0.1879, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [5640/5817], Loss: 0.1816, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5650/5817], Loss: 0.1867, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [5660/5817], Loss: 0.1786, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5670/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [5680/5817], Loss: 0.1659, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [5690/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5700/5817], Loss: 0.1828, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5710/5817], Loss: 0.1745, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [5720/5817], Loss: 0.1893, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [5730/5817], Loss: 0.1832, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5740/5817], Loss: 0.1900, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [5750/5817], Loss: 0.1676, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [5760/5817], Loss: 0.1779, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5770/5817], Loss: 0.1753, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [5780/5817], Loss: 0.1833, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [5790/5817], Loss: 0.1863, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5800/5817], Loss: 0.1835, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5810/5817], Loss: 0.1966, Accuracy: 0.7656\n",
      "Epoch [7/10], Step [10/5817], Loss: 0.1850, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [20/5817], Loss: 0.1884, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [30/5817], Loss: 0.1643, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [40/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [50/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [60/5817], Loss: 0.1703, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [70/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [80/5817], Loss: 0.1708, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [90/5817], Loss: 0.1842, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [100/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [110/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [120/5817], Loss: 0.1847, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [130/5817], Loss: 0.1776, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [140/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [150/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [160/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [170/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [180/5817], Loss: 0.1706, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [190/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [200/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [210/5817], Loss: 0.1747, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [220/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [230/5817], Loss: 0.1886, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [240/5817], Loss: 0.1840, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [250/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [260/5817], Loss: 0.1673, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [270/5817], Loss: 0.1714, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [280/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [290/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [300/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [310/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [320/5817], Loss: 0.1868, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [330/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [340/5817], Loss: 0.1740, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [350/5817], Loss: 0.1892, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [360/5817], Loss: 0.1688, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [370/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [380/5817], Loss: 0.1760, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [390/5817], Loss: 0.1907, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [400/5817], Loss: 0.1805, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [410/5817], Loss: 0.1853, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [420/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [430/5817], Loss: 0.1899, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [440/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [450/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [460/5817], Loss: 0.1620, Accuracy: 0.9141\n",
      "Epoch [7/10], Step [470/5817], Loss: 0.1678, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [480/5817], Loss: 0.1626, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [490/5817], Loss: 0.1663, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [500/5817], Loss: 0.1690, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [510/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [520/5817], Loss: 0.1858, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [530/5817], Loss: 0.1823, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [540/5817], Loss: 0.1701, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [550/5817], Loss: 0.1685, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [560/5817], Loss: 0.1899, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [570/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [580/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [590/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [600/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [610/5817], Loss: 0.1622, Accuracy: 0.9141\n",
      "Epoch [7/10], Step [620/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [630/5817], Loss: 0.1796, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [640/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [650/5817], Loss: 0.1672, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [660/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [670/5817], Loss: 0.1873, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [680/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [690/5817], Loss: 0.1817, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [700/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [710/5817], Loss: 0.1945, Accuracy: 0.7734\n",
      "Epoch [7/10], Step [720/5817], Loss: 0.1680, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [730/5817], Loss: 0.1786, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [740/5817], Loss: 0.1906, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [750/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [760/5817], Loss: 0.1756, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [770/5817], Loss: 0.1867, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [780/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [790/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [800/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [810/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [820/5817], Loss: 0.1839, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [830/5817], Loss: 0.1876, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [840/5817], Loss: 0.1726, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [850/5817], Loss: 0.1726, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [860/5817], Loss: 0.1894, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [870/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [880/5817], Loss: 0.1867, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [890/5817], Loss: 0.1694, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [900/5817], Loss: 0.1751, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [910/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [920/5817], Loss: 0.1931, Accuracy: 0.7734\n",
      "Epoch [7/10], Step [930/5817], Loss: 0.1897, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [940/5817], Loss: 0.1734, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [950/5817], Loss: 0.1858, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [960/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [970/5817], Loss: 0.1877, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [980/5817], Loss: 0.1762, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [990/5817], Loss: 0.1873, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [1000/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [1010/5817], Loss: 0.1737, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [1020/5817], Loss: 0.1802, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1030/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1040/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1050/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1060/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [1070/5817], Loss: 0.1904, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [1080/5817], Loss: 0.1864, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [1090/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [1100/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [1110/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [1120/5817], Loss: 0.2033, Accuracy: 0.7344\n",
      "Epoch [7/10], Step [1130/5817], Loss: 0.1767, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1140/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1150/5817], Loss: 0.1907, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [1160/5817], Loss: 0.1678, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [1170/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1180/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [1190/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [1200/5817], Loss: 0.1873, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [1210/5817], Loss: 0.1839, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [1220/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1230/5817], Loss: 0.1633, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [1240/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1250/5817], Loss: 0.1653, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [1260/5817], Loss: 0.1816, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1270/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1280/5817], Loss: 0.1705, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [1290/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [1300/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1310/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1320/5817], Loss: 0.1966, Accuracy: 0.7656\n",
      "Epoch [7/10], Step [1330/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1340/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1350/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1360/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [1370/5817], Loss: 0.1816, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1380/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [1390/5817], Loss: 0.1656, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [1400/5817], Loss: 0.1791, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [1410/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1420/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1430/5817], Loss: 0.1759, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1440/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [1450/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1460/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [1470/5817], Loss: 0.1887, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [1480/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [1490/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1500/5817], Loss: 0.1802, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1510/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [1520/5817], Loss: 0.1720, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1530/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1540/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [1550/5817], Loss: 0.1782, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1560/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1570/5817], Loss: 0.1803, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1580/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [1590/5817], Loss: 0.1848, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1600/5817], Loss: 0.1900, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [1610/5817], Loss: 0.1888, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [1620/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1630/5817], Loss: 0.1809, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1640/5817], Loss: 0.1902, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [1650/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1660/5817], Loss: 0.1927, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [1670/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [1680/5817], Loss: 0.1912, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [1690/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1700/5817], Loss: 0.1681, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [1710/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1720/5817], Loss: 0.1871, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [1730/5817], Loss: 0.1872, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [1740/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1750/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1760/5817], Loss: 0.1837, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1770/5817], Loss: 0.1861, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [1780/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1790/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [1800/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [1810/5817], Loss: 0.1784, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1820/5817], Loss: 0.1823, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1830/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1840/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1850/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [1860/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1870/5817], Loss: 0.1851, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [1880/5817], Loss: 0.1902, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [1890/5817], Loss: 0.1941, Accuracy: 0.7656\n",
      "Epoch [7/10], Step [1900/5817], Loss: 0.1852, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [1910/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [1920/5817], Loss: 0.1945, Accuracy: 0.7656\n",
      "Epoch [7/10], Step [1930/5817], Loss: 0.1812, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [1940/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [1950/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [1960/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [1970/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1980/5817], Loss: 0.1733, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [1990/5817], Loss: 0.1562, Accuracy: 0.9375\n",
      "Epoch [7/10], Step [2000/5817], Loss: 0.1807, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2010/5817], Loss: 0.1914, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [2020/5817], Loss: 0.1889, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2030/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [2040/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [2050/5817], Loss: 0.1682, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [2060/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [2070/5817], Loss: 0.1899, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [2080/5817], Loss: 0.1902, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [2090/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [2100/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2110/5817], Loss: 0.1880, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [2120/5817], Loss: 0.1876, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [2130/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2140/5817], Loss: 0.1686, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [2150/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [2160/5817], Loss: 0.1803, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2170/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2180/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [2190/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2200/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2210/5817], Loss: 0.1865, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2220/5817], Loss: 0.1737, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [2230/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [2240/5817], Loss: 0.1817, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2250/5817], Loss: 0.1857, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [2260/5817], Loss: 0.1916, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [2270/5817], Loss: 0.1859, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2280/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [2290/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [2300/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [2310/5817], Loss: 0.1688, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [2320/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2330/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2340/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2350/5817], Loss: 0.1871, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2360/5817], Loss: 0.1898, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [2370/5817], Loss: 0.1907, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [2380/5817], Loss: 0.1868, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [2390/5817], Loss: 0.1883, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [2400/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [2410/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2420/5817], Loss: 0.1870, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [2430/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [2440/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2450/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2460/5817], Loss: 0.1839, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2470/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2480/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [2490/5817], Loss: 0.1828, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2500/5817], Loss: 0.1889, Accuracy: 0.7734\n",
      "Epoch [7/10], Step [2510/5817], Loss: 0.1812, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2520/5817], Loss: 0.1845, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2530/5817], Loss: 0.1993, Accuracy: 0.7500\n",
      "Epoch [7/10], Step [2540/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2550/5817], Loss: 0.1998, Accuracy: 0.7500\n",
      "Epoch [7/10], Step [2560/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2570/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2580/5817], Loss: 0.1691, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [2590/5817], Loss: 0.1867, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2600/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [2610/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2620/5817], Loss: 0.1875, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2630/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2640/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [2650/5817], Loss: 0.1917, Accuracy: 0.7734\n",
      "Epoch [7/10], Step [2660/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [2670/5817], Loss: 0.1808, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2680/5817], Loss: 0.1689, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [2690/5817], Loss: 0.1660, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [2700/5817], Loss: 0.1664, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [2710/5817], Loss: 0.1870, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [2720/5817], Loss: 0.1841, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2730/5817], Loss: 0.1849, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2740/5817], Loss: 0.1867, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2750/5817], Loss: 0.1835, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2760/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [2770/5817], Loss: 0.1868, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [2780/5817], Loss: 0.1850, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [2790/5817], Loss: 0.1664, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [2800/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [2810/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2820/5817], Loss: 0.1953, Accuracy: 0.7656\n",
      "Epoch [7/10], Step [2830/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [2840/5817], Loss: 0.1864, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [2850/5817], Loss: 0.2015, Accuracy: 0.7422\n",
      "Epoch [7/10], Step [2860/5817], Loss: 0.1971, Accuracy: 0.7500\n",
      "Epoch [7/10], Step [2870/5817], Loss: 0.1814, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2880/5817], Loss: 0.1821, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2890/5817], Loss: 0.1817, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2900/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2910/5817], Loss: 0.1935, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [2920/5817], Loss: 0.1730, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [2930/5817], Loss: 0.1649, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [2940/5817], Loss: 0.1823, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2950/5817], Loss: 0.1711, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [2960/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2970/5817], Loss: 0.1803, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2980/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2990/5817], Loss: 0.1791, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3000/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3010/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [3020/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3030/5817], Loss: 0.1848, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [3040/5817], Loss: 0.1715, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3050/5817], Loss: 0.1660, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [3060/5817], Loss: 0.1857, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [3070/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3080/5817], Loss: 0.1678, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [3090/5817], Loss: 0.1799, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [3100/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [3110/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3120/5817], Loss: 0.1687, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [3130/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3140/5817], Loss: 0.1907, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [3150/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3160/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [3170/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3180/5817], Loss: 0.1900, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [3190/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [3200/5817], Loss: 0.1906, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [3210/5817], Loss: 0.1769, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3220/5817], Loss: 0.1742, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3230/5817], Loss: 0.1697, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [3240/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [3250/5817], Loss: 0.1679, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [3260/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [3270/5817], Loss: 0.1653, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [3280/5817], Loss: 0.1703, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [3290/5817], Loss: 0.1948, Accuracy: 0.7734\n",
      "Epoch [7/10], Step [3300/5817], Loss: 0.1706, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [3310/5817], Loss: 0.1737, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3320/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3330/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3340/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [3350/5817], Loss: 0.1874, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [3360/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3370/5817], Loss: 0.1893, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [3380/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3390/5817], Loss: 0.1797, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [3400/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [3410/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [3420/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3430/5817], Loss: 0.1864, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [3440/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [3450/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [3460/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3470/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [3480/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [3490/5817], Loss: 0.1843, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3500/5817], Loss: 0.1875, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3510/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [3520/5817], Loss: 0.1822, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3530/5817], Loss: 0.1722, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [3540/5817], Loss: 0.1776, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [3550/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3560/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3570/5817], Loss: 0.1840, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [3580/5817], Loss: 0.1830, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3590/5817], Loss: 0.1917, Accuracy: 0.7734\n",
      "Epoch [7/10], Step [3600/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [3610/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3620/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3630/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3640/5817], Loss: 0.1848, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [3650/5817], Loss: 0.2034, Accuracy: 0.7266\n",
      "Epoch [7/10], Step [3660/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [3670/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3680/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3690/5817], Loss: 0.1759, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3700/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [3710/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [3720/5817], Loss: 0.1863, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [3730/5817], Loss: 0.1869, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [3740/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3750/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3760/5817], Loss: 0.1619, Accuracy: 0.9062\n",
      "Epoch [7/10], Step [3770/5817], Loss: 0.1707, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3780/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3790/5817], Loss: 0.1863, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [3800/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3810/5817], Loss: 0.1841, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [3820/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [3830/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3840/5817], Loss: 0.1911, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [3850/5817], Loss: 0.1871, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [3860/5817], Loss: 0.1677, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [3870/5817], Loss: 0.1884, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [3880/5817], Loss: 0.1754, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3890/5817], Loss: 0.1890, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [3900/5817], Loss: 0.1737, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3910/5817], Loss: 0.1724, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3920/5817], Loss: 0.1729, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3930/5817], Loss: 0.1830, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3940/5817], Loss: 0.1899, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [3950/5817], Loss: 0.1941, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [3960/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3970/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [3980/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [3990/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4000/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4010/5817], Loss: 0.1878, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [4020/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4030/5817], Loss: 0.1823, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4040/5817], Loss: 0.1807, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4050/5817], Loss: 0.1748, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4060/5817], Loss: 0.1745, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4070/5817], Loss: 0.1969, Accuracy: 0.7578\n",
      "Epoch [7/10], Step [4080/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4090/5817], Loss: 0.1849, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [4100/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [4110/5817], Loss: 0.1836, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [4120/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [4130/5817], Loss: 0.1759, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4140/5817], Loss: 0.1783, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [4150/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4160/5817], Loss: 0.1823, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4170/5817], Loss: 0.1754, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4180/5817], Loss: 0.1681, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [4190/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4200/5817], Loss: 0.1852, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [4210/5817], Loss: 0.1760, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4220/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4230/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4240/5817], Loss: 0.1916, Accuracy: 0.7734\n",
      "Epoch [7/10], Step [4250/5817], Loss: 0.1782, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4260/5817], Loss: 0.1697, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [4270/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [4280/5817], Loss: 0.1769, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4290/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [4300/5817], Loss: 0.1852, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [4310/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [4320/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [4330/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [4340/5817], Loss: 0.1965, Accuracy: 0.7656\n",
      "Epoch [7/10], Step [4350/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4360/5817], Loss: 0.1778, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4370/5817], Loss: 0.1769, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4380/5817], Loss: 0.1896, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [4390/5817], Loss: 0.1916, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [4400/5817], Loss: 0.1860, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [4410/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4420/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4430/5817], Loss: 0.1678, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [4440/5817], Loss: 0.1769, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4450/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [4460/5817], Loss: 0.1904, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [4470/5817], Loss: 0.1840, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4480/5817], Loss: 0.1915, Accuracy: 0.7734\n",
      "Epoch [7/10], Step [4490/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [4500/5817], Loss: 0.1759, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4510/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [4520/5817], Loss: 0.1898, Accuracy: 0.7734\n",
      "Epoch [7/10], Step [4530/5817], Loss: 0.1853, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [4540/5817], Loss: 0.1838, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [4550/5817], Loss: 0.1847, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [4560/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4570/5817], Loss: 0.1892, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [4580/5817], Loss: 0.1748, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [4590/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4600/5817], Loss: 0.1924, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [4610/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4620/5817], Loss: 0.1808, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [4630/5817], Loss: 0.1806, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4640/5817], Loss: 0.1839, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4650/5817], Loss: 0.1830, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4660/5817], Loss: 0.1781, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4670/5817], Loss: 0.2001, Accuracy: 0.7344\n",
      "Epoch [7/10], Step [4680/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4690/5817], Loss: 0.1956, Accuracy: 0.7656\n",
      "Epoch [7/10], Step [4700/5817], Loss: 0.1666, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [4710/5817], Loss: 0.1713, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4720/5817], Loss: 0.1899, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [4730/5817], Loss: 0.1796, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4740/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [4750/5817], Loss: 0.1643, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [4760/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4770/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4780/5817], Loss: 0.1963, Accuracy: 0.7656\n",
      "Epoch [7/10], Step [4790/5817], Loss: 0.1824, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [4800/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4810/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [4820/5817], Loss: 0.1754, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4830/5817], Loss: 0.1834, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4840/5817], Loss: 0.1874, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [4850/5817], Loss: 0.1802, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4860/5817], Loss: 0.2007, Accuracy: 0.7500\n",
      "Epoch [7/10], Step [4870/5817], Loss: 0.1682, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [4880/5817], Loss: 0.1912, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [4890/5817], Loss: 0.1953, Accuracy: 0.7656\n",
      "Epoch [7/10], Step [4900/5817], Loss: 0.1634, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [4910/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [4920/5817], Loss: 0.1915, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [4930/5817], Loss: 0.1760, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4940/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [4950/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4960/5817], Loss: 0.1865, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [4970/5817], Loss: 0.1825, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4980/5817], Loss: 0.1677, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [4990/5817], Loss: 0.1779, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5000/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [5010/5817], Loss: 0.1740, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5020/5817], Loss: 0.1673, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5030/5817], Loss: 0.1851, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [5040/5817], Loss: 0.1838, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [5050/5817], Loss: 0.1861, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [5060/5817], Loss: 0.1748, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [5070/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [5080/5817], Loss: 0.1683, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [5090/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [5100/5817], Loss: 0.1819, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [5110/5817], Loss: 0.1829, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [5120/5817], Loss: 0.1659, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [5130/5817], Loss: 0.1805, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [5140/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [5150/5817], Loss: 0.1707, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [5160/5817], Loss: 0.1709, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5170/5817], Loss: 0.1646, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [5180/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5190/5817], Loss: 0.1825, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [5200/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5210/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [5220/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5230/5817], Loss: 0.1830, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [5240/5817], Loss: 0.1876, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [5250/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5260/5817], Loss: 0.1750, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [5270/5817], Loss: 0.1812, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [5280/5817], Loss: 0.1935, Accuracy: 0.7656\n",
      "Epoch [7/10], Step [5290/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [5300/5817], Loss: 0.1887, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [5310/5817], Loss: 0.1919, Accuracy: 0.7734\n",
      "Epoch [7/10], Step [5320/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5330/5817], Loss: 0.1898, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [5340/5817], Loss: 0.1784, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [5350/5817], Loss: 0.1695, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5360/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [5370/5817], Loss: 0.1671, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5380/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [5390/5817], Loss: 0.1854, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [5400/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5410/5817], Loss: 0.1722, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [5420/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [5430/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5440/5817], Loss: 0.1685, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [5450/5817], Loss: 0.1784, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5460/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [5470/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [5480/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [5490/5817], Loss: 0.1756, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [5500/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5510/5817], Loss: 0.1705, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5520/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5530/5817], Loss: 0.1856, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [5540/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [5550/5817], Loss: 0.1839, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [5560/5817], Loss: 0.1860, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [5570/5817], Loss: 0.1821, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [5580/5817], Loss: 0.1761, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [5590/5817], Loss: 0.1890, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [5600/5817], Loss: 0.1744, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5610/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [5620/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [5630/5817], Loss: 0.1918, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [5640/5817], Loss: 0.1893, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [5650/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [5660/5817], Loss: 0.1689, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [5670/5817], Loss: 0.1749, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5680/5817], Loss: 0.1775, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [5690/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5700/5817], Loss: 0.1718, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [5710/5817], Loss: 0.1671, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [5720/5817], Loss: 0.1826, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [5730/5817], Loss: 0.1876, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [5740/5817], Loss: 0.1837, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [5750/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5760/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5770/5817], Loss: 0.1749, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5780/5817], Loss: 0.1939, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [5790/5817], Loss: 0.1715, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [5800/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5810/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [10/5817], Loss: 0.1749, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [20/5817], Loss: 0.1689, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [30/5817], Loss: 0.1873, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [40/5817], Loss: 0.1706, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [50/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [60/5817], Loss: 0.1758, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [70/5817], Loss: 0.1804, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [80/5817], Loss: 0.1930, Accuracy: 0.7734\n",
      "Epoch [8/10], Step [90/5817], Loss: 0.1712, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [100/5817], Loss: 0.1845, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [110/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [120/5817], Loss: 0.1874, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [130/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [140/5817], Loss: 0.1683, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [150/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [160/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [170/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [180/5817], Loss: 0.1589, Accuracy: 0.9219\n",
      "Epoch [8/10], Step [190/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [200/5817], Loss: 0.1945, Accuracy: 0.7734\n",
      "Epoch [8/10], Step [210/5817], Loss: 0.1704, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [220/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [230/5817], Loss: 0.1726, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [240/5817], Loss: 0.1663, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [250/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [260/5817], Loss: 0.1644, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [270/5817], Loss: 0.1698, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [280/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [290/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [300/5817], Loss: 0.1799, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [310/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [320/5817], Loss: 0.1823, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [330/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [340/5817], Loss: 0.1788, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [350/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [360/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [370/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [380/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [390/5817], Loss: 0.1681, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [400/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [410/5817], Loss: 0.1883, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [420/5817], Loss: 0.1906, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [430/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [440/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [450/5817], Loss: 0.1871, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [460/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [470/5817], Loss: 0.1852, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [480/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [490/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [500/5817], Loss: 0.1876, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [510/5817], Loss: 0.1789, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [520/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [530/5817], Loss: 0.1727, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [540/5817], Loss: 0.1811, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [550/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [560/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [570/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [580/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [590/5817], Loss: 0.1685, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [600/5817], Loss: 0.1673, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [610/5817], Loss: 0.1616, Accuracy: 0.9141\n",
      "Epoch [8/10], Step [620/5817], Loss: 0.1805, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [630/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [640/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [650/5817], Loss: 0.1677, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [660/5817], Loss: 0.1848, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [670/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [680/5817], Loss: 0.1731, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [690/5817], Loss: 0.1689, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [700/5817], Loss: 0.1989, Accuracy: 0.7578\n",
      "Epoch [8/10], Step [710/5817], Loss: 0.1723, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [720/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [730/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [740/5817], Loss: 0.1851, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [750/5817], Loss: 0.1707, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [760/5817], Loss: 0.1726, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [770/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [780/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [790/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [800/5817], Loss: 0.1780, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [810/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [820/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [830/5817], Loss: 0.1662, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [840/5817], Loss: 0.1664, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [850/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [860/5817], Loss: 0.1829, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [870/5817], Loss: 0.1739, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [880/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [890/5817], Loss: 0.1810, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [900/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [910/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [920/5817], Loss: 0.1909, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [930/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [940/5817], Loss: 0.1885, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [950/5817], Loss: 0.1893, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [960/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [970/5817], Loss: 0.1856, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [980/5817], Loss: 0.1726, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [990/5817], Loss: 0.1856, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [1000/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1010/5817], Loss: 0.1649, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [1020/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [1030/5817], Loss: 0.1787, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [1040/5817], Loss: 0.1714, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1050/5817], Loss: 0.1826, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [1060/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1070/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [1080/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1090/5817], Loss: 0.1668, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [1100/5817], Loss: 0.1856, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [1110/5817], Loss: 0.1648, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [1120/5817], Loss: 0.1890, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [1130/5817], Loss: 0.1797, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [1140/5817], Loss: 0.1901, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [1150/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1160/5817], Loss: 0.1858, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [1170/5817], Loss: 0.1910, Accuracy: 0.7734\n",
      "Epoch [8/10], Step [1180/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1190/5817], Loss: 0.1857, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [1200/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [1210/5817], Loss: 0.1738, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1220/5817], Loss: 0.1693, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [1230/5817], Loss: 0.1752, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1240/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [1250/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [1260/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1270/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [1280/5817], Loss: 0.1675, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [1290/5817], Loss: 0.1842, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [1300/5817], Loss: 0.1737, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1310/5817], Loss: 0.1749, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1320/5817], Loss: 0.1687, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [1330/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1340/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [1350/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1360/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1370/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [1380/5817], Loss: 0.1800, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1390/5817], Loss: 0.1899, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [1400/5817], Loss: 0.1782, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1410/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [1420/5817], Loss: 0.1844, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [1430/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1440/5817], Loss: 0.1916, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [1450/5817], Loss: 0.1845, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [1460/5817], Loss: 0.1828, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [1470/5817], Loss: 0.1710, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1480/5817], Loss: 0.1860, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [1490/5817], Loss: 0.1894, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [1500/5817], Loss: 0.1684, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [1510/5817], Loss: 0.1784, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1520/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [1530/5817], Loss: 0.1864, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [1540/5817], Loss: 0.1849, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [1550/5817], Loss: 0.1705, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [1560/5817], Loss: 0.1722, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1570/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1580/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1590/5817], Loss: 0.1791, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [1600/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [1610/5817], Loss: 0.1722, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1620/5817], Loss: 0.1849, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [1630/5817], Loss: 0.1791, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [1640/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1650/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1660/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1670/5817], Loss: 0.1674, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [1680/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1690/5817], Loss: 0.1917, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [1700/5817], Loss: 0.1682, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [1710/5817], Loss: 0.1638, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [1720/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [1730/5817], Loss: 0.1800, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1740/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [1750/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1760/5817], Loss: 0.1803, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [1770/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1780/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [1790/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [1800/5817], Loss: 0.1680, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [1810/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1820/5817], Loss: 0.1700, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [1830/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1840/5817], Loss: 0.1654, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [1850/5817], Loss: 0.1774, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1860/5817], Loss: 0.1734, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1870/5817], Loss: 0.1888, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [1880/5817], Loss: 0.1836, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [1890/5817], Loss: 0.1758, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1900/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [1910/5817], Loss: 0.1652, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [1920/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1930/5817], Loss: 0.1848, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [1940/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1950/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1960/5817], Loss: 0.1781, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [1970/5817], Loss: 0.1848, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [1980/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [1990/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2000/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2010/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [2020/5817], Loss: 0.1760, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2030/5817], Loss: 0.1719, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2040/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2050/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [2060/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [2070/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2080/5817], Loss: 0.1781, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2090/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [2100/5817], Loss: 0.1746, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2110/5817], Loss: 0.1842, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [2120/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2130/5817], Loss: 0.1855, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [2140/5817], Loss: 0.1715, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2150/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [2160/5817], Loss: 0.1754, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2170/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [2180/5817], Loss: 0.1621, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [2190/5817], Loss: 0.1662, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [2200/5817], Loss: 0.1946, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [2210/5817], Loss: 0.1831, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [2220/5817], Loss: 0.1885, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [2230/5817], Loss: 0.1852, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [2240/5817], Loss: 0.1703, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [2250/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2260/5817], Loss: 0.1736, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2270/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2280/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [2290/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2300/5817], Loss: 0.1805, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2310/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [2320/5817], Loss: 0.1884, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [2330/5817], Loss: 0.1721, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2340/5817], Loss: 0.1735, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2350/5817], Loss: 0.1820, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [2360/5817], Loss: 0.1660, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [2370/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [2380/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2390/5817], Loss: 0.1896, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [2400/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2410/5817], Loss: 0.1840, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [2420/5817], Loss: 0.1681, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [2430/5817], Loss: 0.1907, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [2440/5817], Loss: 0.1682, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [2450/5817], Loss: 0.1712, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2460/5817], Loss: 0.1892, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [2470/5817], Loss: 0.1914, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [2480/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2490/5817], Loss: 0.1924, Accuracy: 0.7734\n",
      "Epoch [8/10], Step [2500/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2510/5817], Loss: 0.1738, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2520/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2530/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2540/5817], Loss: 0.1739, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2550/5817], Loss: 0.1738, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2560/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [2570/5817], Loss: 0.1707, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2580/5817], Loss: 0.1853, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [2590/5817], Loss: 0.1897, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [2600/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2610/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2620/5817], Loss: 0.1813, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [2630/5817], Loss: 0.1720, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [2640/5817], Loss: 0.1820, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2650/5817], Loss: 0.1892, Accuracy: 0.7734\n",
      "Epoch [8/10], Step [2660/5817], Loss: 0.1915, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [2670/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2680/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [2690/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2700/5817], Loss: 0.1852, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [2710/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [2720/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2730/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2740/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2750/5817], Loss: 0.1698, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [2760/5817], Loss: 0.1618, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [2770/5817], Loss: 0.1877, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [2780/5817], Loss: 0.1797, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [2790/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2800/5817], Loss: 0.1811, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2810/5817], Loss: 0.1984, Accuracy: 0.7734\n",
      "Epoch [8/10], Step [2820/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [2830/5817], Loss: 0.1793, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [2840/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2850/5817], Loss: 0.1765, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2860/5817], Loss: 0.1815, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [2870/5817], Loss: 0.1763, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2880/5817], Loss: 0.1700, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [2890/5817], Loss: 0.1731, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2900/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2910/5817], Loss: 0.1741, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2920/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2930/5817], Loss: 0.1806, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [2940/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [2950/5817], Loss: 0.1831, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [2960/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2970/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2980/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2990/5817], Loss: 0.1704, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3000/5817], Loss: 0.1877, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [3010/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [3020/5817], Loss: 0.1812, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [3030/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3040/5817], Loss: 0.1902, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [3050/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3060/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [3070/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3080/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [3090/5817], Loss: 0.1806, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3100/5817], Loss: 0.1909, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [3110/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [3120/5817], Loss: 0.1834, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [3130/5817], Loss: 0.1810, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3140/5817], Loss: 0.1690, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [3150/5817], Loss: 0.1970, Accuracy: 0.7656\n",
      "Epoch [8/10], Step [3160/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3170/5817], Loss: 0.1935, Accuracy: 0.7734\n",
      "Epoch [8/10], Step [3180/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3190/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [3200/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3210/5817], Loss: 0.1695, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3220/5817], Loss: 0.1704, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3230/5817], Loss: 0.1597, Accuracy: 0.9219\n",
      "Epoch [8/10], Step [3240/5817], Loss: 0.1593, Accuracy: 0.9219\n",
      "Epoch [8/10], Step [3250/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3260/5817], Loss: 0.1697, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [3270/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3280/5817], Loss: 0.1849, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [3290/5817], Loss: 0.1832, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [3300/5817], Loss: 0.1648, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [3310/5817], Loss: 0.1888, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [3320/5817], Loss: 0.1632, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [3330/5817], Loss: 0.1737, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3340/5817], Loss: 0.1761, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3350/5817], Loss: 0.1878, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [3360/5817], Loss: 0.1781, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3370/5817], Loss: 0.1825, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3380/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [3390/5817], Loss: 0.1880, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [3400/5817], Loss: 0.1710, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3410/5817], Loss: 0.1672, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [3420/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3430/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [3440/5817], Loss: 0.1756, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3450/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [3460/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3470/5817], Loss: 0.1705, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3480/5817], Loss: 0.1737, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3490/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3500/5817], Loss: 0.1884, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [3510/5817], Loss: 0.1844, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [3520/5817], Loss: 0.1866, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [3530/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3540/5817], Loss: 0.1873, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [3550/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3560/5817], Loss: 0.1792, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3570/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [3580/5817], Loss: 0.1669, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3590/5817], Loss: 0.1849, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [3600/5817], Loss: 0.1864, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [3610/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3620/5817], Loss: 0.1812, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [3630/5817], Loss: 0.1697, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3640/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [3650/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3660/5817], Loss: 0.1936, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [3670/5817], Loss: 0.1719, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3680/5817], Loss: 0.1892, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [3690/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3700/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3710/5817], Loss: 0.1785, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3720/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3730/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [3740/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3750/5817], Loss: 0.1817, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [3760/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3770/5817], Loss: 0.1816, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [3780/5817], Loss: 0.1852, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [3790/5817], Loss: 0.1809, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [3800/5817], Loss: 0.1777, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3810/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [3820/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3830/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3840/5817], Loss: 0.1763, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3850/5817], Loss: 0.1815, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [3860/5817], Loss: 0.1722, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3870/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3880/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3890/5817], Loss: 0.1652, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [3900/5817], Loss: 0.1630, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [3910/5817], Loss: 0.1688, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3920/5817], Loss: 0.1921, Accuracy: 0.7734\n",
      "Epoch [8/10], Step [3930/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3940/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3950/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3960/5817], Loss: 0.1836, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [3970/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3980/5817], Loss: 0.1718, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3990/5817], Loss: 0.1690, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [4000/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [4010/5817], Loss: 0.1846, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4020/5817], Loss: 0.1899, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [4030/5817], Loss: 0.1704, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [4040/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [4050/5817], Loss: 0.1917, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [4060/5817], Loss: 0.1791, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4070/5817], Loss: 0.1746, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [4080/5817], Loss: 0.1868, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [4090/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4100/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [4110/5817], Loss: 0.1782, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4120/5817], Loss: 0.1856, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [4130/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4140/5817], Loss: 0.1664, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [4150/5817], Loss: 0.1857, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [4160/5817], Loss: 0.1972, Accuracy: 0.7578\n",
      "Epoch [8/10], Step [4170/5817], Loss: 0.1832, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [4180/5817], Loss: 0.1718, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [4190/5817], Loss: 0.1975, Accuracy: 0.7500\n",
      "Epoch [8/10], Step [4200/5817], Loss: 0.1668, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [4210/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [4220/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4230/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4240/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4250/5817], Loss: 0.1820, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [4260/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4270/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4280/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4290/5817], Loss: 0.1814, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4300/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [4310/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4320/5817], Loss: 0.1697, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [4330/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4340/5817], Loss: 0.1851, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [4350/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4360/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4370/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4380/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4390/5817], Loss: 0.1713, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4400/5817], Loss: 0.1725, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [4410/5817], Loss: 0.2025, Accuracy: 0.7344\n",
      "Epoch [8/10], Step [4420/5817], Loss: 0.1822, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [4430/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [4440/5817], Loss: 0.1737, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4450/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [4460/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [4470/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [4480/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4490/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [4500/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [4510/5817], Loss: 0.1790, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [4520/5817], Loss: 0.1732, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4530/5817], Loss: 0.1843, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4540/5817], Loss: 0.1796, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4550/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [4560/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [4570/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [4580/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [4590/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4600/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4610/5817], Loss: 0.1755, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4620/5817], Loss: 0.1664, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [4630/5817], Loss: 0.1808, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4640/5817], Loss: 0.1948, Accuracy: 0.7656\n",
      "Epoch [8/10], Step [4650/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4660/5817], Loss: 0.1813, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4670/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [4680/5817], Loss: 0.1845, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [4690/5817], Loss: 0.1736, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [4700/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [4710/5817], Loss: 0.1753, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4720/5817], Loss: 0.1850, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [4730/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4740/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [4750/5817], Loss: 0.1945, Accuracy: 0.7656\n",
      "Epoch [8/10], Step [4760/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4770/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4780/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [4790/5817], Loss: 0.1960, Accuracy: 0.7578\n",
      "Epoch [8/10], Step [4800/5817], Loss: 0.1696, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4810/5817], Loss: 0.1848, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [4820/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4830/5817], Loss: 0.1804, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4840/5817], Loss: 0.1767, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4850/5817], Loss: 0.1658, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [4860/5817], Loss: 0.1895, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [4870/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4880/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4890/5817], Loss: 0.1869, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [4900/5817], Loss: 0.1919, Accuracy: 0.7734\n",
      "Epoch [8/10], Step [4910/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4920/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4930/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [4940/5817], Loss: 0.1852, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [4950/5817], Loss: 0.1836, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [4960/5817], Loss: 0.1871, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [4970/5817], Loss: 0.1795, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4980/5817], Loss: 0.1772, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4990/5817], Loss: 0.1800, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [5000/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [5010/5817], Loss: 0.1748, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5020/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5030/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [5040/5817], Loss: 0.1813, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [5050/5817], Loss: 0.1717, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [5060/5817], Loss: 0.1719, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [5070/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [5080/5817], Loss: 0.1859, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [5090/5817], Loss: 0.1671, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [5100/5817], Loss: 0.1873, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [5110/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [5120/5817], Loss: 0.1704, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [5130/5817], Loss: 0.1814, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [5140/5817], Loss: 0.1690, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [5150/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [5160/5817], Loss: 0.1935, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [5170/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [5180/5817], Loss: 0.1620, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [5190/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [5200/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [5210/5817], Loss: 0.1965, Accuracy: 0.7500\n",
      "Epoch [8/10], Step [5220/5817], Loss: 0.1874, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [5230/5817], Loss: 0.1954, Accuracy: 0.7656\n",
      "Epoch [8/10], Step [5240/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [5250/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5260/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5270/5817], Loss: 0.1823, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [5280/5817], Loss: 0.1758, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5290/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [5300/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [5310/5817], Loss: 0.1915, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [5320/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [5330/5817], Loss: 0.1712, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [5340/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [5350/5817], Loss: 0.2007, Accuracy: 0.7344\n",
      "Epoch [8/10], Step [5360/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [5370/5817], Loss: 0.1812, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [5380/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [5390/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [5400/5817], Loss: 0.1896, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [5410/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5420/5817], Loss: 0.1884, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [5430/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [5440/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5450/5817], Loss: 0.1661, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [5460/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [5470/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5480/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5490/5817], Loss: 0.1871, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [5500/5817], Loss: 0.1920, Accuracy: 0.7734\n",
      "Epoch [8/10], Step [5510/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [5520/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [5530/5817], Loss: 0.1816, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [5540/5817], Loss: 0.1805, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [5550/5817], Loss: 0.1614, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [5560/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [5570/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [5580/5817], Loss: 0.1747, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [5590/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [5600/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [5610/5817], Loss: 0.1934, Accuracy: 0.7734\n",
      "Epoch [8/10], Step [5620/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [5630/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [5640/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [5650/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [5660/5817], Loss: 0.1758, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [5670/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [5680/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5690/5817], Loss: 0.1800, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5700/5817], Loss: 0.1797, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5710/5817], Loss: 0.1834, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [5720/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [5730/5817], Loss: 0.1887, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [5740/5817], Loss: 0.1901, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [5750/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [5760/5817], Loss: 0.1912, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [5770/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [5780/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5790/5817], Loss: 0.1867, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [5800/5817], Loss: 0.1738, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [5810/5817], Loss: 0.1845, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [10/5817], Loss: 0.1631, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [20/5817], Loss: 0.1624, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [30/5817], Loss: 0.1764, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [40/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [50/5817], Loss: 0.1708, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [60/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [70/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [80/5817], Loss: 0.1776, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [90/5817], Loss: 0.1846, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [100/5817], Loss: 0.1671, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [110/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [120/5817], Loss: 0.1686, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [130/5817], Loss: 0.1643, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [140/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [150/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [160/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [170/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [180/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [190/5817], Loss: 0.1667, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [200/5817], Loss: 0.1792, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [210/5817], Loss: 0.1838, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [220/5817], Loss: 0.1828, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [230/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [240/5817], Loss: 0.1674, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [250/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [260/5817], Loss: 0.1833, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [270/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [280/5817], Loss: 0.1673, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [290/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [300/5817], Loss: 0.1857, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [310/5817], Loss: 0.1595, Accuracy: 0.9219\n",
      "Epoch [9/10], Step [320/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [330/5817], Loss: 0.1710, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [340/5817], Loss: 0.1645, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [350/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [360/5817], Loss: 0.1680, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [370/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [380/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [390/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [400/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [410/5817], Loss: 0.1848, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [420/5817], Loss: 0.1753, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [430/5817], Loss: 0.1657, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [440/5817], Loss: 0.1586, Accuracy: 0.9219\n",
      "Epoch [9/10], Step [450/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [460/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [470/5817], Loss: 0.1764, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [480/5817], Loss: 0.1996, Accuracy: 0.7344\n",
      "Epoch [9/10], Step [490/5817], Loss: 0.1843, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [500/5817], Loss: 0.1810, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [510/5817], Loss: 0.1805, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [520/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [530/5817], Loss: 0.1674, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [540/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [550/5817], Loss: 0.1750, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [560/5817], Loss: 0.1744, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [570/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [580/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [590/5817], Loss: 0.1886, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [600/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [610/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [620/5817], Loss: 0.1701, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [630/5817], Loss: 0.1655, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [640/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [650/5817], Loss: 0.1780, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [660/5817], Loss: 0.1663, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [670/5817], Loss: 0.1703, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [680/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [690/5817], Loss: 0.1926, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [700/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [710/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [720/5817], Loss: 0.1763, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [730/5817], Loss: 0.1685, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [740/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [750/5817], Loss: 0.1627, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [760/5817], Loss: 0.1934, Accuracy: 0.7656\n",
      "Epoch [9/10], Step [770/5817], Loss: 0.1912, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [780/5817], Loss: 0.1915, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [790/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [800/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [810/5817], Loss: 0.1732, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [820/5817], Loss: 0.1645, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [830/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [840/5817], Loss: 0.1750, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [850/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [860/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [870/5817], Loss: 0.1723, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [880/5817], Loss: 0.1600, Accuracy: 0.9141\n",
      "Epoch [9/10], Step [890/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [900/5817], Loss: 0.1661, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [910/5817], Loss: 0.1822, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [920/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [930/5817], Loss: 0.1893, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [940/5817], Loss: 0.1740, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [950/5817], Loss: 0.1594, Accuracy: 0.9219\n",
      "Epoch [9/10], Step [960/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [970/5817], Loss: 0.1697, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [980/5817], Loss: 0.1832, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [990/5817], Loss: 0.1784, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1000/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1010/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1020/5817], Loss: 0.1762, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1030/5817], Loss: 0.1589, Accuracy: 0.9141\n",
      "Epoch [9/10], Step [1040/5817], Loss: 0.1909, Accuracy: 0.7734\n",
      "Epoch [9/10], Step [1050/5817], Loss: 0.1759, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1060/5817], Loss: 0.1773, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1070/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1080/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [1090/5817], Loss: 0.1725, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1100/5817], Loss: 0.1662, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [1110/5817], Loss: 0.1682, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1120/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1130/5817], Loss: 0.1803, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1140/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1150/5817], Loss: 0.1684, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [1160/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1170/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1180/5817], Loss: 0.1907, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [1190/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1200/5817], Loss: 0.1808, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [1210/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1220/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1230/5817], Loss: 0.1801, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [1240/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [1250/5817], Loss: 0.1779, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1260/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1270/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1280/5817], Loss: 0.1872, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [1290/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [1300/5817], Loss: 0.1697, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1310/5817], Loss: 0.1764, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [1320/5817], Loss: 0.1706, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1330/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1340/5817], Loss: 0.1844, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [1350/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1360/5817], Loss: 0.1750, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1370/5817], Loss: 0.1648, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [1380/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1390/5817], Loss: 0.1894, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [1400/5817], Loss: 0.1834, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [1410/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1420/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [1430/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1440/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1450/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1460/5817], Loss: 0.1849, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [1470/5817], Loss: 0.1699, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [1480/5817], Loss: 0.1679, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [1490/5817], Loss: 0.1836, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [1500/5817], Loss: 0.1652, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [1510/5817], Loss: 0.1735, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1520/5817], Loss: 0.1724, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [1530/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [1540/5817], Loss: 0.1932, Accuracy: 0.7656\n",
      "Epoch [9/10], Step [1550/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1560/5817], Loss: 0.1766, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [1570/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1580/5817], Loss: 0.1869, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [1590/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1600/5817], Loss: 0.1850, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [1610/5817], Loss: 0.1685, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1620/5817], Loss: 0.1972, Accuracy: 0.7578\n",
      "Epoch [9/10], Step [1630/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [1640/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1650/5817], Loss: 0.1617, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [1660/5817], Loss: 0.1711, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [1670/5817], Loss: 0.1677, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [1680/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1690/5817], Loss: 0.1633, Accuracy: 0.9141\n",
      "Epoch [9/10], Step [1700/5817], Loss: 0.1727, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [1710/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1720/5817], Loss: 0.1795, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [1730/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1740/5817], Loss: 0.1845, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [1750/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [1760/5817], Loss: 0.1642, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [1770/5817], Loss: 0.1772, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [1780/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1790/5817], Loss: 0.1854, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [1800/5817], Loss: 0.1898, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [1810/5817], Loss: 0.1874, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [1820/5817], Loss: 0.1681, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [1830/5817], Loss: 0.1885, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [1840/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1850/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1860/5817], Loss: 0.1699, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1870/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1880/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [1890/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [1900/5817], Loss: 0.1731, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1910/5817], Loss: 0.1820, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1920/5817], Loss: 0.1827, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [1930/5817], Loss: 0.1573, Accuracy: 0.9297\n",
      "Epoch [9/10], Step [1940/5817], Loss: 0.1667, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [1950/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [1960/5817], Loss: 0.1581, Accuracy: 0.9297\n",
      "Epoch [9/10], Step [1970/5817], Loss: 0.1690, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1980/5817], Loss: 0.1918, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [1990/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [2000/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2010/5817], Loss: 0.1671, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2020/5817], Loss: 0.1904, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [2030/5817], Loss: 0.1944, Accuracy: 0.7734\n",
      "Epoch [9/10], Step [2040/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [2050/5817], Loss: 0.1908, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [2060/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2070/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [2080/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2090/5817], Loss: 0.1756, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2100/5817], Loss: 0.1670, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [2110/5817], Loss: 0.1852, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [2120/5817], Loss: 0.1806, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2130/5817], Loss: 0.1769, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2140/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2150/5817], Loss: 0.1747, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2160/5817], Loss: 0.1690, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [2170/5817], Loss: 0.1911, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [2180/5817], Loss: 0.1746, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2190/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2200/5817], Loss: 0.1862, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [2210/5817], Loss: 0.1908, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [2220/5817], Loss: 0.1834, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [2230/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [2240/5817], Loss: 0.1899, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [2250/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2260/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2270/5817], Loss: 0.1874, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [2280/5817], Loss: 0.1887, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [2290/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2300/5817], Loss: 0.1676, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2310/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [2320/5817], Loss: 0.1785, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2330/5817], Loss: 0.1682, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2340/5817], Loss: 0.1709, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2350/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2360/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2370/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2380/5817], Loss: 0.1779, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2390/5817], Loss: 0.1843, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [2400/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2410/5817], Loss: 0.1771, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2420/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [2430/5817], Loss: 0.1660, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2440/5817], Loss: 0.1649, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [2450/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [2460/5817], Loss: 0.1649, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [2470/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [2480/5817], Loss: 0.1901, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [2490/5817], Loss: 0.1698, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2500/5817], Loss: 0.1819, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2510/5817], Loss: 0.1879, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [2520/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [2530/5817], Loss: 0.1668, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [2540/5817], Loss: 0.1661, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [2550/5817], Loss: 0.1754, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2560/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2570/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2580/5817], Loss: 0.1671, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [2590/5817], Loss: 0.1826, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2600/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [2610/5817], Loss: 0.1870, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [2620/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2630/5817], Loss: 0.1757, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2640/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2650/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [2660/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2670/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [2680/5817], Loss: 0.1745, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2690/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2700/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2710/5817], Loss: 0.1757, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2720/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [2730/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [2740/5817], Loss: 0.1656, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [2750/5817], Loss: 0.1869, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [2760/5817], Loss: 0.1735, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2770/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2780/5817], Loss: 0.1680, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [2790/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2800/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2810/5817], Loss: 0.1801, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2820/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2830/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2840/5817], Loss: 0.1781, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2850/5817], Loss: 0.1719, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2860/5817], Loss: 0.1856, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [2870/5817], Loss: 0.1711, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2880/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2890/5817], Loss: 0.1775, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2900/5817], Loss: 0.1815, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [2910/5817], Loss: 0.1701, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2920/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [2930/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2940/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [2950/5817], Loss: 0.1764, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2960/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2970/5817], Loss: 0.1654, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [2980/5817], Loss: 0.1855, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [2990/5817], Loss: 0.1723, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3000/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [3010/5817], Loss: 0.1744, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3020/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [3030/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [3040/5817], Loss: 0.1692, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3050/5817], Loss: 0.1820, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [3060/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3070/5817], Loss: 0.1824, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [3080/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3090/5817], Loss: 0.1754, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3100/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3110/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [3120/5817], Loss: 0.1873, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [3130/5817], Loss: 0.1590, Accuracy: 0.9219\n",
      "Epoch [9/10], Step [3140/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3150/5817], Loss: 0.1860, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [3160/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3170/5817], Loss: 0.1830, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3180/5817], Loss: 0.1849, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [3190/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3200/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3210/5817], Loss: 0.1805, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3220/5817], Loss: 0.1753, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3230/5817], Loss: 0.1622, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [3240/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [3250/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [3260/5817], Loss: 0.1843, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [3270/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3280/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3290/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3300/5817], Loss: 0.1902, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [3310/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3320/5817], Loss: 0.1879, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [3330/5817], Loss: 0.1749, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3340/5817], Loss: 0.1712, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3350/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3360/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [3370/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3380/5817], Loss: 0.1719, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3390/5817], Loss: 0.1829, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3400/5817], Loss: 0.1864, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [3410/5817], Loss: 0.1874, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [3420/5817], Loss: 0.1645, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [3430/5817], Loss: 0.1791, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [3440/5817], Loss: 0.1885, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [3450/5817], Loss: 0.1738, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [3460/5817], Loss: 0.1841, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3470/5817], Loss: 0.1915, Accuracy: 0.7656\n",
      "Epoch [9/10], Step [3480/5817], Loss: 0.1654, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [3490/5817], Loss: 0.1761, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3500/5817], Loss: 0.1700, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3510/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3520/5817], Loss: 0.1743, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3530/5817], Loss: 0.1647, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [3540/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [3550/5817], Loss: 0.1783, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3560/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3570/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3580/5817], Loss: 0.1829, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3590/5817], Loss: 0.1711, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3600/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [3610/5817], Loss: 0.1764, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3620/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [3630/5817], Loss: 0.1674, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3640/5817], Loss: 0.1917, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [3650/5817], Loss: 0.1720, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [3660/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [3670/5817], Loss: 0.1776, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [3680/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3690/5817], Loss: 0.1872, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [3700/5817], Loss: 0.1887, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [3710/5817], Loss: 0.1813, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3720/5817], Loss: 0.1830, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3730/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [3740/5817], Loss: 0.1855, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [3750/5817], Loss: 0.1834, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3760/5817], Loss: 0.1776, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3770/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [3780/5817], Loss: 0.1794, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3790/5817], Loss: 0.1707, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3800/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3810/5817], Loss: 0.1702, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3820/5817], Loss: 0.1817, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3830/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [3840/5817], Loss: 0.1693, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3850/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3860/5817], Loss: 0.1856, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3870/5817], Loss: 0.1841, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [3880/5817], Loss: 0.1907, Accuracy: 0.7734\n",
      "Epoch [9/10], Step [3890/5817], Loss: 0.1890, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [3900/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3910/5817], Loss: 0.1913, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [3920/5817], Loss: 0.1742, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3930/5817], Loss: 0.1869, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [3940/5817], Loss: 0.1715, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3950/5817], Loss: 0.1904, Accuracy: 0.7734\n",
      "Epoch [9/10], Step [3960/5817], Loss: 0.1824, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [3970/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3980/5817], Loss: 0.1827, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [3990/5817], Loss: 0.1781, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [4000/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [4010/5817], Loss: 0.1855, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [4020/5817], Loss: 0.1716, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4030/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [4040/5817], Loss: 0.1959, Accuracy: 0.7578\n",
      "Epoch [9/10], Step [4050/5817], Loss: 0.1722, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4060/5817], Loss: 0.1870, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [4070/5817], Loss: 0.1864, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [4080/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4090/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4100/5817], Loss: 0.1926, Accuracy: 0.7656\n",
      "Epoch [9/10], Step [4110/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4120/5817], Loss: 0.1910, Accuracy: 0.7734\n",
      "Epoch [9/10], Step [4130/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4140/5817], Loss: 0.1875, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [4150/5817], Loss: 0.1803, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [4160/5817], Loss: 0.1850, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [4170/5817], Loss: 0.1864, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [4180/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4190/5817], Loss: 0.1906, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [4200/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4210/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4220/5817], Loss: 0.1706, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4230/5817], Loss: 0.1714, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4240/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4250/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [4260/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4270/5817], Loss: 0.1757, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4280/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4290/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4300/5817], Loss: 0.1744, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [4310/5817], Loss: 0.1903, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [4320/5817], Loss: 0.1728, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4330/5817], Loss: 0.1941, Accuracy: 0.7734\n",
      "Epoch [9/10], Step [4340/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4350/5817], Loss: 0.1936, Accuracy: 0.7734\n",
      "Epoch [9/10], Step [4360/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4370/5817], Loss: 0.1708, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4380/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4390/5817], Loss: 0.1879, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [4400/5817], Loss: 0.1716, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4410/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [4420/5817], Loss: 0.1707, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4430/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [4440/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4450/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [4460/5817], Loss: 0.1880, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [4470/5817], Loss: 0.1829, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [4480/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4490/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [4500/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4510/5817], Loss: 0.1697, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4520/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4530/5817], Loss: 0.1839, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [4540/5817], Loss: 0.1712, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4550/5817], Loss: 0.1754, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4560/5817], Loss: 0.1649, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [4570/5817], Loss: 0.1695, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4580/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4590/5817], Loss: 0.1830, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [4600/5817], Loss: 0.1898, Accuracy: 0.7734\n",
      "Epoch [9/10], Step [4610/5817], Loss: 0.1880, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [4620/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [4630/5817], Loss: 0.1700, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4640/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4650/5817], Loss: 0.1678, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [4660/5817], Loss: 0.1726, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4670/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4680/5817], Loss: 0.1902, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [4690/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4700/5817], Loss: 0.1945, Accuracy: 0.7656\n",
      "Epoch [9/10], Step [4710/5817], Loss: 0.1737, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [4720/5817], Loss: 0.1828, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [4730/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [4740/5817], Loss: 0.1841, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [4750/5817], Loss: 0.1735, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4760/5817], Loss: 0.1886, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [4770/5817], Loss: 0.1685, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [4780/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [4790/5817], Loss: 0.1884, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [4800/5817], Loss: 0.1881, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [4810/5817], Loss: 0.1821, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [4820/5817], Loss: 0.1783, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [4830/5817], Loss: 0.1665, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [4840/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4850/5817], Loss: 0.1815, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [4860/5817], Loss: 0.1674, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4870/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [4880/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [4890/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4900/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4910/5817], Loss: 0.1717, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4920/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [4930/5817], Loss: 0.1722, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4940/5817], Loss: 0.1871, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [4950/5817], Loss: 0.1801, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4960/5817], Loss: 0.1946, Accuracy: 0.7578\n",
      "Epoch [9/10], Step [4970/5817], Loss: 0.1843, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [4980/5817], Loss: 0.1705, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4990/5817], Loss: 0.1687, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [5000/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [5010/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5020/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [5030/5817], Loss: 0.1952, Accuracy: 0.7734\n",
      "Epoch [9/10], Step [5040/5817], Loss: 0.1944, Accuracy: 0.7734\n",
      "Epoch [9/10], Step [5050/5817], Loss: 0.1807, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [5060/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5070/5817], Loss: 0.1701, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [5080/5817], Loss: 0.1778, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [5090/5817], Loss: 0.1847, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [5100/5817], Loss: 0.1800, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [5110/5817], Loss: 0.1701, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [5120/5817], Loss: 0.1676, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [5130/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [5140/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [5150/5817], Loss: 0.1853, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [5160/5817], Loss: 0.1718, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [5170/5817], Loss: 0.1975, Accuracy: 0.7578\n",
      "Epoch [9/10], Step [5180/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [5190/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [5200/5817], Loss: 0.1852, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [5210/5817], Loss: 0.1947, Accuracy: 0.7656\n",
      "Epoch [9/10], Step [5220/5817], Loss: 0.1818, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [5230/5817], Loss: 0.1844, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [5240/5817], Loss: 0.1785, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5250/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [5260/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [5270/5817], Loss: 0.1877, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [5280/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [5290/5817], Loss: 0.1966, Accuracy: 0.7578\n",
      "Epoch [9/10], Step [5300/5817], Loss: 0.1739, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [5310/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5320/5817], Loss: 0.1646, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [5330/5817], Loss: 0.1933, Accuracy: 0.7734\n",
      "Epoch [9/10], Step [5340/5817], Loss: 0.1807, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [5350/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [5360/5817], Loss: 0.1683, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [5370/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [5380/5817], Loss: 0.1954, Accuracy: 0.7734\n",
      "Epoch [9/10], Step [5390/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5400/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [5410/5817], Loss: 0.1685, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [5420/5817], Loss: 0.1646, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [5430/5817], Loss: 0.1845, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [5440/5817], Loss: 0.1952, Accuracy: 0.7734\n",
      "Epoch [9/10], Step [5450/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [5460/5817], Loss: 0.1902, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [5470/5817], Loss: 0.1771, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [5480/5817], Loss: 0.1711, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [5490/5817], Loss: 0.1924, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [5500/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [5510/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [5520/5817], Loss: 0.1954, Accuracy: 0.7578\n",
      "Epoch [9/10], Step [5530/5817], Loss: 0.1782, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [5540/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [5550/5817], Loss: 0.1786, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5560/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [5570/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [5580/5817], Loss: 0.1702, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [5590/5817], Loss: 0.1868, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [5600/5817], Loss: 0.1899, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [5610/5817], Loss: 0.1937, Accuracy: 0.7656\n",
      "Epoch [9/10], Step [5620/5817], Loss: 0.1700, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [5630/5817], Loss: 0.1823, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [5640/5817], Loss: 0.1872, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [5650/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [5660/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [5670/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5680/5817], Loss: 0.1782, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5690/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [5700/5817], Loss: 0.1617, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [5710/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [5720/5817], Loss: 0.1667, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [5730/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [5740/5817], Loss: 0.1802, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [5750/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [5760/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5770/5817], Loss: 0.1717, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5780/5817], Loss: 0.1687, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [5790/5817], Loss: 0.1825, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [5800/5817], Loss: 0.1985, Accuracy: 0.7500\n",
      "Epoch [9/10], Step [5810/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [10/5817], Loss: 0.1678, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [20/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [30/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [40/5817], Loss: 0.1607, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [50/5817], Loss: 0.1730, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [60/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [70/5817], Loss: 0.1698, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [80/5817], Loss: 0.1850, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [90/5817], Loss: 0.1637, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [100/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [110/5817], Loss: 0.1754, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [120/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [130/5817], Loss: 0.1702, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [140/5817], Loss: 0.1868, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [150/5817], Loss: 0.1684, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [160/5817], Loss: 0.1704, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [170/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [180/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [190/5817], Loss: 0.1639, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [200/5817], Loss: 0.1810, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [210/5817], Loss: 0.1727, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [220/5817], Loss: 0.1886, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [230/5817], Loss: 0.1664, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [240/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [250/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [260/5817], Loss: 0.1890, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [270/5817], Loss: 0.1694, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [280/5817], Loss: 0.1671, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [290/5817], Loss: 0.1735, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [300/5817], Loss: 0.1723, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [310/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [320/5817], Loss: 0.1781, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [330/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [340/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [350/5817], Loss: 0.1686, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [360/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [370/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [380/5817], Loss: 0.1708, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [390/5817], Loss: 0.1773, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [400/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [410/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [420/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [430/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [440/5817], Loss: 0.1868, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [450/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [460/5817], Loss: 0.1884, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [470/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [480/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [490/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [500/5817], Loss: 0.1742, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [510/5817], Loss: 0.1873, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [520/5817], Loss: 0.1687, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [530/5817], Loss: 0.1777, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [540/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [550/5817], Loss: 0.1827, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [560/5817], Loss: 0.1873, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [570/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [580/5817], Loss: 0.1878, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [590/5817], Loss: 0.1738, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [600/5817], Loss: 0.1738, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [610/5817], Loss: 0.1828, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [620/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [630/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [640/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [650/5817], Loss: 0.1781, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [660/5817], Loss: 0.1806, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [670/5817], Loss: 0.1907, Accuracy: 0.7812\n",
      "Epoch [10/10], Step [680/5817], Loss: 0.1608, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [690/5817], Loss: 0.1635, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [700/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [710/5817], Loss: 0.1753, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [720/5817], Loss: 0.1812, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [730/5817], Loss: 0.1573, Accuracy: 0.9219\n",
      "Epoch [10/10], Step [740/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [750/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [760/5817], Loss: 0.1697, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [770/5817], Loss: 0.1658, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [780/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [790/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [800/5817], Loss: 0.1856, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [810/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [820/5817], Loss: 0.1902, Accuracy: 0.7812\n",
      "Epoch [10/10], Step [830/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [840/5817], Loss: 0.1695, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [850/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [860/5817], Loss: 0.1847, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [870/5817], Loss: 0.1587, Accuracy: 0.9141\n",
      "Epoch [10/10], Step [880/5817], Loss: 0.1652, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [890/5817], Loss: 0.1778, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [900/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [910/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [920/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [930/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [940/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [950/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [960/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [970/5817], Loss: 0.1587, Accuracy: 0.9219\n",
      "Epoch [10/10], Step [980/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [990/5817], Loss: 0.1847, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [1000/5817], Loss: 0.1658, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [1010/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [1020/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [1030/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1040/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1050/5817], Loss: 0.1633, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [1060/5817], Loss: 0.1671, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1070/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1080/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1090/5817], Loss: 0.1711, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1100/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1110/5817], Loss: 0.1888, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [1120/5817], Loss: 0.1895, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [1130/5817], Loss: 0.1687, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1140/5817], Loss: 0.1883, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [1150/5817], Loss: 0.1831, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [1160/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1170/5817], Loss: 0.1733, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1180/5817], Loss: 0.1763, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1190/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1200/5817], Loss: 0.1845, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [1210/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [1220/5817], Loss: 0.1758, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1230/5817], Loss: 0.1630, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [1240/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1250/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1260/5817], Loss: 0.1956, Accuracy: 0.7656\n",
      "Epoch [10/10], Step [1270/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1280/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [1290/5817], Loss: 0.1728, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1300/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [1310/5817], Loss: 0.1778, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1320/5817], Loss: 0.1681, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [1330/5817], Loss: 0.1744, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1340/5817], Loss: 0.1666, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [1350/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1360/5817], Loss: 0.1746, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [1370/5817], Loss: 0.1884, Accuracy: 0.7812\n",
      "Epoch [10/10], Step [1380/5817], Loss: 0.1700, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1390/5817], Loss: 0.1723, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [1400/5817], Loss: 0.2049, Accuracy: 0.7266\n",
      "Epoch [10/10], Step [1410/5817], Loss: 0.1820, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1420/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1430/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [1440/5817], Loss: 0.1701, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1450/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1460/5817], Loss: 0.1881, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [1470/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1480/5817], Loss: 0.1582, Accuracy: 0.9219\n",
      "Epoch [10/10], Step [1490/5817], Loss: 0.1900, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [1500/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1510/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1520/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1530/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1540/5817], Loss: 0.1692, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1550/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [1560/5817], Loss: 0.1704, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1570/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [1580/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [1590/5817], Loss: 0.1841, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [1600/5817], Loss: 0.1833, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [1610/5817], Loss: 0.1839, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [1620/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1630/5817], Loss: 0.1926, Accuracy: 0.7734\n",
      "Epoch [10/10], Step [1640/5817], Loss: 0.1702, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1650/5817], Loss: 0.1750, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1660/5817], Loss: 0.1622, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [1670/5817], Loss: 0.1850, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [1680/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1690/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1700/5817], Loss: 0.1689, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1710/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1720/5817], Loss: 0.1987, Accuracy: 0.7500\n",
      "Epoch [10/10], Step [1730/5817], Loss: 0.1692, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1740/5817], Loss: 0.1634, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [1750/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [1760/5817], Loss: 0.1717, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1770/5817], Loss: 0.1941, Accuracy: 0.7734\n",
      "Epoch [10/10], Step [1780/5817], Loss: 0.1697, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [1790/5817], Loss: 0.1695, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1800/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [1810/5817], Loss: 0.1706, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1820/5817], Loss: 0.1703, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [1830/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1840/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1850/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1860/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1870/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1880/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1890/5817], Loss: 0.1770, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1900/5817], Loss: 0.1805, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [1910/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1920/5817], Loss: 0.1865, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [1930/5817], Loss: 0.1791, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1940/5817], Loss: 0.1769, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1950/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1960/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [1970/5817], Loss: 0.1861, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [1980/5817], Loss: 0.1913, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [1990/5817], Loss: 0.1822, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2000/5817], Loss: 0.2078, Accuracy: 0.7109\n",
      "Epoch [10/10], Step [2010/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2020/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2030/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2040/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2050/5817], Loss: 0.1891, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [2060/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [2070/5817], Loss: 0.1754, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2080/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2090/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2100/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2110/5817], Loss: 0.1778, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2120/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2130/5817], Loss: 0.1672, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [2140/5817], Loss: 0.1737, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2150/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [2160/5817], Loss: 0.1847, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [2170/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2180/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2190/5817], Loss: 0.1754, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2200/5817], Loss: 0.1667, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [2210/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [2220/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2230/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2240/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2250/5817], Loss: 0.1780, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2260/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2270/5817], Loss: 0.1766, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2280/5817], Loss: 0.1710, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2290/5817], Loss: 0.1792, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2300/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [2310/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2320/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2330/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2340/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2350/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2360/5817], Loss: 0.1752, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2370/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2380/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2390/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2400/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2410/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2420/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [2430/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2440/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [2450/5817], Loss: 0.1734, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2460/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2470/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [2480/5817], Loss: 0.1904, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [2490/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2500/5817], Loss: 0.1717, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2510/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2520/5817], Loss: 0.1725, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2530/5817], Loss: 0.1637, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [2540/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2550/5817], Loss: 0.1779, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2560/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2570/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2580/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [2590/5817], Loss: 0.1647, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [2600/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2610/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2620/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2630/5817], Loss: 0.1686, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2640/5817], Loss: 0.1835, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [2650/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [2660/5817], Loss: 0.1725, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2670/5817], Loss: 0.1833, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2680/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [2690/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2700/5817], Loss: 0.1640, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [2710/5817], Loss: 0.1668, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [2720/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2730/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2740/5817], Loss: 0.1874, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [2750/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2760/5817], Loss: 0.1812, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2770/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2780/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2790/5817], Loss: 0.1859, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [2800/5817], Loss: 0.1796, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2810/5817], Loss: 0.1618, Accuracy: 0.9141\n",
      "Epoch [10/10], Step [2820/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [2830/5817], Loss: 0.1830, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [2840/5817], Loss: 0.1816, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2850/5817], Loss: 0.1784, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2860/5817], Loss: 0.1834, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [2870/5817], Loss: 0.1681, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2880/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2890/5817], Loss: 0.1696, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2900/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [2910/5817], Loss: 0.1768, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2920/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [2930/5817], Loss: 0.1723, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2940/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2950/5817], Loss: 0.1890, Accuracy: 0.7812\n",
      "Epoch [10/10], Step [2960/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [2970/5817], Loss: 0.1652, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [2980/5817], Loss: 0.1901, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [2990/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3000/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3010/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3020/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [3030/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3040/5817], Loss: 0.1886, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [3050/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3060/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [3070/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [3080/5817], Loss: 0.1933, Accuracy: 0.7734\n",
      "Epoch [10/10], Step [3090/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3100/5817], Loss: 0.1807, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [3110/5817], Loss: 0.1675, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [3120/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [3130/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3140/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3150/5817], Loss: 0.1855, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [3160/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [3170/5817], Loss: 0.1887, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [3180/5817], Loss: 0.1719, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [3190/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3200/5817], Loss: 0.1682, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3210/5817], Loss: 0.1808, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [3220/5817], Loss: 0.1829, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [3230/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3240/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [3250/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [3260/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3270/5817], Loss: 0.1668, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [3280/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [3290/5817], Loss: 0.1895, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [3300/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [3310/5817], Loss: 0.1682, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3320/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3330/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [3340/5817], Loss: 0.1914, Accuracy: 0.7734\n",
      "Epoch [10/10], Step [3350/5817], Loss: 0.1705, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3360/5817], Loss: 0.1846, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [3370/5817], Loss: 0.1895, Accuracy: 0.7812\n",
      "Epoch [10/10], Step [3380/5817], Loss: 0.1894, Accuracy: 0.7812\n",
      "Epoch [10/10], Step [3390/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3400/5817], Loss: 0.1687, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3410/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3420/5817], Loss: 0.1760, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3430/5817], Loss: 0.1889, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [3440/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [3450/5817], Loss: 0.1809, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [3460/5817], Loss: 0.1694, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3470/5817], Loss: 0.1869, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [3480/5817], Loss: 0.1871, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [3490/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [3500/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3510/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3520/5817], Loss: 0.1739, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [3530/5817], Loss: 0.1682, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3540/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3550/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [3560/5817], Loss: 0.1853, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [3570/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3580/5817], Loss: 0.1659, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [3590/5817], Loss: 0.1725, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [3600/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3610/5817], Loss: 0.1688, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3620/5817], Loss: 0.1886, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [3630/5817], Loss: 0.1625, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [3640/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [3650/5817], Loss: 0.1827, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [3660/5817], Loss: 0.1754, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3670/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3680/5817], Loss: 0.1674, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [3690/5817], Loss: 0.1812, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [3700/5817], Loss: 0.1927, Accuracy: 0.7734\n",
      "Epoch [10/10], Step [3710/5817], Loss: 0.1861, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [3720/5817], Loss: 0.1593, Accuracy: 0.9141\n",
      "Epoch [10/10], Step [3730/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [3740/5817], Loss: 0.1954, Accuracy: 0.7578\n",
      "Epoch [10/10], Step [3750/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3760/5817], Loss: 0.1684, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3770/5817], Loss: 0.1592, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [3780/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3790/5817], Loss: 0.1828, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [3800/5817], Loss: 0.1807, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [3810/5817], Loss: 0.1824, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [3820/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3830/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3840/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [3850/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [3860/5817], Loss: 0.1965, Accuracy: 0.7656\n",
      "Epoch [10/10], Step [3870/5817], Loss: 0.1830, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [3880/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3890/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3900/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3910/5817], Loss: 0.1859, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [3920/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [3930/5817], Loss: 0.1667, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3940/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [3950/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3960/5817], Loss: 0.1747, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [3970/5817], Loss: 0.1915, Accuracy: 0.7812\n",
      "Epoch [10/10], Step [3980/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3990/5817], Loss: 0.1742, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4000/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4010/5817], Loss: 0.1839, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [4020/5817], Loss: 0.1727, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4030/5817], Loss: 0.1807, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [4040/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4050/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4060/5817], Loss: 0.1675, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [4070/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4080/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4090/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4100/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4110/5817], Loss: 0.1879, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [4120/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [4130/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [4140/5817], Loss: 0.1811, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4150/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [4160/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4170/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4180/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [4190/5817], Loss: 0.1715, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4200/5817], Loss: 0.1855, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [4210/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4220/5817], Loss: 0.1731, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4230/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4240/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4250/5817], Loss: 0.1815, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [4260/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [4270/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [4280/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4290/5817], Loss: 0.1737, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4300/5817], Loss: 0.1689, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4310/5817], Loss: 0.1664, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [4320/5817], Loss: 0.1726, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4330/5817], Loss: 0.1721, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4340/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4350/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4360/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4370/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4380/5817], Loss: 0.1859, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [4390/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [4400/5817], Loss: 0.1850, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [4410/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4420/5817], Loss: 0.1750, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4430/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [4440/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4450/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [4460/5817], Loss: 0.1886, Accuracy: 0.7812\n",
      "Epoch [10/10], Step [4470/5817], Loss: 0.1739, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4480/5817], Loss: 0.1695, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4490/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [4500/5817], Loss: 0.1784, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4510/5817], Loss: 0.1941, Accuracy: 0.7812\n",
      "Epoch [10/10], Step [4520/5817], Loss: 0.1717, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4530/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [4540/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4550/5817], Loss: 0.1700, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4560/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4570/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4580/5817], Loss: 0.1874, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [4590/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4600/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4610/5817], Loss: 0.1636, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [4620/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4630/5817], Loss: 0.1836, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [4640/5817], Loss: 0.1675, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [4650/5817], Loss: 0.1705, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4660/5817], Loss: 0.1869, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [4670/5817], Loss: 0.1785, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4680/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [4690/5817], Loss: 0.1796, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4700/5817], Loss: 0.1688, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4710/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4720/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4730/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [4740/5817], Loss: 0.1666, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [4750/5817], Loss: 0.1711, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4760/5817], Loss: 0.1616, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [4770/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4780/5817], Loss: 0.1743, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4790/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4800/5817], Loss: 0.1666, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [4810/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4820/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [4830/5817], Loss: 0.1851, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [4840/5817], Loss: 0.1692, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4850/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [4860/5817], Loss: 0.1837, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [4870/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4880/5817], Loss: 0.1838, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [4890/5817], Loss: 0.1611, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [4900/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4910/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [4920/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4930/5817], Loss: 0.1859, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [4940/5817], Loss: 0.1581, Accuracy: 0.9219\n",
      "Epoch [10/10], Step [4950/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4960/5817], Loss: 0.1667, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [4970/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4980/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4990/5817], Loss: 0.1700, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [5000/5817], Loss: 0.1758, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5010/5817], Loss: 0.1787, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [5020/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [5030/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5040/5817], Loss: 0.1933, Accuracy: 0.7734\n",
      "Epoch [10/10], Step [5050/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5060/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5070/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [5080/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [5090/5817], Loss: 0.1835, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [5100/5817], Loss: 0.1632, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [5110/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5120/5817], Loss: 0.1869, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [5130/5817], Loss: 0.1754, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5140/5817], Loss: 0.1706, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [5150/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [5160/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5170/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5180/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5190/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5200/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5210/5817], Loss: 0.1681, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [5220/5817], Loss: 0.1728, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5230/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5240/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5250/5817], Loss: 0.1739, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5260/5817], Loss: 0.1881, Accuracy: 0.7812\n",
      "Epoch [10/10], Step [5270/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [5280/5817], Loss: 0.1778, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [5290/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [5300/5817], Loss: 0.1690, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5310/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5320/5817], Loss: 0.1701, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5330/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [5340/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5350/5817], Loss: 0.1591, Accuracy: 0.9141\n",
      "Epoch [10/10], Step [5360/5817], Loss: 0.1693, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [5370/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [5380/5817], Loss: 0.1711, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5390/5817], Loss: 0.1869, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [5400/5817], Loss: 0.1700, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [5410/5817], Loss: 0.1867, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [5420/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5430/5817], Loss: 0.1913, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [5440/5817], Loss: 0.1758, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5450/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [5460/5817], Loss: 0.1678, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5470/5817], Loss: 0.1674, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [5480/5817], Loss: 0.1719, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5490/5817], Loss: 0.1725, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5500/5817], Loss: 0.1787, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [5510/5817], Loss: 0.1812, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [5520/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [5530/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [5540/5817], Loss: 0.1662, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [5550/5817], Loss: 0.1900, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [5560/5817], Loss: 0.1814, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [5570/5817], Loss: 0.1762, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5580/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [5590/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5600/5817], Loss: 0.1836, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [5610/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [5620/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [5630/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5640/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [5650/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5660/5817], Loss: 0.1694, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5670/5817], Loss: 0.1753, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5680/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5690/5817], Loss: 0.1701, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5700/5817], Loss: 0.1954, Accuracy: 0.7734\n",
      "Epoch [10/10], Step [5710/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5720/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [5730/5817], Loss: 0.1968, Accuracy: 0.7578\n",
      "Epoch [10/10], Step [5740/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [5750/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [5760/5817], Loss: 0.1866, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [5770/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5780/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5790/5817], Loss: 0.1622, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [5800/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [5810/5817], Loss: 0.1845, Accuracy: 0.8047\n",
      "Test accuracy: 0.7999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path, embedding_dim, word_index, max_features):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Load the GloVe embeddings matrix\n",
    "glove_file_path = 'glove.twitter.27B.200d.txt'  # Update the path to the downloaded GloVe file\n",
    "embedding_dim = 200\n",
    "embedding_matrix = load_glove_embeddings(glove_file_path, embedding_dim, tokenizer.word_index, max_features)\n",
    "\n",
    "class CNNTLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_dim=200, lstm_out=448, dropout_rate=0.3, num_classes=3, num_filters=128, filter_size=3, pool_size=2):\n",
    "        super(CNNTLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.conv1d = nn.Conv1d(embed_dim, num_filters, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool1d = nn.MaxPool1d(pool_size)\n",
    "        self.bi_lstm = nn.LSTM(num_filters, lstm_out // 2, batch_first=True, bidirectional=True, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(lstm_out, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.bi_lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return self.softmax(x)\n",
    "    \n",
    "y_train_np = y_train.values.reshape(-1, 1)\n",
    "y_val_np = y_val.values.reshape(-1, 1)\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_val_torch = torch.tensor(X_val, dtype=torch.long)\n",
    "y_val_torch = torch.tensor(y_val.values, dtype=torch.long)\n",
    "\n",
    "# Initialize model, loss function and optimizer\n",
    "model = CNNTLSTM(embedding_matrix).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "accumulation_steps = 4  # Adjust this value based on your GPU memory capacity\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (texts, labels) in enumerate(train_loader):\n",
    "        texts, labels = texts.cuda(), labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels) / accumulation_steps\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == labels).sum().item() / labels.size(0)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item() * accumulation_steps\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "test_dataset = TensorDataset(X_val_torch, y_val_torch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)  # Set a smaller batch size for test dat| qZ\\ASWQ1\n",
    "\n",
    "total_correct = 0 \n",
    "total_samples = 0\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.cuda(), labels.cuda()\n",
    "        test_outputs = model(texts)\n",
    "        _, predicted = torch.max(test_outputs.data, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = total_correct / total_samples\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU with CNN+LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/5817], Loss: 0.2749, Accuracy: 0.2812\n",
      "Epoch [1/10], Step [20/5817], Loss: 0.2697, Accuracy: 0.4766\n",
      "Epoch [1/10], Step [30/5817], Loss: 0.2681, Accuracy: 0.4141\n",
      "Epoch [1/10], Step [40/5817], Loss: 0.2612, Accuracy: 0.5312\n",
      "Epoch [1/10], Step [50/5817], Loss: 0.2562, Accuracy: 0.5547\n",
      "Epoch [1/10], Step [60/5817], Loss: 0.2535, Accuracy: 0.5859\n",
      "Epoch [1/10], Step [70/5817], Loss: 0.2381, Accuracy: 0.5781\n",
      "Epoch [1/10], Step [80/5817], Loss: 0.2264, Accuracy: 0.6484\n",
      "Epoch [1/10], Step [90/5817], Loss: 0.2394, Accuracy: 0.6094\n",
      "Epoch [1/10], Step [100/5817], Loss: 0.2379, Accuracy: 0.6016\n",
      "Epoch [1/10], Step [110/5817], Loss: 0.2265, Accuracy: 0.6406\n",
      "Epoch [1/10], Step [120/5817], Loss: 0.2136, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [130/5817], Loss: 0.2032, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [140/5817], Loss: 0.2009, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [150/5817], Loss: 0.2197, Accuracy: 0.6484\n",
      "Epoch [1/10], Step [160/5817], Loss: 0.1980, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [170/5817], Loss: 0.1895, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [180/5817], Loss: 0.2051, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [190/5817], Loss: 0.2049, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [200/5817], Loss: 0.2084, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [210/5817], Loss: 0.2005, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [220/5817], Loss: 0.1891, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [230/5817], Loss: 0.1895, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [240/5817], Loss: 0.2088, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [250/5817], Loss: 0.2101, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [260/5817], Loss: 0.2067, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [270/5817], Loss: 0.2002, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [280/5817], Loss: 0.2045, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [290/5817], Loss: 0.1883, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [300/5817], Loss: 0.1925, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [310/5817], Loss: 0.2035, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [320/5817], Loss: 0.2035, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [330/5817], Loss: 0.1963, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [340/5817], Loss: 0.1947, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [350/5817], Loss: 0.1976, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [360/5817], Loss: 0.1962, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [370/5817], Loss: 0.2072, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [380/5817], Loss: 0.1987, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [390/5817], Loss: 0.1955, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [400/5817], Loss: 0.2068, Accuracy: 0.7031\n",
      "Epoch [1/10], Step [410/5817], Loss: 0.2057, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [420/5817], Loss: 0.1746, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [430/5817], Loss: 0.1900, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [440/5817], Loss: 0.2109, Accuracy: 0.6797\n",
      "Epoch [1/10], Step [450/5817], Loss: 0.1886, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [460/5817], Loss: 0.2038, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [470/5817], Loss: 0.1841, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [480/5817], Loss: 0.1924, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [490/5817], Loss: 0.1944, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [500/5817], Loss: 0.2124, Accuracy: 0.6641\n",
      "Epoch [1/10], Step [510/5817], Loss: 0.1730, Accuracy: 0.8672\n",
      "Epoch [1/10], Step [520/5817], Loss: 0.1950, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [530/5817], Loss: 0.2027, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [540/5817], Loss: 0.1870, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [550/5817], Loss: 0.2021, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [560/5817], Loss: 0.2071, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [570/5817], Loss: 0.1904, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [580/5817], Loss: 0.2032, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [590/5817], Loss: 0.1818, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [600/5817], Loss: 0.1898, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [610/5817], Loss: 0.1939, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [620/5817], Loss: 0.1948, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [630/5817], Loss: 0.1943, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [640/5817], Loss: 0.2006, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [650/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [660/5817], Loss: 0.1980, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [670/5817], Loss: 0.1775, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [680/5817], Loss: 0.2062, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [690/5817], Loss: 0.1889, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [700/5817], Loss: 0.2035, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [710/5817], Loss: 0.1813, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [720/5817], Loss: 0.1948, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [730/5817], Loss: 0.1962, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [740/5817], Loss: 0.2003, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [750/5817], Loss: 0.1950, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [760/5817], Loss: 0.2112, Accuracy: 0.6953\n",
      "Epoch [1/10], Step [770/5817], Loss: 0.1858, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [780/5817], Loss: 0.1865, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [790/5817], Loss: 0.2010, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [800/5817], Loss: 0.1866, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [810/5817], Loss: 0.1912, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [820/5817], Loss: 0.1898, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [830/5817], Loss: 0.1921, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [840/5817], Loss: 0.2001, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [850/5817], Loss: 0.1964, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [860/5817], Loss: 0.2034, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [870/5817], Loss: 0.2052, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [880/5817], Loss: 0.1982, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [890/5817], Loss: 0.1983, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [900/5817], Loss: 0.2068, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [910/5817], Loss: 0.1926, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [920/5817], Loss: 0.1865, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [930/5817], Loss: 0.1894, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [940/5817], Loss: 0.1963, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [950/5817], Loss: 0.1842, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [960/5817], Loss: 0.1941, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [970/5817], Loss: 0.1986, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [980/5817], Loss: 0.1889, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [990/5817], Loss: 0.1898, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1000/5817], Loss: 0.2030, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [1010/5817], Loss: 0.2121, Accuracy: 0.6797\n",
      "Epoch [1/10], Step [1020/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [1030/5817], Loss: 0.1859, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [1040/5817], Loss: 0.1846, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [1050/5817], Loss: 0.1943, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1060/5817], Loss: 0.1907, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1070/5817], Loss: 0.2072, Accuracy: 0.7031\n",
      "Epoch [1/10], Step [1080/5817], Loss: 0.1913, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1090/5817], Loss: 0.1960, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1100/5817], Loss: 0.1822, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [1110/5817], Loss: 0.1985, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1120/5817], Loss: 0.1871, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1130/5817], Loss: 0.1886, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [1140/5817], Loss: 0.1994, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [1150/5817], Loss: 0.1876, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [1160/5817], Loss: 0.1912, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1170/5817], Loss: 0.2026, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [1180/5817], Loss: 0.1868, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [1190/5817], Loss: 0.1855, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1200/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [1210/5817], Loss: 0.1911, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1220/5817], Loss: 0.1846, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1230/5817], Loss: 0.1901, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1240/5817], Loss: 0.1947, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [1250/5817], Loss: 0.1983, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [1260/5817], Loss: 0.1997, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [1270/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1280/5817], Loss: 0.1815, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [1290/5817], Loss: 0.1919, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1300/5817], Loss: 0.2111, Accuracy: 0.6797\n",
      "Epoch [1/10], Step [1310/5817], Loss: 0.1870, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [1320/5817], Loss: 0.1904, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1330/5817], Loss: 0.1881, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [1340/5817], Loss: 0.1928, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1350/5817], Loss: 0.1998, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [1360/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [1370/5817], Loss: 0.1932, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [1380/5817], Loss: 0.1915, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1390/5817], Loss: 0.1861, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [1400/5817], Loss: 0.1999, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [1410/5817], Loss: 0.1866, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1420/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [1430/5817], Loss: 0.1884, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1440/5817], Loss: 0.2001, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [1450/5817], Loss: 0.1839, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [1460/5817], Loss: 0.1948, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [1470/5817], Loss: 0.1860, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [1480/5817], Loss: 0.1901, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1490/5817], Loss: 0.1916, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1500/5817], Loss: 0.1761, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [1510/5817], Loss: 0.2007, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [1520/5817], Loss: 0.1892, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1530/5817], Loss: 0.1868, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1540/5817], Loss: 0.1887, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1550/5817], Loss: 0.1803, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [1560/5817], Loss: 0.1750, Accuracy: 0.8594\n",
      "Epoch [1/10], Step [1570/5817], Loss: 0.1839, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1580/5817], Loss: 0.1968, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [1590/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1600/5817], Loss: 0.1895, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1610/5817], Loss: 0.2141, Accuracy: 0.6875\n",
      "Epoch [1/10], Step [1620/5817], Loss: 0.1977, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [1630/5817], Loss: 0.1876, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1640/5817], Loss: 0.1903, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1650/5817], Loss: 0.1950, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1660/5817], Loss: 0.1798, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [1670/5817], Loss: 0.1787, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [1680/5817], Loss: 0.1969, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [1690/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1700/5817], Loss: 0.1851, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1710/5817], Loss: 0.1873, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1720/5817], Loss: 0.1834, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1730/5817], Loss: 0.1991, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [1740/5817], Loss: 0.1701, Accuracy: 0.8750\n",
      "Epoch [1/10], Step [1750/5817], Loss: 0.2052, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [1760/5817], Loss: 0.1942, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1770/5817], Loss: 0.1935, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1780/5817], Loss: 0.1829, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [1790/5817], Loss: 0.1944, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [1800/5817], Loss: 0.1849, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [1810/5817], Loss: 0.1923, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1820/5817], Loss: 0.1846, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [1830/5817], Loss: 0.1933, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1840/5817], Loss: 0.1838, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1850/5817], Loss: 0.1790, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [1860/5817], Loss: 0.1889, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [1870/5817], Loss: 0.1745, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [1880/5817], Loss: 0.1888, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1890/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [1900/5817], Loss: 0.1855, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1910/5817], Loss: 0.1958, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [1920/5817], Loss: 0.1784, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [1930/5817], Loss: 0.1829, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [1940/5817], Loss: 0.1919, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [1950/5817], Loss: 0.1726, Accuracy: 0.8672\n",
      "Epoch [1/10], Step [1960/5817], Loss: 0.1971, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [1970/5817], Loss: 0.1906, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [1980/5817], Loss: 0.1832, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [1990/5817], Loss: 0.1896, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [2000/5817], Loss: 0.1967, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2010/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [2020/5817], Loss: 0.1805, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [2030/5817], Loss: 0.2008, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [2040/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2050/5817], Loss: 0.1963, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2060/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [2070/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [2080/5817], Loss: 0.1845, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [2090/5817], Loss: 0.1999, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [2100/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2110/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2120/5817], Loss: 0.1942, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [2130/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [2140/5817], Loss: 0.1962, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2150/5817], Loss: 0.1940, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [2160/5817], Loss: 0.1924, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2170/5817], Loss: 0.1856, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2180/5817], Loss: 0.1967, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [2190/5817], Loss: 0.1892, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2200/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [2210/5817], Loss: 0.1870, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2220/5817], Loss: 0.1984, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [2230/5817], Loss: 0.1936, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2240/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2250/5817], Loss: 0.1903, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [2260/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2270/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [2280/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2290/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [2300/5817], Loss: 0.1882, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [2310/5817], Loss: 0.1989, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [2320/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2330/5817], Loss: 0.1759, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [2340/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [2350/5817], Loss: 0.2009, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2360/5817], Loss: 0.1953, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2370/5817], Loss: 0.1904, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [2380/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2390/5817], Loss: 0.1876, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [2400/5817], Loss: 0.1884, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [2410/5817], Loss: 0.1917, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2420/5817], Loss: 0.1982, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [2430/5817], Loss: 0.1918, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [2440/5817], Loss: 0.1716, Accuracy: 0.8750\n",
      "Epoch [1/10], Step [2450/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2460/5817], Loss: 0.1876, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2470/5817], Loss: 0.1899, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [2480/5817], Loss: 0.1751, Accuracy: 0.8594\n",
      "Epoch [1/10], Step [2490/5817], Loss: 0.1823, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [2500/5817], Loss: 0.1926, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [2510/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2520/5817], Loss: 0.1995, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2530/5817], Loss: 0.1803, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [2540/5817], Loss: 0.1953, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2550/5817], Loss: 0.1961, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2560/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [2570/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2580/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [2590/5817], Loss: 0.2005, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [2600/5817], Loss: 0.1860, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2610/5817], Loss: 0.1848, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [2620/5817], Loss: 0.1929, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2630/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2640/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [2650/5817], Loss: 0.1850, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2660/5817], Loss: 0.1769, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [2670/5817], Loss: 0.1885, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2680/5817], Loss: 0.1864, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2690/5817], Loss: 0.1920, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2700/5817], Loss: 0.1869, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2710/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [2720/5817], Loss: 0.1946, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2730/5817], Loss: 0.1970, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [2740/5817], Loss: 0.1885, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [2750/5817], Loss: 0.1881, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2760/5817], Loss: 0.1865, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [2770/5817], Loss: 0.1741, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [2780/5817], Loss: 0.1742, Accuracy: 0.8594\n",
      "Epoch [1/10], Step [2790/5817], Loss: 0.2061, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [2800/5817], Loss: 0.1865, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [2810/5817], Loss: 0.1957, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2820/5817], Loss: 0.1825, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [2830/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2840/5817], Loss: 0.1825, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [2850/5817], Loss: 0.1825, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [2860/5817], Loss: 0.1961, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [2870/5817], Loss: 0.1865, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2880/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [2890/5817], Loss: 0.1842, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2900/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [2910/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [2920/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [2930/5817], Loss: 0.1892, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [2940/5817], Loss: 0.1968, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [2950/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [2960/5817], Loss: 0.1895, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [2970/5817], Loss: 0.1904, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [2980/5817], Loss: 0.1742, Accuracy: 0.8594\n",
      "Epoch [1/10], Step [2990/5817], Loss: 0.1927, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3000/5817], Loss: 0.1997, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [3010/5817], Loss: 0.1920, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3020/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [3030/5817], Loss: 0.1854, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3040/5817], Loss: 0.1873, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3050/5817], Loss: 0.1924, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3060/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3070/5817], Loss: 0.2008, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [3080/5817], Loss: 0.1887, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3090/5817], Loss: 0.1897, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3100/5817], Loss: 0.1935, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3110/5817], Loss: 0.1877, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3120/5817], Loss: 0.1901, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3130/5817], Loss: 0.1868, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [3140/5817], Loss: 0.1675, Accuracy: 0.8828\n",
      "Epoch [1/10], Step [3150/5817], Loss: 0.1906, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3160/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [3170/5817], Loss: 0.2002, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3180/5817], Loss: 0.1791, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [3190/5817], Loss: 0.1940, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3200/5817], Loss: 0.1789, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [3210/5817], Loss: 0.1929, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3220/5817], Loss: 0.1946, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3230/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [3240/5817], Loss: 0.1868, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3250/5817], Loss: 0.1979, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [3260/5817], Loss: 0.1897, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3270/5817], Loss: 0.2019, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [3280/5817], Loss: 0.2021, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [3290/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [3300/5817], Loss: 0.1732, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [3310/5817], Loss: 0.1925, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3320/5817], Loss: 0.1897, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3330/5817], Loss: 0.1963, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [3340/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [3350/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [3360/5817], Loss: 0.1951, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3370/5817], Loss: 0.1829, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [3380/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [3390/5817], Loss: 0.1957, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3400/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [3410/5817], Loss: 0.1861, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [3420/5817], Loss: 0.1820, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [3430/5817], Loss: 0.1961, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [3440/5817], Loss: 0.1852, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [3450/5817], Loss: 0.1994, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [3460/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [3470/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [3480/5817], Loss: 0.1900, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3490/5817], Loss: 0.1843, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [3500/5817], Loss: 0.1823, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [3510/5817], Loss: 0.1905, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3520/5817], Loss: 0.1849, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3530/5817], Loss: 0.1894, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3540/5817], Loss: 0.1838, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [3550/5817], Loss: 0.1864, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3560/5817], Loss: 0.1772, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [3570/5817], Loss: 0.1945, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [3580/5817], Loss: 0.1846, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3590/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [3600/5817], Loss: 0.1647, Accuracy: 0.8906\n",
      "Epoch [1/10], Step [3610/5817], Loss: 0.1845, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3620/5817], Loss: 0.1934, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3630/5817], Loss: 0.1830, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3640/5817], Loss: 0.1890, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3650/5817], Loss: 0.1939, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3660/5817], Loss: 0.2021, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [3670/5817], Loss: 0.1917, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3680/5817], Loss: 0.1760, Accuracy: 0.8594\n",
      "Epoch [1/10], Step [3690/5817], Loss: 0.1858, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3700/5817], Loss: 0.1946, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [3710/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [3720/5817], Loss: 0.1883, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3730/5817], Loss: 0.1841, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [3740/5817], Loss: 0.1918, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3750/5817], Loss: 0.1838, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [3760/5817], Loss: 0.1768, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [3770/5817], Loss: 0.1858, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3780/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [3790/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3800/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [3810/5817], Loss: 0.1869, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [3820/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [3830/5817], Loss: 0.1927, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3840/5817], Loss: 0.1976, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [3850/5817], Loss: 0.1902, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3860/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [3870/5817], Loss: 0.1814, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [3880/5817], Loss: 0.2010, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [3890/5817], Loss: 0.1858, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3900/5817], Loss: 0.1819, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [3910/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [3920/5817], Loss: 0.1911, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [3930/5817], Loss: 0.1774, Accuracy: 0.8594\n",
      "Epoch [1/10], Step [3940/5817], Loss: 0.1919, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [3950/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [3960/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3970/5817], Loss: 0.1742, Accuracy: 0.8594\n",
      "Epoch [1/10], Step [3980/5817], Loss: 0.1932, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [3990/5817], Loss: 0.2016, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [4000/5817], Loss: 0.1983, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [4010/5817], Loss: 0.1993, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [4020/5817], Loss: 0.1950, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [4030/5817], Loss: 0.1898, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4040/5817], Loss: 0.1960, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [4050/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [4060/5817], Loss: 0.1894, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4070/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4080/5817], Loss: 0.1817, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [4090/5817], Loss: 0.1883, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4100/5817], Loss: 0.1863, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4110/5817], Loss: 0.1914, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4120/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4130/5817], Loss: 0.1894, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4140/5817], Loss: 0.1896, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4150/5817], Loss: 0.1945, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [4160/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4170/5817], Loss: 0.1813, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4180/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [4190/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [4200/5817], Loss: 0.1919, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4210/5817], Loss: 0.1943, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [4220/5817], Loss: 0.1848, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4230/5817], Loss: 0.1737, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [4240/5817], Loss: 0.1927, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [4250/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [4260/5817], Loss: 0.1889, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4270/5817], Loss: 0.2011, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [4280/5817], Loss: 0.1895, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4290/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4300/5817], Loss: 0.1707, Accuracy: 0.8594\n",
      "Epoch [1/10], Step [4310/5817], Loss: 0.1930, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [4320/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4330/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [4340/5817], Loss: 0.1866, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4350/5817], Loss: 0.1924, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [4360/5817], Loss: 0.1972, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [4370/5817], Loss: 0.1863, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4380/5817], Loss: 0.1828, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [4390/5817], Loss: 0.1939, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [4400/5817], Loss: 0.1906, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4410/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [4420/5817], Loss: 0.1845, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4430/5817], Loss: 0.1867, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4440/5817], Loss: 0.1906, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4450/5817], Loss: 0.1885, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4460/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [4470/5817], Loss: 0.1995, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [4480/5817], Loss: 0.1827, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4490/5817], Loss: 0.1838, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4500/5817], Loss: 0.1825, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4510/5817], Loss: 0.1700, Accuracy: 0.8750\n",
      "Epoch [1/10], Step [4520/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [4530/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [1/10], Step [4540/5817], Loss: 0.1886, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4550/5817], Loss: 0.1972, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [4560/5817], Loss: 0.1862, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4570/5817], Loss: 0.1844, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [4580/5817], Loss: 0.1779, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [4590/5817], Loss: 0.1764, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [4600/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [4610/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [4620/5817], Loss: 0.2041, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [4630/5817], Loss: 0.1808, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [4640/5817], Loss: 0.1846, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [4650/5817], Loss: 0.2063, Accuracy: 0.7188\n",
      "Epoch [1/10], Step [4660/5817], Loss: 0.1676, Accuracy: 0.8750\n",
      "Epoch [1/10], Step [4670/5817], Loss: 0.1893, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4680/5817], Loss: 0.1799, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [4690/5817], Loss: 0.1928, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [4700/5817], Loss: 0.1953, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [4710/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [4720/5817], Loss: 0.1868, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4730/5817], Loss: 0.1869, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4740/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [4750/5817], Loss: 0.1831, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4760/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4770/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [4780/5817], Loss: 0.1907, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [4790/5817], Loss: 0.1841, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4800/5817], Loss: 0.1859, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4810/5817], Loss: 0.1904, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4820/5817], Loss: 0.1808, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [4830/5817], Loss: 0.1878, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4840/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4850/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [4860/5817], Loss: 0.1887, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4870/5817], Loss: 0.1868, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4880/5817], Loss: 0.1928, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4890/5817], Loss: 0.1794, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [4900/5817], Loss: 0.1796, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [4910/5817], Loss: 0.1754, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [4920/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [4930/5817], Loss: 0.1947, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [4940/5817], Loss: 0.1779, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [4950/5817], Loss: 0.1889, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [4960/5817], Loss: 0.1884, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [4970/5817], Loss: 0.1899, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [4980/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [4990/5817], Loss: 0.1865, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [5000/5817], Loss: 0.1875, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [5010/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [5020/5817], Loss: 0.1810, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5030/5817], Loss: 0.1884, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5040/5817], Loss: 0.1966, Accuracy: 0.7422\n",
      "Epoch [1/10], Step [5050/5817], Loss: 0.2111, Accuracy: 0.7109\n",
      "Epoch [1/10], Step [5060/5817], Loss: 0.1901, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5070/5817], Loss: 0.1780, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [5080/5817], Loss: 0.1833, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [5090/5817], Loss: 0.1917, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5100/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5110/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [1/10], Step [5120/5817], Loss: 0.1870, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5130/5817], Loss: 0.1859, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [5140/5817], Loss: 0.1798, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5150/5817], Loss: 0.1935, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [5160/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [5170/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [5180/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [5190/5817], Loss: 0.1893, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [5200/5817], Loss: 0.1924, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [5210/5817], Loss: 0.1852, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [5220/5817], Loss: 0.1920, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [5230/5817], Loss: 0.1964, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [5240/5817], Loss: 0.1831, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [5250/5817], Loss: 0.1960, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [5260/5817], Loss: 0.1785, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5270/5817], Loss: 0.1897, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [5280/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [5290/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [1/10], Step [5300/5817], Loss: 0.1815, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5310/5817], Loss: 0.1820, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [5320/5817], Loss: 0.1998, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [5330/5817], Loss: 0.1872, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [5340/5817], Loss: 0.1795, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [5350/5817], Loss: 0.1818, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [5360/5817], Loss: 0.2034, Accuracy: 0.7344\n",
      "Epoch [1/10], Step [5370/5817], Loss: 0.1985, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [5380/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [5390/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [5400/5817], Loss: 0.1865, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [5410/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [5420/5817], Loss: 0.1902, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5430/5817], Loss: 0.1913, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [5440/5817], Loss: 0.1873, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [5450/5817], Loss: 0.1921, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [5460/5817], Loss: 0.1951, Accuracy: 0.7734\n",
      "Epoch [1/10], Step [5470/5817], Loss: 0.1873, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [5480/5817], Loss: 0.1947, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [5490/5817], Loss: 0.1881, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5500/5817], Loss: 0.1818, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5510/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [1/10], Step [5520/5817], Loss: 0.1824, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5530/5817], Loss: 0.1734, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [5540/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5550/5817], Loss: 0.1855, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5560/5817], Loss: 0.1682, Accuracy: 0.8828\n",
      "Epoch [1/10], Step [5570/5817], Loss: 0.1770, Accuracy: 0.8281\n",
      "Epoch [1/10], Step [5580/5817], Loss: 0.1963, Accuracy: 0.7500\n",
      "Epoch [1/10], Step [5590/5817], Loss: 0.1830, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5600/5817], Loss: 0.1920, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [5610/5817], Loss: 0.2017, Accuracy: 0.7266\n",
      "Epoch [1/10], Step [5620/5817], Loss: 0.1867, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5630/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [5640/5817], Loss: 0.1966, Accuracy: 0.7656\n",
      "Epoch [1/10], Step [5650/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [5660/5817], Loss: 0.2048, Accuracy: 0.7031\n",
      "Epoch [1/10], Step [5670/5817], Loss: 0.1857, Accuracy: 0.7891\n",
      "Epoch [1/10], Step [5680/5817], Loss: 0.2155, Accuracy: 0.6797\n",
      "Epoch [1/10], Step [5690/5817], Loss: 0.1689, Accuracy: 0.8750\n",
      "Epoch [1/10], Step [5700/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [1/10], Step [5710/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [5720/5817], Loss: 0.1830, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [5730/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [1/10], Step [5740/5817], Loss: 0.1935, Accuracy: 0.7578\n",
      "Epoch [1/10], Step [5750/5817], Loss: 0.1912, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5760/5817], Loss: 0.1836, Accuracy: 0.8047\n",
      "Epoch [1/10], Step [5770/5817], Loss: 0.1865, Accuracy: 0.7969\n",
      "Epoch [1/10], Step [5780/5817], Loss: 0.2070, Accuracy: 0.7031\n",
      "Epoch [1/10], Step [5790/5817], Loss: 0.1845, Accuracy: 0.8125\n",
      "Epoch [1/10], Step [5800/5817], Loss: 0.1878, Accuracy: 0.7812\n",
      "Epoch [1/10], Step [5810/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [10/5817], Loss: 0.1918, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [20/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [30/5817], Loss: 0.1814, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [40/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [50/5817], Loss: 0.1936, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [60/5817], Loss: 0.1889, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [70/5817], Loss: 0.1835, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [80/5817], Loss: 0.1863, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [90/5817], Loss: 0.1890, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [100/5817], Loss: 0.1721, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [110/5817], Loss: 0.1909, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [120/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [130/5817], Loss: 0.1851, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [140/5817], Loss: 0.1768, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [150/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [160/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [170/5817], Loss: 0.1872, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [180/5817], Loss: 0.1926, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [190/5817], Loss: 0.1724, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [200/5817], Loss: 0.1945, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [210/5817], Loss: 0.1806, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [220/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [230/5817], Loss: 0.1910, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [240/5817], Loss: 0.1857, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [250/5817], Loss: 0.2032, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [260/5817], Loss: 0.1914, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [270/5817], Loss: 0.1868, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [280/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [290/5817], Loss: 0.1959, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [300/5817], Loss: 0.1842, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [310/5817], Loss: 0.1893, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [320/5817], Loss: 0.1793, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [330/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [340/5817], Loss: 0.1841, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [350/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [360/5817], Loss: 0.1817, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [370/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [380/5817], Loss: 0.1874, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [390/5817], Loss: 0.1789, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [400/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [410/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [420/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [430/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [440/5817], Loss: 0.1881, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [450/5817], Loss: 0.1834, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [460/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [470/5817], Loss: 0.1821, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [480/5817], Loss: 0.1737, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [490/5817], Loss: 0.1912, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [500/5817], Loss: 0.1714, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [510/5817], Loss: 0.1766, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [520/5817], Loss: 0.1702, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [530/5817], Loss: 0.1897, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [540/5817], Loss: 0.1685, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [550/5817], Loss: 0.1931, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [560/5817], Loss: 0.1880, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [570/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [580/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [590/5817], Loss: 0.1795, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [600/5817], Loss: 0.1919, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [610/5817], Loss: 0.1719, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [620/5817], Loss: 0.1832, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [630/5817], Loss: 0.1820, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [640/5817], Loss: 0.1905, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [650/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [660/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [670/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [680/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [690/5817], Loss: 0.1830, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [700/5817], Loss: 0.1885, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [710/5817], Loss: 0.1693, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [720/5817], Loss: 0.1874, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [730/5817], Loss: 0.1875, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [740/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [750/5817], Loss: 0.1760, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [760/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [770/5817], Loss: 0.1901, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [780/5817], Loss: 0.1823, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [790/5817], Loss: 0.1738, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [800/5817], Loss: 0.1825, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [810/5817], Loss: 0.1894, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [820/5817], Loss: 0.1810, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [830/5817], Loss: 0.1873, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [840/5817], Loss: 0.1936, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [850/5817], Loss: 0.1967, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [860/5817], Loss: 0.1723, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [870/5817], Loss: 0.1814, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [880/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [890/5817], Loss: 0.1811, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [900/5817], Loss: 0.1849, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [910/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [920/5817], Loss: 0.1683, Accuracy: 0.8828\n",
      "Epoch [2/10], Step [930/5817], Loss: 0.1894, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [940/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [950/5817], Loss: 0.1862, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [960/5817], Loss: 0.1726, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [970/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [980/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [990/5817], Loss: 0.1851, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [1000/5817], Loss: 0.1672, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [1010/5817], Loss: 0.1950, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [1020/5817], Loss: 0.1714, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [1030/5817], Loss: 0.1725, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [1040/5817], Loss: 0.1865, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1050/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1060/5817], Loss: 0.1815, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1070/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [1080/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [1090/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1100/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [1110/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [1120/5817], Loss: 0.1829, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1130/5817], Loss: 0.1823, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1140/5817], Loss: 0.1707, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [1150/5817], Loss: 0.1742, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [1160/5817], Loss: 0.1871, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [1170/5817], Loss: 0.1716, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [1180/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1190/5817], Loss: 0.1746, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [1200/5817], Loss: 0.1805, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [1210/5817], Loss: 0.1940, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [1220/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [1230/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1240/5817], Loss: 0.1873, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1250/5817], Loss: 0.1815, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1260/5817], Loss: 0.1916, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [1270/5817], Loss: 0.1805, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [1280/5817], Loss: 0.1901, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [1290/5817], Loss: 0.1809, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [1300/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1310/5817], Loss: 0.1871, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1320/5817], Loss: 0.1843, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1330/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1340/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [1350/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [1360/5817], Loss: 0.1931, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [1370/5817], Loss: 0.1834, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1380/5817], Loss: 0.1873, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [1390/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1400/5817], Loss: 0.1983, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [1410/5817], Loss: 0.1873, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [1420/5817], Loss: 0.1831, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1430/5817], Loss: 0.1932, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [1440/5817], Loss: 0.1723, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [1450/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [1460/5817], Loss: 0.1964, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [1470/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [1480/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [1490/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [1500/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [1510/5817], Loss: 0.1925, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [1520/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1530/5817], Loss: 0.1921, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [1540/5817], Loss: 0.2002, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [1550/5817], Loss: 0.1803, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1560/5817], Loss: 0.1910, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [1570/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [1580/5817], Loss: 0.1879, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [1590/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [1600/5817], Loss: 0.1715, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [1610/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1620/5817], Loss: 0.1841, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1630/5817], Loss: 0.2017, Accuracy: 0.7266\n",
      "Epoch [2/10], Step [1640/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [1650/5817], Loss: 0.1732, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [1660/5817], Loss: 0.1714, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [1670/5817], Loss: 0.1974, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [1680/5817], Loss: 0.1881, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1690/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [1700/5817], Loss: 0.1908, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [1710/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1720/5817], Loss: 0.1802, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [1730/5817], Loss: 0.1904, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [1740/5817], Loss: 0.1863, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1750/5817], Loss: 0.1932, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [1760/5817], Loss: 0.1879, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1770/5817], Loss: 0.1813, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [1780/5817], Loss: 0.1775, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [1790/5817], Loss: 0.1818, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1800/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [1810/5817], Loss: 0.1771, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1820/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1830/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1840/5817], Loss: 0.1953, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [1850/5817], Loss: 0.1900, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [1860/5817], Loss: 0.1781, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1870/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1880/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1890/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [1900/5817], Loss: 0.1903, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [1910/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [1920/5817], Loss: 0.1872, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [1930/5817], Loss: 0.1806, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [1940/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [1950/5817], Loss: 0.1872, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [1960/5817], Loss: 0.1747, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [1970/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [1980/5817], Loss: 0.1844, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [1990/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [2000/5817], Loss: 0.1705, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [2010/5817], Loss: 0.1890, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2020/5817], Loss: 0.1876, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2030/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2040/5817], Loss: 0.1828, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2050/5817], Loss: 0.1839, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2060/5817], Loss: 0.1620, Accuracy: 0.9141\n",
      "Epoch [2/10], Step [2070/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [2080/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2090/5817], Loss: 0.1811, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2100/5817], Loss: 0.1712, Accuracy: 0.8828\n",
      "Epoch [2/10], Step [2110/5817], Loss: 0.1923, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [2120/5817], Loss: 0.1834, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2130/5817], Loss: 0.1874, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [2140/5817], Loss: 0.1795, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2150/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2160/5817], Loss: 0.1949, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [2170/5817], Loss: 0.1781, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [2180/5817], Loss: 0.1770, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [2190/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [2200/5817], Loss: 0.1838, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2210/5817], Loss: 0.1865, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2220/5817], Loss: 0.1816, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2230/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2240/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2250/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [2260/5817], Loss: 0.1882, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [2270/5817], Loss: 0.1853, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2280/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2290/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2300/5817], Loss: 0.1847, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2310/5817], Loss: 0.1843, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2320/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2330/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2340/5817], Loss: 0.1610, Accuracy: 0.8984\n",
      "Epoch [2/10], Step [2350/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2360/5817], Loss: 0.1807, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2370/5817], Loss: 0.1917, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [2380/5817], Loss: 0.1753, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2390/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [2400/5817], Loss: 0.1964, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [2410/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2420/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2430/5817], Loss: 0.1781, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2440/5817], Loss: 0.1845, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2450/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2460/5817], Loss: 0.1784, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2470/5817], Loss: 0.1675, Accuracy: 0.8984\n",
      "Epoch [2/10], Step [2480/5817], Loss: 0.1844, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2490/5817], Loss: 0.1861, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [2500/5817], Loss: 0.1901, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [2510/5817], Loss: 0.1800, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2520/5817], Loss: 0.1727, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [2530/5817], Loss: 0.1799, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2540/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2550/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2560/5817], Loss: 0.1737, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [2570/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [2580/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2590/5817], Loss: 0.1753, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2600/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [2610/5817], Loss: 0.1901, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [2620/5817], Loss: 0.1873, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [2630/5817], Loss: 0.1933, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [2640/5817], Loss: 0.1925, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [2650/5817], Loss: 0.1799, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2660/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [2670/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2680/5817], Loss: 0.1815, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2690/5817], Loss: 0.1830, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [2700/5817], Loss: 0.1775, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [2710/5817], Loss: 0.1779, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2720/5817], Loss: 0.1778, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2730/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [2740/5817], Loss: 0.1893, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [2750/5817], Loss: 0.1869, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [2760/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [2770/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2780/5817], Loss: 0.1696, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [2790/5817], Loss: 0.1825, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [2800/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [2810/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2820/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [2830/5817], Loss: 0.1859, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2840/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [2850/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [2860/5817], Loss: 0.1954, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [2870/5817], Loss: 0.1819, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2880/5817], Loss: 0.1972, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [2890/5817], Loss: 0.1976, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [2900/5817], Loss: 0.1901, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [2910/5817], Loss: 0.1904, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [2920/5817], Loss: 0.1705, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [2930/5817], Loss: 0.1851, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2940/5817], Loss: 0.1973, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [2950/5817], Loss: 0.1750, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [2960/5817], Loss: 0.1762, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [2970/5817], Loss: 0.1805, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [2980/5817], Loss: 0.1914, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [2990/5817], Loss: 0.1800, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [3000/5817], Loss: 0.1786, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [3010/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [3020/5817], Loss: 0.1747, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [3030/5817], Loss: 0.1901, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3040/5817], Loss: 0.1784, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [3050/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [3060/5817], Loss: 0.1892, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3070/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [3080/5817], Loss: 0.1796, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [3090/5817], Loss: 0.1978, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [3100/5817], Loss: 0.1769, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [3110/5817], Loss: 0.1787, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [3120/5817], Loss: 0.1870, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3130/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [3140/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [3150/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [3160/5817], Loss: 0.1934, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [3170/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [3180/5817], Loss: 0.1842, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [3190/5817], Loss: 0.1994, Accuracy: 0.7344\n",
      "Epoch [2/10], Step [3200/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [3210/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [3220/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [3230/5817], Loss: 0.1734, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [3240/5817], Loss: 0.1803, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [3250/5817], Loss: 0.1947, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [3260/5817], Loss: 0.1681, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [3270/5817], Loss: 0.1896, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [3280/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [3290/5817], Loss: 0.1825, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3300/5817], Loss: 0.1802, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [3310/5817], Loss: 0.1808, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3320/5817], Loss: 0.1815, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [3330/5817], Loss: 0.1693, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [3340/5817], Loss: 0.1761, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [3350/5817], Loss: 0.1897, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [3360/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3370/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [3380/5817], Loss: 0.1864, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3390/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [3400/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [3410/5817], Loss: 0.1902, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3420/5817], Loss: 0.1806, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3430/5817], Loss: 0.1797, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [3440/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3450/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [3460/5817], Loss: 0.1881, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3470/5817], Loss: 0.1693, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [3480/5817], Loss: 0.1931, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [3490/5817], Loss: 0.1914, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [3500/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [3510/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [3520/5817], Loss: 0.1863, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3530/5817], Loss: 0.1915, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [3540/5817], Loss: 0.1876, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [3550/5817], Loss: 0.1610, Accuracy: 0.9062\n",
      "Epoch [2/10], Step [3560/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [3570/5817], Loss: 0.1801, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [3580/5817], Loss: 0.1821, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3590/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [3600/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [3610/5817], Loss: 0.1813, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3620/5817], Loss: 0.1761, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [3630/5817], Loss: 0.1822, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [3640/5817], Loss: 0.1914, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [3650/5817], Loss: 0.1829, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [3660/5817], Loss: 0.1834, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3670/5817], Loss: 0.1749, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [3680/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [3690/5817], Loss: 0.1764, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [3700/5817], Loss: 0.1889, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [3710/5817], Loss: 0.1902, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [3720/5817], Loss: 0.1918, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [3730/5817], Loss: 0.1888, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3740/5817], Loss: 0.1967, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [3750/5817], Loss: 0.1818, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3760/5817], Loss: 0.1665, Accuracy: 0.8828\n",
      "Epoch [2/10], Step [3770/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [3780/5817], Loss: 0.1918, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [3790/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [3800/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [3810/5817], Loss: 0.1716, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [3820/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [3830/5817], Loss: 0.1758, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [3840/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [3850/5817], Loss: 0.1678, Accuracy: 0.8828\n",
      "Epoch [2/10], Step [3860/5817], Loss: 0.1865, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3870/5817], Loss: 0.1653, Accuracy: 0.8984\n",
      "Epoch [2/10], Step [3880/5817], Loss: 0.1852, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3890/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3900/5817], Loss: 0.1830, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3910/5817], Loss: 0.1856, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3920/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [3930/5817], Loss: 0.1871, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [3940/5817], Loss: 0.1692, Accuracy: 0.8828\n",
      "Epoch [2/10], Step [3950/5817], Loss: 0.1817, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [3960/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3970/5817], Loss: 0.1910, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [3980/5817], Loss: 0.1827, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [3990/5817], Loss: 0.1832, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4000/5817], Loss: 0.1843, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4010/5817], Loss: 0.1723, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [4020/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [4030/5817], Loss: 0.1900, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4040/5817], Loss: 0.1801, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4050/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [4060/5817], Loss: 0.1834, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [4070/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [4080/5817], Loss: 0.1833, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [4090/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [4100/5817], Loss: 0.1948, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [4110/5817], Loss: 0.1720, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [4120/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [4130/5817], Loss: 0.1878, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [4140/5817], Loss: 0.1723, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [4150/5817], Loss: 0.1906, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [4160/5817], Loss: 0.2003, Accuracy: 0.7344\n",
      "Epoch [2/10], Step [4170/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [4180/5817], Loss: 0.1889, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4190/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [4200/5817], Loss: 0.1850, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [4210/5817], Loss: 0.1837, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [4220/5817], Loss: 0.1781, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [4230/5817], Loss: 0.1856, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [4240/5817], Loss: 0.1777, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4250/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4260/5817], Loss: 0.1773, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4270/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [4280/5817], Loss: 0.1809, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4290/5817], Loss: 0.1726, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [4300/5817], Loss: 0.1907, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4310/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4320/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4330/5817], Loss: 0.1760, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4340/5817], Loss: 0.1896, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4350/5817], Loss: 0.1936, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [4360/5817], Loss: 0.1874, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4370/5817], Loss: 0.1877, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [4380/5817], Loss: 0.1873, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4390/5817], Loss: 0.1699, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [4400/5817], Loss: 0.1874, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [4410/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [4420/5817], Loss: 0.1839, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [4430/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [4440/5817], Loss: 0.1833, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4450/5817], Loss: 0.1845, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4460/5817], Loss: 0.1890, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4470/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4480/5817], Loss: 0.1803, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4490/5817], Loss: 0.1952, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [4500/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [4510/5817], Loss: 0.1850, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4520/5817], Loss: 0.1789, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [4530/5817], Loss: 0.1708, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [4540/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4550/5817], Loss: 0.1926, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [4560/5817], Loss: 0.1665, Accuracy: 0.8906\n",
      "Epoch [2/10], Step [4570/5817], Loss: 0.1895, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [4580/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [4590/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [4600/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4610/5817], Loss: 0.1888, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [4620/5817], Loss: 0.1859, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [4630/5817], Loss: 0.1738, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [4640/5817], Loss: 0.1775, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [4650/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [4660/5817], Loss: 0.1814, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [4670/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4680/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [4690/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [4700/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4710/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [4720/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [4730/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4740/5817], Loss: 0.1912, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [4750/5817], Loss: 0.1940, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [4760/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [4770/5817], Loss: 0.1780, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4780/5817], Loss: 0.1820, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4790/5817], Loss: 0.1769, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4800/5817], Loss: 0.1795, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [4810/5817], Loss: 0.1909, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4820/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [4830/5817], Loss: 0.1812, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4840/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [4850/5817], Loss: 0.1856, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [4860/5817], Loss: 0.1821, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [4870/5817], Loss: 0.1711, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [4880/5817], Loss: 0.1818, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4890/5817], Loss: 0.1986, Accuracy: 0.7344\n",
      "Epoch [2/10], Step [4900/5817], Loss: 0.1878, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [4910/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4920/5817], Loss: 0.1727, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [4930/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [4940/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [4950/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [4960/5817], Loss: 0.1865, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [4970/5817], Loss: 0.1837, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [4980/5817], Loss: 0.1888, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [4990/5817], Loss: 0.1707, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [5000/5817], Loss: 0.1682, Accuracy: 0.8750\n",
      "Epoch [2/10], Step [5010/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [5020/5817], Loss: 0.1944, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [5030/5817], Loss: 0.1863, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [5040/5817], Loss: 0.1948, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [5050/5817], Loss: 0.1839, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5060/5817], Loss: 0.1822, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5070/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [5080/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5090/5817], Loss: 0.1888, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [5100/5817], Loss: 0.1837, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [5110/5817], Loss: 0.1972, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [5120/5817], Loss: 0.1825, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5130/5817], Loss: 0.1825, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [5140/5817], Loss: 0.1874, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5150/5817], Loss: 0.1837, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5160/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [5170/5817], Loss: 0.1765, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5180/5817], Loss: 0.1944, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [5190/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [5200/5817], Loss: 0.1702, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [5210/5817], Loss: 0.1742, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [5220/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [5230/5817], Loss: 0.1691, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [5240/5817], Loss: 0.1880, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [5250/5817], Loss: 0.1815, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5260/5817], Loss: 0.1902, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [5270/5817], Loss: 0.1753, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [5280/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5290/5817], Loss: 0.1644, Accuracy: 0.8828\n",
      "Epoch [2/10], Step [5300/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [5310/5817], Loss: 0.1805, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5320/5817], Loss: 0.1837, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5330/5817], Loss: 0.1796, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [5340/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5350/5817], Loss: 0.1824, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5360/5817], Loss: 0.1761, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [5370/5817], Loss: 0.1734, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [5380/5817], Loss: 0.1783, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [5390/5817], Loss: 0.1878, Accuracy: 0.7812\n",
      "Epoch [2/10], Step [5400/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [5410/5817], Loss: 0.1774, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5420/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5430/5817], Loss: 0.1830, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5440/5817], Loss: 0.1959, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [5450/5817], Loss: 0.1684, Accuracy: 0.8906\n",
      "Epoch [2/10], Step [5460/5817], Loss: 0.1983, Accuracy: 0.7500\n",
      "Epoch [2/10], Step [5470/5817], Loss: 0.1892, Accuracy: 0.7891\n",
      "Epoch [2/10], Step [5480/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [5490/5817], Loss: 0.1820, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [5500/5817], Loss: 0.1865, Accuracy: 0.8047\n",
      "Epoch [2/10], Step [5510/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [5520/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [5530/5817], Loss: 0.1816, Accuracy: 0.8125\n",
      "Epoch [2/10], Step [5540/5817], Loss: 0.1646, Accuracy: 0.8828\n",
      "Epoch [2/10], Step [5550/5817], Loss: 0.1723, Accuracy: 0.8438\n",
      "Epoch [2/10], Step [5560/5817], Loss: 0.1933, Accuracy: 0.7578\n",
      "Epoch [2/10], Step [5570/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [5580/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [5590/5817], Loss: 0.1735, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [5600/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [5610/5817], Loss: 0.1983, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [5620/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [2/10], Step [5630/5817], Loss: 0.1705, Accuracy: 0.8594\n",
      "Epoch [2/10], Step [5640/5817], Loss: 0.1774, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [5650/5817], Loss: 0.1722, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [5660/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [2/10], Step [5670/5817], Loss: 0.1690, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [5680/5817], Loss: 0.1946, Accuracy: 0.7656\n",
      "Epoch [2/10], Step [5690/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [5700/5817], Loss: 0.1681, Accuracy: 0.8828\n",
      "Epoch [2/10], Step [5710/5817], Loss: 0.1999, Accuracy: 0.7422\n",
      "Epoch [2/10], Step [5720/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [2/10], Step [5730/5817], Loss: 0.1714, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [5740/5817], Loss: 0.1832, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5750/5817], Loss: 0.1649, Accuracy: 0.8984\n",
      "Epoch [2/10], Step [5760/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [2/10], Step [5770/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [2/10], Step [5780/5817], Loss: 0.1906, Accuracy: 0.7734\n",
      "Epoch [2/10], Step [5790/5817], Loss: 0.1714, Accuracy: 0.8672\n",
      "Epoch [2/10], Step [5800/5817], Loss: 0.1651, Accuracy: 0.8984\n",
      "Epoch [2/10], Step [5810/5817], Loss: 0.1931, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [10/5817], Loss: 0.1815, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [20/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [30/5817], Loss: 0.1881, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [40/5817], Loss: 0.1780, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [50/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [60/5817], Loss: 0.1846, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [70/5817], Loss: 0.1808, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [80/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [90/5817], Loss: 0.1845, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [100/5817], Loss: 0.1715, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [110/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [120/5817], Loss: 0.1692, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [130/5817], Loss: 0.1908, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [140/5817], Loss: 0.1908, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [150/5817], Loss: 0.1879, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [160/5817], Loss: 0.1905, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [170/5817], Loss: 0.1699, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [180/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [190/5817], Loss: 0.1679, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [200/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [210/5817], Loss: 0.1694, Accuracy: 0.8906\n",
      "Epoch [3/10], Step [220/5817], Loss: 0.1780, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [230/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [240/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [250/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [260/5817], Loss: 0.1956, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [270/5817], Loss: 0.1829, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [280/5817], Loss: 0.1853, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [290/5817], Loss: 0.1880, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [300/5817], Loss: 0.1792, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [310/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [320/5817], Loss: 0.1906, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [330/5817], Loss: 0.1844, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [340/5817], Loss: 0.1727, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [350/5817], Loss: 0.1745, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [360/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [370/5817], Loss: 0.1959, Accuracy: 0.7422\n",
      "Epoch [3/10], Step [380/5817], Loss: 0.1829, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [390/5817], Loss: 0.1718, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [400/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [410/5817], Loss: 0.1661, Accuracy: 0.8906\n",
      "Epoch [3/10], Step [420/5817], Loss: 0.1717, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [430/5817], Loss: 0.1907, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [440/5817], Loss: 0.1834, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [450/5817], Loss: 0.1884, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [460/5817], Loss: 0.1921, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [470/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [480/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [490/5817], Loss: 0.1764, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [500/5817], Loss: 0.1978, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [510/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [520/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [530/5817], Loss: 0.1808, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [540/5817], Loss: 0.1738, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [550/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [560/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [570/5817], Loss: 0.1892, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [580/5817], Loss: 0.1854, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [590/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [600/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [610/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [620/5817], Loss: 0.1927, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [630/5817], Loss: 0.1804, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [640/5817], Loss: 0.1780, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [650/5817], Loss: 0.1665, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [660/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [670/5817], Loss: 0.1975, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [680/5817], Loss: 0.1754, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [690/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [700/5817], Loss: 0.1894, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [710/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [720/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [730/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [740/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [750/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [760/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [770/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [780/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [790/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [800/5817], Loss: 0.1822, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [810/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [820/5817], Loss: 0.1693, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [830/5817], Loss: 0.1708, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [840/5817], Loss: 0.1849, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [850/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [860/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [870/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [880/5817], Loss: 0.1805, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [890/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [900/5817], Loss: 0.1793, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [910/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [920/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [930/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [940/5817], Loss: 0.1779, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [950/5817], Loss: 0.1818, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [960/5817], Loss: 0.1864, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [970/5817], Loss: 0.1929, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [980/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [990/5817], Loss: 0.1851, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [1000/5817], Loss: 0.1929, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [1010/5817], Loss: 0.1821, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1020/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [1030/5817], Loss: 0.1778, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [1040/5817], Loss: 0.1816, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1050/5817], Loss: 0.1866, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [1060/5817], Loss: 0.1811, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1070/5817], Loss: 0.1798, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1080/5817], Loss: 0.1690, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [1090/5817], Loss: 0.1851, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [1100/5817], Loss: 0.1830, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1110/5817], Loss: 0.1834, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1120/5817], Loss: 0.1685, Accuracy: 0.8906\n",
      "Epoch [3/10], Step [1130/5817], Loss: 0.1764, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1140/5817], Loss: 0.1835, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1150/5817], Loss: 0.1822, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1160/5817], Loss: 0.1873, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [1170/5817], Loss: 0.1722, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1180/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1190/5817], Loss: 0.1845, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1200/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [1210/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [1220/5817], Loss: 0.1943, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [1230/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [1240/5817], Loss: 0.1697, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [1250/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [1260/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1270/5817], Loss: 0.1789, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1280/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1290/5817], Loss: 0.1909, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [1300/5817], Loss: 0.1773, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1310/5817], Loss: 0.1833, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1320/5817], Loss: 0.1975, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [1330/5817], Loss: 0.1744, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [1340/5817], Loss: 0.1896, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [1350/5817], Loss: 0.1888, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [1360/5817], Loss: 0.1829, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [1370/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [1380/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [1390/5817], Loss: 0.1892, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [1400/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1410/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1420/5817], Loss: 0.1758, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1430/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1440/5817], Loss: 0.1679, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [1450/5817], Loss: 0.1751, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [1460/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [1470/5817], Loss: 0.1714, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1480/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1490/5817], Loss: 0.1808, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1500/5817], Loss: 0.1811, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1510/5817], Loss: 0.1734, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [1520/5817], Loss: 0.1873, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [1530/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [1540/5817], Loss: 0.1813, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [1550/5817], Loss: 0.1808, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [1560/5817], Loss: 0.1766, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1570/5817], Loss: 0.1708, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [1580/5817], Loss: 0.1691, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [1590/5817], Loss: 0.1672, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [1600/5817], Loss: 0.1771, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1610/5817], Loss: 0.1923, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [1620/5817], Loss: 0.1735, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [1630/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [1640/5817], Loss: 0.1850, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [1650/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [1660/5817], Loss: 0.1777, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1670/5817], Loss: 0.1757, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [1680/5817], Loss: 0.1841, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1690/5817], Loss: 0.1859, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [1700/5817], Loss: 0.1880, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [1710/5817], Loss: 0.1809, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [1720/5817], Loss: 0.1805, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1730/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [1740/5817], Loss: 0.1963, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [1750/5817], Loss: 0.1761, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [1760/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [1770/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [1780/5817], Loss: 0.1663, Accuracy: 0.8906\n",
      "Epoch [3/10], Step [1790/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1800/5817], Loss: 0.1783, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [1810/5817], Loss: 0.1932, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [1820/5817], Loss: 0.1682, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [1830/5817], Loss: 0.1841, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1840/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1850/5817], Loss: 0.1814, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1860/5817], Loss: 0.1770, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1870/5817], Loss: 0.1703, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [1880/5817], Loss: 0.1767, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1890/5817], Loss: 0.1735, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [1900/5817], Loss: 0.1767, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [1910/5817], Loss: 0.1824, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [1920/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [1930/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [1940/5817], Loss: 0.1754, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [1950/5817], Loss: 0.1912, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [1960/5817], Loss: 0.1846, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [1970/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [1980/5817], Loss: 0.1946, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [1990/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [2000/5817], Loss: 0.1737, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [2010/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2020/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2030/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [2040/5817], Loss: 0.1760, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2050/5817], Loss: 0.1977, Accuracy: 0.7422\n",
      "Epoch [3/10], Step [2060/5817], Loss: 0.1805, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2070/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [2080/5817], Loss: 0.1847, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2090/5817], Loss: 0.2010, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [2100/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2110/5817], Loss: 0.1825, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2120/5817], Loss: 0.1798, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2130/5817], Loss: 0.1951, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [2140/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2150/5817], Loss: 0.1809, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2160/5817], Loss: 0.1828, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2170/5817], Loss: 0.1823, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2180/5817], Loss: 0.1718, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [2190/5817], Loss: 0.1805, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2200/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2210/5817], Loss: 0.1820, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2220/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2230/5817], Loss: 0.1662, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [2240/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [2250/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2260/5817], Loss: 0.1689, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [2270/5817], Loss: 0.1872, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2280/5817], Loss: 0.1923, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [2290/5817], Loss: 0.1813, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2300/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [2310/5817], Loss: 0.1888, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2320/5817], Loss: 0.1875, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [2330/5817], Loss: 0.1820, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2340/5817], Loss: 0.1676, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [2350/5817], Loss: 0.1815, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2360/5817], Loss: 0.1821, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [2370/5817], Loss: 0.1678, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [2380/5817], Loss: 0.1716, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2390/5817], Loss: 0.1692, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2400/5817], Loss: 0.1836, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2410/5817], Loss: 0.1872, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2420/5817], Loss: 0.1719, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2430/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2440/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2450/5817], Loss: 0.1906, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2460/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [2470/5817], Loss: 0.1665, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [2480/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2490/5817], Loss: 0.1879, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2500/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [2510/5817], Loss: 0.1699, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [2520/5817], Loss: 0.1835, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2530/5817], Loss: 0.1821, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2540/5817], Loss: 0.1723, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [2550/5817], Loss: 0.1823, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2560/5817], Loss: 0.1852, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2570/5817], Loss: 0.1878, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2580/5817], Loss: 0.1812, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2590/5817], Loss: 0.1919, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [2600/5817], Loss: 0.1711, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2610/5817], Loss: 0.1786, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [2620/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2630/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [2640/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [2650/5817], Loss: 0.1908, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [2660/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [2670/5817], Loss: 0.1870, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [2680/5817], Loss: 0.1739, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [2690/5817], Loss: 0.1767, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2700/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2710/5817], Loss: 0.1838, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2720/5817], Loss: 0.1809, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2730/5817], Loss: 0.1974, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [2740/5817], Loss: 0.1800, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2750/5817], Loss: 0.1952, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [2760/5817], Loss: 0.1728, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2770/5817], Loss: 0.1716, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [2780/5817], Loss: 0.1895, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [2790/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2800/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [2810/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [2820/5817], Loss: 0.1741, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [2830/5817], Loss: 0.1859, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2840/5817], Loss: 0.1865, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2850/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [2860/5817], Loss: 0.2037, Accuracy: 0.7266\n",
      "Epoch [3/10], Step [2870/5817], Loss: 0.1962, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [2880/5817], Loss: 0.1795, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [2890/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2900/5817], Loss: 0.1689, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [2910/5817], Loss: 0.1971, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [2920/5817], Loss: 0.1860, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [2930/5817], Loss: 0.1882, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [2940/5817], Loss: 0.1796, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [2950/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [2960/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [2970/5817], Loss: 0.1856, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [2980/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [2990/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [3000/5817], Loss: 0.1824, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [3010/5817], Loss: 0.1903, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [3020/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3030/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [3040/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [3050/5817], Loss: 0.1626, Accuracy: 0.8906\n",
      "Epoch [3/10], Step [3060/5817], Loss: 0.1822, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3070/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3080/5817], Loss: 0.1870, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [3090/5817], Loss: 0.1925, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [3100/5817], Loss: 0.1831, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [3110/5817], Loss: 0.1901, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3120/5817], Loss: 0.1797, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [3130/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [3140/5817], Loss: 0.1805, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [3150/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3160/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3170/5817], Loss: 0.1791, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [3180/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [3190/5817], Loss: 0.1836, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3200/5817], Loss: 0.1961, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [3210/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3220/5817], Loss: 0.1925, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [3230/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [3240/5817], Loss: 0.1744, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [3250/5817], Loss: 0.1782, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [3260/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3270/5817], Loss: 0.1765, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3280/5817], Loss: 0.1789, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3290/5817], Loss: 0.1920, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [3300/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [3310/5817], Loss: 0.1906, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3320/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [3330/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [3340/5817], Loss: 0.1820, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3350/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [3360/5817], Loss: 0.1923, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [3370/5817], Loss: 0.1930, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [3380/5817], Loss: 0.1791, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3390/5817], Loss: 0.1659, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [3400/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [3410/5817], Loss: 0.1648, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [3420/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3430/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3440/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [3450/5817], Loss: 0.1805, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [3460/5817], Loss: 0.1702, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [3470/5817], Loss: 0.1830, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3480/5817], Loss: 0.1811, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [3490/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3500/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [3510/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [3520/5817], Loss: 0.1815, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3530/5817], Loss: 0.1705, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [3540/5817], Loss: 0.1677, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [3550/5817], Loss: 0.1774, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3560/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [3570/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [3580/5817], Loss: 0.1797, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3590/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3600/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3610/5817], Loss: 0.1703, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [3620/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3630/5817], Loss: 0.1769, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [3640/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3650/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [3660/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3670/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [3680/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [3690/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3700/5817], Loss: 0.1861, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3710/5817], Loss: 0.1897, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3720/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3730/5817], Loss: 0.1760, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [3740/5817], Loss: 0.1884, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [3750/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3760/5817], Loss: 0.1626, Accuracy: 0.9062\n",
      "Epoch [3/10], Step [3770/5817], Loss: 0.1697, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [3780/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [3790/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [3800/5817], Loss: 0.1704, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [3810/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [3820/5817], Loss: 0.1872, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [3830/5817], Loss: 0.1911, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [3840/5817], Loss: 0.1879, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [3850/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [3860/5817], Loss: 0.1887, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [3870/5817], Loss: 0.1737, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [3880/5817], Loss: 0.1756, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [3890/5817], Loss: 0.2055, Accuracy: 0.7188\n",
      "Epoch [3/10], Step [3900/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [3910/5817], Loss: 0.1681, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [3920/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [3930/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3940/5817], Loss: 0.1747, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [3950/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [3960/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [3970/5817], Loss: 0.1652, Accuracy: 0.8906\n",
      "Epoch [3/10], Step [3980/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [3990/5817], Loss: 0.1894, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [4000/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [4010/5817], Loss: 0.1851, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4020/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4030/5817], Loss: 0.1800, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4040/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4050/5817], Loss: 0.1819, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4060/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4070/5817], Loss: 0.1825, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4080/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4090/5817], Loss: 0.1810, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4100/5817], Loss: 0.1661, Accuracy: 0.8906\n",
      "Epoch [3/10], Step [4110/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4120/5817], Loss: 0.1854, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4130/5817], Loss: 0.1922, Accuracy: 0.7656\n",
      "Epoch [3/10], Step [4140/5817], Loss: 0.1892, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [4150/5817], Loss: 0.1901, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4160/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [4170/5817], Loss: 0.1798, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4180/5817], Loss: 0.1918, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [4190/5817], Loss: 0.1880, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [4200/5817], Loss: 0.1664, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [4210/5817], Loss: 0.1820, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4220/5817], Loss: 0.1823, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4230/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4240/5817], Loss: 0.1704, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [4250/5817], Loss: 0.1935, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [4260/5817], Loss: 0.1823, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4270/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4280/5817], Loss: 0.1717, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [4290/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [4300/5817], Loss: 0.1844, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4310/5817], Loss: 0.1728, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [4320/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4330/5817], Loss: 0.1979, Accuracy: 0.7422\n",
      "Epoch [3/10], Step [4340/5817], Loss: 0.1887, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [4350/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [4360/5817], Loss: 0.2020, Accuracy: 0.7344\n",
      "Epoch [3/10], Step [4370/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [4380/5817], Loss: 0.1808, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4390/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4400/5817], Loss: 0.1910, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [4410/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [4420/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [4430/5817], Loss: 0.1797, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4440/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4450/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4460/5817], Loss: 0.1718, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [4470/5817], Loss: 0.1845, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4480/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4490/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [4500/5817], Loss: 0.1723, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [4510/5817], Loss: 0.1819, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4520/5817], Loss: 0.1739, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [4530/5817], Loss: 0.1829, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4540/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [4550/5817], Loss: 0.1974, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [4560/5817], Loss: 0.1841, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4570/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4580/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [4590/5817], Loss: 0.1943, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [4600/5817], Loss: 0.1828, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4610/5817], Loss: 0.1671, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [4620/5817], Loss: 0.1732, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [4630/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4640/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4650/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4660/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [4670/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4680/5817], Loss: 0.1836, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4690/5817], Loss: 0.1703, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [4700/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4710/5817], Loss: 0.1677, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [4720/5817], Loss: 0.1923, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [4730/5817], Loss: 0.1803, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4740/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [4750/5817], Loss: 0.1817, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4760/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [4770/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4780/5817], Loss: 0.1943, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [4790/5817], Loss: 0.1772, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4800/5817], Loss: 0.1843, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4810/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4820/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4830/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4840/5817], Loss: 0.1853, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4850/5817], Loss: 0.1768, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4860/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [4870/5817], Loss: 0.1869, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [4880/5817], Loss: 0.1959, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [4890/5817], Loss: 0.1633, Accuracy: 0.8984\n",
      "Epoch [3/10], Step [4900/5817], Loss: 0.1773, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4910/5817], Loss: 0.1906, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [4920/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4930/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4940/5817], Loss: 0.1820, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [4950/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [4960/5817], Loss: 0.1869, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [4970/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [4980/5817], Loss: 0.1861, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [4990/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5000/5817], Loss: 0.1763, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [5010/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [5020/5817], Loss: 0.1933, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [5030/5817], Loss: 0.1927, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [5040/5817], Loss: 0.1714, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5050/5817], Loss: 0.1712, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [5060/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [5070/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [5080/5817], Loss: 0.1781, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [5090/5817], Loss: 0.1838, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [5100/5817], Loss: 0.1746, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [5110/5817], Loss: 0.1766, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [5120/5817], Loss: 0.1732, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5130/5817], Loss: 0.1697, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [5140/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [5150/5817], Loss: 0.1784, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [5160/5817], Loss: 0.1771, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5170/5817], Loss: 0.1915, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [5180/5817], Loss: 0.1871, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [5190/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [5200/5817], Loss: 0.1845, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [5210/5817], Loss: 0.1668, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [5220/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [5230/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [5240/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [5250/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [5260/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5270/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [5280/5817], Loss: 0.1773, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5290/5817], Loss: 0.1812, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [5300/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5310/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [5320/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5330/5817], Loss: 0.1667, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [5340/5817], Loss: 0.1914, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [5350/5817], Loss: 0.1859, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [5360/5817], Loss: 0.1657, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [5370/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [5380/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5390/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [5400/5817], Loss: 0.1714, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [5410/5817], Loss: 0.1784, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [5420/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5430/5817], Loss: 0.1713, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [5440/5817], Loss: 0.1913, Accuracy: 0.7578\n",
      "Epoch [3/10], Step [5450/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [5460/5817], Loss: 0.1670, Accuracy: 0.8906\n",
      "Epoch [3/10], Step [5470/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [5480/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5490/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [5500/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5510/5817], Loss: 0.1714, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5520/5817], Loss: 0.1808, Accuracy: 0.8125\n",
      "Epoch [3/10], Step [5530/5817], Loss: 0.1917, Accuracy: 0.7734\n",
      "Epoch [3/10], Step [5540/5817], Loss: 0.1860, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [5550/5817], Loss: 0.1737, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5560/5817], Loss: 0.1654, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [5570/5817], Loss: 0.1911, Accuracy: 0.7500\n",
      "Epoch [3/10], Step [5580/5817], Loss: 0.1848, Accuracy: 0.7891\n",
      "Epoch [3/10], Step [5590/5817], Loss: 0.1780, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [5600/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [3/10], Step [5610/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [5620/5817], Loss: 0.1803, Accuracy: 0.7969\n",
      "Epoch [3/10], Step [5630/5817], Loss: 0.1683, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [5640/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5650/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [5660/5817], Loss: 0.2010, Accuracy: 0.7344\n",
      "Epoch [3/10], Step [5670/5817], Loss: 0.1872, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [5680/5817], Loss: 0.1669, Accuracy: 0.8750\n",
      "Epoch [3/10], Step [5690/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [5700/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5710/5817], Loss: 0.1881, Accuracy: 0.7812\n",
      "Epoch [3/10], Step [5720/5817], Loss: 0.1834, Accuracy: 0.8047\n",
      "Epoch [3/10], Step [5730/5817], Loss: 0.1689, Accuracy: 0.8828\n",
      "Epoch [3/10], Step [5740/5817], Loss: 0.1787, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5750/5817], Loss: 0.1760, Accuracy: 0.8516\n",
      "Epoch [3/10], Step [5760/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [3/10], Step [5770/5817], Loss: 0.1676, Accuracy: 0.8672\n",
      "Epoch [3/10], Step [5780/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [3/10], Step [5790/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [3/10], Step [5800/5817], Loss: 0.1793, Accuracy: 0.8203\n",
      "Epoch [3/10], Step [5810/5817], Loss: 0.1788, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [10/5817], Loss: 0.1761, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [20/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [30/5817], Loss: 0.1863, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [40/5817], Loss: 0.1838, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [50/5817], Loss: 0.1855, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [60/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [70/5817], Loss: 0.1711, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [80/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [90/5817], Loss: 0.1775, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [100/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [110/5817], Loss: 0.1930, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [120/5817], Loss: 0.1711, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [130/5817], Loss: 0.1823, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [140/5817], Loss: 0.1931, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [150/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [160/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [170/5817], Loss: 0.1715, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [180/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [190/5817], Loss: 0.1628, Accuracy: 0.8984\n",
      "Epoch [4/10], Step [200/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [210/5817], Loss: 0.1691, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [220/5817], Loss: 0.1880, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [230/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [240/5817], Loss: 0.1763, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [250/5817], Loss: 0.1703, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [260/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [270/5817], Loss: 0.1841, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [280/5817], Loss: 0.1835, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [290/5817], Loss: 0.1831, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [300/5817], Loss: 0.1766, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [310/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [320/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [330/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [340/5817], Loss: 0.1951, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [350/5817], Loss: 0.1757, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [360/5817], Loss: 0.1727, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [370/5817], Loss: 0.1954, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [380/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [390/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [400/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [410/5817], Loss: 0.1746, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [420/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [430/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [440/5817], Loss: 0.1780, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [450/5817], Loss: 0.1804, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [460/5817], Loss: 0.1734, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [470/5817], Loss: 0.1645, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [480/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [490/5817], Loss: 0.1800, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [500/5817], Loss: 0.1851, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [510/5817], Loss: 0.1711, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [520/5817], Loss: 0.1738, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [530/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [540/5817], Loss: 0.1916, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [550/5817], Loss: 0.1852, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [560/5817], Loss: 0.1653, Accuracy: 0.8984\n",
      "Epoch [4/10], Step [570/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [580/5817], Loss: 0.1711, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [590/5817], Loss: 0.1702, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [600/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [610/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [620/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [630/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [640/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [650/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [660/5817], Loss: 0.1889, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [670/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [680/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [690/5817], Loss: 0.1760, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [700/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [710/5817], Loss: 0.1749, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [720/5817], Loss: 0.1633, Accuracy: 0.8906\n",
      "Epoch [4/10], Step [730/5817], Loss: 0.1639, Accuracy: 0.8984\n",
      "Epoch [4/10], Step [740/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [750/5817], Loss: 0.1720, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [760/5817], Loss: 0.1787, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [770/5817], Loss: 0.1670, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [780/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [790/5817], Loss: 0.1705, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [800/5817], Loss: 0.1701, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [810/5817], Loss: 0.1688, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [820/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [830/5817], Loss: 0.1878, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [840/5817], Loss: 0.1798, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [850/5817], Loss: 0.1782, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [860/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [870/5817], Loss: 0.1741, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [880/5817], Loss: 0.1833, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [890/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [900/5817], Loss: 0.1645, Accuracy: 0.8984\n",
      "Epoch [4/10], Step [910/5817], Loss: 0.1725, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [920/5817], Loss: 0.1746, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [930/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [940/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [950/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [960/5817], Loss: 0.1744, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [970/5817], Loss: 0.1738, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [980/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [990/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1000/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [1010/5817], Loss: 0.1788, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [1020/5817], Loss: 0.1715, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1030/5817], Loss: 0.1750, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1040/5817], Loss: 0.1713, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [1050/5817], Loss: 0.1728, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [1060/5817], Loss: 0.1773, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1070/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [1080/5817], Loss: 0.1664, Accuracy: 0.8906\n",
      "Epoch [4/10], Step [1090/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1100/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1110/5817], Loss: 0.1796, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1120/5817], Loss: 0.1693, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1130/5817], Loss: 0.1696, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1140/5817], Loss: 0.1842, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1150/5817], Loss: 0.1786, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1160/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1170/5817], Loss: 0.1911, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [1180/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [1190/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1200/5817], Loss: 0.1847, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1210/5817], Loss: 0.1693, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [1220/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1230/5817], Loss: 0.1919, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [1240/5817], Loss: 0.1655, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [1250/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1260/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [1270/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1280/5817], Loss: 0.1809, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1290/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1300/5817], Loss: 0.1845, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [1310/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [1320/5817], Loss: 0.1704, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1330/5817], Loss: 0.1779, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1340/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1350/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [1360/5817], Loss: 0.1782, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1370/5817], Loss: 0.1955, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [1380/5817], Loss: 0.1779, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1390/5817], Loss: 0.1939, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [1400/5817], Loss: 0.1718, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1410/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [1420/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1430/5817], Loss: 0.1739, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1440/5817], Loss: 0.1837, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [1450/5817], Loss: 0.1805, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1460/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [1470/5817], Loss: 0.1712, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1480/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1490/5817], Loss: 0.1754, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1500/5817], Loss: 0.1716, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1510/5817], Loss: 0.1677, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [1520/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1530/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [1540/5817], Loss: 0.1839, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1550/5817], Loss: 0.1787, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1560/5817], Loss: 0.1873, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [1570/5817], Loss: 0.1825, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1580/5817], Loss: 0.1809, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1590/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [1600/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1610/5817], Loss: 0.1642, Accuracy: 0.8984\n",
      "Epoch [4/10], Step [1620/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [1630/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1640/5817], Loss: 0.1639, Accuracy: 0.8984\n",
      "Epoch [4/10], Step [1650/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1660/5817], Loss: 0.1799, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1670/5817], Loss: 0.1831, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [1680/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1690/5817], Loss: 0.1767, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1700/5817], Loss: 0.1839, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [1710/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1720/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1730/5817], Loss: 0.1680, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [1740/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1750/5817], Loss: 0.1675, Accuracy: 0.8906\n",
      "Epoch [4/10], Step [1760/5817], Loss: 0.1761, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1770/5817], Loss: 0.1658, Accuracy: 0.8906\n",
      "Epoch [4/10], Step [1780/5817], Loss: 0.1832, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1790/5817], Loss: 0.1837, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [1800/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1810/5817], Loss: 0.1883, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [1820/5817], Loss: 0.1724, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1830/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [1840/5817], Loss: 0.1796, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [1850/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [1860/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [1870/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1880/5817], Loss: 0.1775, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1890/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [1900/5817], Loss: 0.1825, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1910/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [1920/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [1930/5817], Loss: 0.1871, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [1940/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1950/5817], Loss: 0.1820, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [1960/5817], Loss: 0.1804, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [1970/5817], Loss: 0.1642, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [1980/5817], Loss: 0.1727, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [1990/5817], Loss: 0.1688, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [2000/5817], Loss: 0.2001, Accuracy: 0.7422\n",
      "Epoch [4/10], Step [2010/5817], Loss: 0.1779, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2020/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2030/5817], Loss: 0.1637, Accuracy: 0.8984\n",
      "Epoch [4/10], Step [2040/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2050/5817], Loss: 0.1741, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [2060/5817], Loss: 0.1818, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2070/5817], Loss: 0.1860, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [2080/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2090/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2100/5817], Loss: 0.1699, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [2110/5817], Loss: 0.1848, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [2120/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [2130/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2140/5817], Loss: 0.1757, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [2150/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [2160/5817], Loss: 0.1705, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [2170/5817], Loss: 0.1631, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [2180/5817], Loss: 0.1892, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [2190/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [2200/5817], Loss: 0.1879, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [2210/5817], Loss: 0.1683, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [2220/5817], Loss: 0.1735, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [2230/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [2240/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [2250/5817], Loss: 0.1669, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [2260/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2270/5817], Loss: 0.1953, Accuracy: 0.7500\n",
      "Epoch [4/10], Step [2280/5817], Loss: 0.1844, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [2290/5817], Loss: 0.1820, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2300/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [2310/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [2320/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [2330/5817], Loss: 0.1782, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [2340/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2350/5817], Loss: 0.1908, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [2360/5817], Loss: 0.1881, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [2370/5817], Loss: 0.1714, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [2380/5817], Loss: 0.1727, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2390/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [2400/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2410/5817], Loss: 0.1694, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2420/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [2430/5817], Loss: 0.1670, Accuracy: 0.8906\n",
      "Epoch [4/10], Step [2440/5817], Loss: 0.1678, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [2450/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2460/5817], Loss: 0.1704, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [2470/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [2480/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2490/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [2500/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [2510/5817], Loss: 0.1835, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [2520/5817], Loss: 0.1700, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [2530/5817], Loss: 0.1809, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2540/5817], Loss: 0.1627, Accuracy: 0.8906\n",
      "Epoch [4/10], Step [2550/5817], Loss: 0.1700, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [2560/5817], Loss: 0.1677, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [2570/5817], Loss: 0.1866, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [2580/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [2590/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2600/5817], Loss: 0.1698, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [2610/5817], Loss: 0.1854, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [2620/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [2630/5817], Loss: 0.1844, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [2640/5817], Loss: 0.1664, Accuracy: 0.8906\n",
      "Epoch [4/10], Step [2650/5817], Loss: 0.1952, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [2660/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2670/5817], Loss: 0.1770, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2680/5817], Loss: 0.1808, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [2690/5817], Loss: 0.1781, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2700/5817], Loss: 0.1869, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [2710/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2720/5817], Loss: 0.1766, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2730/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [2740/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2750/5817], Loss: 0.1803, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [2760/5817], Loss: 0.1785, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [2770/5817], Loss: 0.1765, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2780/5817], Loss: 0.1713, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2790/5817], Loss: 0.1893, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [2800/5817], Loss: 0.1888, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [2810/5817], Loss: 0.1766, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2820/5817], Loss: 0.1904, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [2830/5817], Loss: 0.1933, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [2840/5817], Loss: 0.1771, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2850/5817], Loss: 0.1791, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [2860/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [2870/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [2880/5817], Loss: 0.1975, Accuracy: 0.7422\n",
      "Epoch [4/10], Step [2890/5817], Loss: 0.1864, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [2900/5817], Loss: 0.1744, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [2910/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [2920/5817], Loss: 0.1708, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [2930/5817], Loss: 0.1744, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [2940/5817], Loss: 0.1710, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [2950/5817], Loss: 0.1687, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [2960/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [2970/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [2980/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [2990/5817], Loss: 0.1807, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3000/5817], Loss: 0.1780, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3010/5817], Loss: 0.1944, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [3020/5817], Loss: 0.1743, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3030/5817], Loss: 0.1737, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3040/5817], Loss: 0.1762, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3050/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3060/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [3070/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [3080/5817], Loss: 0.1791, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3090/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [3100/5817], Loss: 0.1806, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [3110/5817], Loss: 0.1747, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3120/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [3130/5817], Loss: 0.1764, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3140/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3150/5817], Loss: 0.1856, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [3160/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3170/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3180/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3190/5817], Loss: 0.1663, Accuracy: 0.8906\n",
      "Epoch [4/10], Step [3200/5817], Loss: 0.1695, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [3210/5817], Loss: 0.1764, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3220/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3230/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3240/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3250/5817], Loss: 0.1892, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [3260/5817], Loss: 0.1814, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3270/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3280/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3290/5817], Loss: 0.1845, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3300/5817], Loss: 0.1940, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [3310/5817], Loss: 0.1787, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3320/5817], Loss: 0.1925, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [3330/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3340/5817], Loss: 0.1817, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3350/5817], Loss: 0.1821, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [3360/5817], Loss: 0.1737, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3370/5817], Loss: 0.1640, Accuracy: 0.8984\n",
      "Epoch [4/10], Step [3380/5817], Loss: 0.1822, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3390/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3400/5817], Loss: 0.1778, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3410/5817], Loss: 0.1706, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [3420/5817], Loss: 0.1726, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3430/5817], Loss: 0.1738, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3440/5817], Loss: 0.1829, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [3450/5817], Loss: 0.1861, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [3460/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3470/5817], Loss: 0.1710, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [3480/5817], Loss: 0.1887, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [3490/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3500/5817], Loss: 0.1798, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [3510/5817], Loss: 0.1723, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3520/5817], Loss: 0.1760, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3530/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [3540/5817], Loss: 0.1725, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [3550/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3560/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3570/5817], Loss: 0.1799, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [3580/5817], Loss: 0.1656, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [3590/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [3600/5817], Loss: 0.1701, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3610/5817], Loss: 0.1886, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [3620/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3630/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3640/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [3650/5817], Loss: 0.1739, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3660/5817], Loss: 0.1815, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3670/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [3680/5817], Loss: 0.1708, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3690/5817], Loss: 0.1662, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [3700/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3710/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3720/5817], Loss: 0.1707, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [3730/5817], Loss: 0.1841, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [3740/5817], Loss: 0.1748, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3750/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [3760/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3770/5817], Loss: 0.1728, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3780/5817], Loss: 0.1774, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3790/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3800/5817], Loss: 0.1789, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3810/5817], Loss: 0.1986, Accuracy: 0.7500\n",
      "Epoch [4/10], Step [3820/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3830/5817], Loss: 0.1801, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3840/5817], Loss: 0.1792, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3850/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3860/5817], Loss: 0.1764, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3870/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3880/5817], Loss: 0.1714, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [3890/5817], Loss: 0.1834, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [3900/5817], Loss: 0.1825, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [3910/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [3920/5817], Loss: 0.1732, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [3930/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [3940/5817], Loss: 0.1954, Accuracy: 0.7578\n",
      "Epoch [4/10], Step [3950/5817], Loss: 0.1887, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [3960/5817], Loss: 0.1690, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [3970/5817], Loss: 0.1799, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [3980/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [3990/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4000/5817], Loss: 0.1829, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [4010/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4020/5817], Loss: 0.1756, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [4030/5817], Loss: 0.1701, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [4040/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4050/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4060/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4070/5817], Loss: 0.1753, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4080/5817], Loss: 0.1610, Accuracy: 0.9062\n",
      "Epoch [4/10], Step [4090/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [4100/5817], Loss: 0.1802, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4110/5817], Loss: 0.1753, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4120/5817], Loss: 0.1629, Accuracy: 0.8906\n",
      "Epoch [4/10], Step [4130/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4140/5817], Loss: 0.1833, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4150/5817], Loss: 0.1779, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4160/5817], Loss: 0.1612, Accuracy: 0.9062\n",
      "Epoch [4/10], Step [4170/5817], Loss: 0.1665, Accuracy: 0.8984\n",
      "Epoch [4/10], Step [4180/5817], Loss: 0.1730, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [4190/5817], Loss: 0.1872, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [4200/5817], Loss: 0.1713, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [4210/5817], Loss: 0.1783, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4220/5817], Loss: 0.1721, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [4230/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [4240/5817], Loss: 0.1758, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [4250/5817], Loss: 0.1830, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4260/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4270/5817], Loss: 0.1711, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [4280/5817], Loss: 0.1742, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4290/5817], Loss: 0.1819, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [4300/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4310/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4320/5817], Loss: 0.1613, Accuracy: 0.9062\n",
      "Epoch [4/10], Step [4330/5817], Loss: 0.1778, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4340/5817], Loss: 0.1735, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [4350/5817], Loss: 0.1778, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [4360/5817], Loss: 0.1634, Accuracy: 0.8984\n",
      "Epoch [4/10], Step [4370/5817], Loss: 0.1815, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [4380/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4390/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [4400/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [4410/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4420/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4430/5817], Loss: 0.1937, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [4440/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4450/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4460/5817], Loss: 0.1734, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4470/5817], Loss: 0.1698, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [4480/5817], Loss: 0.1769, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4490/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4500/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [4510/5817], Loss: 0.1722, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [4520/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [4530/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [4540/5817], Loss: 0.1804, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4550/5817], Loss: 0.1701, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [4560/5817], Loss: 0.1849, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [4570/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [4580/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [4590/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [4600/5817], Loss: 0.1911, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [4610/5817], Loss: 0.1671, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [4620/5817], Loss: 0.1923, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [4630/5817], Loss: 0.1713, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [4640/5817], Loss: 0.1670, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [4650/5817], Loss: 0.1808, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4660/5817], Loss: 0.1894, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [4670/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4680/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [4690/5817], Loss: 0.1659, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [4700/5817], Loss: 0.1773, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4710/5817], Loss: 0.1884, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [4720/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4730/5817], Loss: 0.1815, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4740/5817], Loss: 0.1856, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [4750/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [4760/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4770/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4780/5817], Loss: 0.1761, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4790/5817], Loss: 0.1754, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4800/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [4810/5817], Loss: 0.1658, Accuracy: 0.8906\n",
      "Epoch [4/10], Step [4820/5817], Loss: 0.1838, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [4830/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [4840/5817], Loss: 0.1746, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4850/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4860/5817], Loss: 0.1719, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [4870/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [4880/5817], Loss: 0.1744, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [4890/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4900/5817], Loss: 0.1662, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [4910/5817], Loss: 0.1681, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [4920/5817], Loss: 0.1706, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [4930/5817], Loss: 0.1828, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [4940/5817], Loss: 0.1811, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [4950/5817], Loss: 0.1810, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4960/5817], Loss: 0.1915, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [4970/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [4980/5817], Loss: 0.1758, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [4990/5817], Loss: 0.1822, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5000/5817], Loss: 0.1737, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [5010/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [5020/5817], Loss: 0.1733, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [5030/5817], Loss: 0.1677, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [5040/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [5050/5817], Loss: 0.1810, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5060/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [5070/5817], Loss: 0.1839, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [5080/5817], Loss: 0.1882, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [5090/5817], Loss: 0.1697, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [5100/5817], Loss: 0.1757, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [5110/5817], Loss: 0.1810, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5120/5817], Loss: 0.1681, Accuracy: 0.8828\n",
      "Epoch [4/10], Step [5130/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [5140/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5150/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [5160/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [5170/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [5180/5817], Loss: 0.1961, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [5190/5817], Loss: 0.1743, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [5200/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [5210/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [5220/5817], Loss: 0.1856, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5230/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [5240/5817], Loss: 0.1922, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [5250/5817], Loss: 0.1760, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [5260/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5270/5817], Loss: 0.1946, Accuracy: 0.7656\n",
      "Epoch [4/10], Step [5280/5817], Loss: 0.1824, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5290/5817], Loss: 0.1900, Accuracy: 0.7734\n",
      "Epoch [4/10], Step [5300/5817], Loss: 0.1697, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [5310/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [5320/5817], Loss: 0.1818, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5330/5817], Loss: 0.1812, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [5340/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5350/5817], Loss: 0.1791, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5360/5817], Loss: 0.1802, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5370/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5380/5817], Loss: 0.1780, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5390/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [5400/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5410/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [5420/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5430/5817], Loss: 0.1787, Accuracy: 0.8203\n",
      "Epoch [4/10], Step [5440/5817], Loss: 0.1697, Accuracy: 0.8750\n",
      "Epoch [4/10], Step [5450/5817], Loss: 0.1816, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5460/5817], Loss: 0.1863, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5470/5817], Loss: 0.1726, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [5480/5817], Loss: 0.1744, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [5490/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5500/5817], Loss: 0.1711, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [5510/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [5520/5817], Loss: 0.1815, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5530/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5540/5817], Loss: 0.1872, Accuracy: 0.7969\n",
      "Epoch [4/10], Step [5550/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5560/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [5570/5817], Loss: 0.1895, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [5580/5817], Loss: 0.1880, Accuracy: 0.7891\n",
      "Epoch [4/10], Step [5590/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5600/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [4/10], Step [5610/5817], Loss: 0.1912, Accuracy: 0.7812\n",
      "Epoch [4/10], Step [5620/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5630/5817], Loss: 0.1852, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5640/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5650/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [5660/5817], Loss: 0.1750, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [5670/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [5680/5817], Loss: 0.1747, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [5690/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5700/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [4/10], Step [5710/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [5720/5817], Loss: 0.1728, Accuracy: 0.8516\n",
      "Epoch [4/10], Step [5730/5817], Loss: 0.1760, Accuracy: 0.8359\n",
      "Epoch [4/10], Step [5740/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5750/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5760/5817], Loss: 0.1725, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [5770/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [4/10], Step [5780/5817], Loss: 0.1747, Accuracy: 0.8672\n",
      "Epoch [4/10], Step [5790/5817], Loss: 0.1847, Accuracy: 0.8125\n",
      "Epoch [4/10], Step [5800/5817], Loss: 0.1816, Accuracy: 0.8047\n",
      "Epoch [4/10], Step [5810/5817], Loss: 0.1833, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [10/5817], Loss: 0.1585, Accuracy: 0.9219\n",
      "Epoch [5/10], Step [20/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [30/5817], Loss: 0.1883, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [40/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [50/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [60/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [70/5817], Loss: 0.1696, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [80/5817], Loss: 0.1761, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [90/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [100/5817], Loss: 0.1677, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [110/5817], Loss: 0.1659, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [120/5817], Loss: 0.1919, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [130/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [140/5817], Loss: 0.1789, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [150/5817], Loss: 0.1856, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [160/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [170/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [180/5817], Loss: 0.1659, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [190/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [200/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [210/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [220/5817], Loss: 0.1744, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [230/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [240/5817], Loss: 0.1754, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [250/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [260/5817], Loss: 0.1763, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [270/5817], Loss: 0.1688, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [280/5817], Loss: 0.1916, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [290/5817], Loss: 0.1585, Accuracy: 0.9219\n",
      "Epoch [5/10], Step [300/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [310/5817], Loss: 0.1684, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [320/5817], Loss: 0.1687, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [330/5817], Loss: 0.1796, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [340/5817], Loss: 0.1672, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [350/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [360/5817], Loss: 0.1604, Accuracy: 0.9141\n",
      "Epoch [5/10], Step [370/5817], Loss: 0.1920, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [380/5817], Loss: 0.1748, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [390/5817], Loss: 0.1641, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [400/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [410/5817], Loss: 0.1684, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [420/5817], Loss: 0.1720, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [430/5817], Loss: 0.1714, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [440/5817], Loss: 0.1827, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [450/5817], Loss: 0.1531, Accuracy: 0.9453\n",
      "Epoch [5/10], Step [460/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [470/5817], Loss: 0.1821, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [480/5817], Loss: 0.1643, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [490/5817], Loss: 0.1885, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [500/5817], Loss: 0.1780, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [510/5817], Loss: 0.1731, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [520/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [530/5817], Loss: 0.1859, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [540/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [550/5817], Loss: 0.1677, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [560/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [570/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [580/5817], Loss: 0.1754, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [590/5817], Loss: 0.1794, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [600/5817], Loss: 0.1693, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [610/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [620/5817], Loss: 0.1684, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [630/5817], Loss: 0.1758, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [640/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [650/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [660/5817], Loss: 0.1662, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [670/5817], Loss: 0.1851, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [680/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [690/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [700/5817], Loss: 0.1728, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [710/5817], Loss: 0.1829, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [720/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [730/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [740/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [750/5817], Loss: 0.1886, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [760/5817], Loss: 0.1702, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [770/5817], Loss: 0.1656, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [780/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [790/5817], Loss: 0.1925, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [800/5817], Loss: 0.1747, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [810/5817], Loss: 0.1894, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [820/5817], Loss: 0.1855, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [830/5817], Loss: 0.1748, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [840/5817], Loss: 0.1801, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [850/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [860/5817], Loss: 0.1702, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [870/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [880/5817], Loss: 0.1659, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [890/5817], Loss: 0.1777, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [900/5817], Loss: 0.1707, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [910/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [920/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [930/5817], Loss: 0.1876, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [940/5817], Loss: 0.1850, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [950/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [960/5817], Loss: 0.1676, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [970/5817], Loss: 0.1762, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [980/5817], Loss: 0.1790, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [990/5817], Loss: 0.1744, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [1000/5817], Loss: 0.1847, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1010/5817], Loss: 0.1912, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [1020/5817], Loss: 0.1867, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [1030/5817], Loss: 0.1737, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1040/5817], Loss: 0.1815, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1050/5817], Loss: 0.1917, Accuracy: 0.7656\n",
      "Epoch [5/10], Step [1060/5817], Loss: 0.1666, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [1070/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1080/5817], Loss: 0.1649, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [1090/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1100/5817], Loss: 0.1807, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [1110/5817], Loss: 0.1872, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [1120/5817], Loss: 0.1669, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [1130/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1140/5817], Loss: 0.1845, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [1150/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [1160/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [1170/5817], Loss: 0.1758, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1180/5817], Loss: 0.1704, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [1190/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1200/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [1210/5817], Loss: 0.1655, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [1220/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [1230/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1240/5817], Loss: 0.1796, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [1250/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1260/5817], Loss: 0.1652, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [1270/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1280/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [1290/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1300/5817], Loss: 0.1724, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1310/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [1320/5817], Loss: 0.1843, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1330/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1340/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [1350/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1360/5817], Loss: 0.1648, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [1370/5817], Loss: 0.1918, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [1380/5817], Loss: 0.1680, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [1390/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [1400/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1410/5817], Loss: 0.1760, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1420/5817], Loss: 0.1800, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1430/5817], Loss: 0.1779, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [1440/5817], Loss: 0.1688, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [1450/5817], Loss: 0.1749, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [1460/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [1470/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1480/5817], Loss: 0.1685, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [1490/5817], Loss: 0.1762, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1500/5817], Loss: 0.1710, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [1510/5817], Loss: 0.1655, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [1520/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1530/5817], Loss: 0.1684, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [1540/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [1550/5817], Loss: 0.1784, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1560/5817], Loss: 0.1748, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [1570/5817], Loss: 0.1806, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1580/5817], Loss: 0.1678, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [1590/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [1600/5817], Loss: 0.1661, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [1610/5817], Loss: 0.1856, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [1620/5817], Loss: 0.1898, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [1630/5817], Loss: 0.1837, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [1640/5817], Loss: 0.1757, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1650/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1660/5817], Loss: 0.1857, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1670/5817], Loss: 0.1702, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [1680/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1690/5817], Loss: 0.1880, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [1700/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1710/5817], Loss: 0.1871, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [1720/5817], Loss: 0.1689, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [1730/5817], Loss: 0.1823, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1740/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [1750/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [1760/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [1770/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1780/5817], Loss: 0.1789, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1790/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1800/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1810/5817], Loss: 0.1848, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [1820/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1830/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [1840/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1850/5817], Loss: 0.1776, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [1860/5817], Loss: 0.1699, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [1870/5817], Loss: 0.1882, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [1880/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [1890/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1900/5817], Loss: 0.1824, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1910/5817], Loss: 0.1831, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [1920/5817], Loss: 0.1756, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1930/5817], Loss: 0.1728, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [1940/5817], Loss: 0.1802, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1950/5817], Loss: 0.1756, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [1960/5817], Loss: 0.1908, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [1970/5817], Loss: 0.1864, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [1980/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [1990/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2000/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2010/5817], Loss: 0.1917, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [2020/5817], Loss: 0.1734, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2030/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2040/5817], Loss: 0.1765, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2050/5817], Loss: 0.1733, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [2060/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [2070/5817], Loss: 0.1754, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2080/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2090/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [2100/5817], Loss: 0.1977, Accuracy: 0.7500\n",
      "Epoch [5/10], Step [2110/5817], Loss: 0.1669, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [2120/5817], Loss: 0.1937, Accuracy: 0.7656\n",
      "Epoch [5/10], Step [2130/5817], Loss: 0.1690, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [2140/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [2150/5817], Loss: 0.1724, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2160/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2170/5817], Loss: 0.1680, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [2180/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2190/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [2200/5817], Loss: 0.1886, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [2210/5817], Loss: 0.1759, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2220/5817], Loss: 0.1762, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2230/5817], Loss: 0.1701, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2240/5817], Loss: 0.1875, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [2250/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2260/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [2270/5817], Loss: 0.1736, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [2280/5817], Loss: 0.1626, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [2290/5817], Loss: 0.1770, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2300/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [2310/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2320/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2330/5817], Loss: 0.1787, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2340/5817], Loss: 0.1923, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [2350/5817], Loss: 0.1727, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2360/5817], Loss: 0.1726, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2370/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2380/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2390/5817], Loss: 0.1691, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [2400/5817], Loss: 0.1641, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [2410/5817], Loss: 0.1774, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2420/5817], Loss: 0.1893, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [2430/5817], Loss: 0.1689, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [2440/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2450/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [2460/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2470/5817], Loss: 0.1718, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2480/5817], Loss: 0.1836, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [2490/5817], Loss: 0.1900, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [2500/5817], Loss: 0.1701, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [2510/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2520/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2530/5817], Loss: 0.1750, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2540/5817], Loss: 0.1750, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2550/5817], Loss: 0.1900, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [2560/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [2570/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2580/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2590/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2600/5817], Loss: 0.1857, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [2610/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2620/5817], Loss: 0.1949, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [2630/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [2640/5817], Loss: 0.1591, Accuracy: 0.9141\n",
      "Epoch [5/10], Step [2650/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2660/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2670/5817], Loss: 0.1807, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [2680/5817], Loss: 0.1788, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2690/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2700/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2710/5817], Loss: 0.1695, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [2720/5817], Loss: 0.1738, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2730/5817], Loss: 0.1691, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [2740/5817], Loss: 0.1874, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [2750/5817], Loss: 0.1680, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [2760/5817], Loss: 0.1757, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2770/5817], Loss: 0.1702, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [2780/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2790/5817], Loss: 0.1928, Accuracy: 0.7656\n",
      "Epoch [5/10], Step [2800/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [2810/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2820/5817], Loss: 0.1613, Accuracy: 0.9062\n",
      "Epoch [5/10], Step [2830/5817], Loss: 0.1671, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [2840/5817], Loss: 0.1840, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [2850/5817], Loss: 0.1832, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [2860/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2870/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2880/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [2890/5817], Loss: 0.1823, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [2900/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [2910/5817], Loss: 0.1743, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [2920/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [2930/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [2940/5817], Loss: 0.1911, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [2950/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [2960/5817], Loss: 0.1845, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [2970/5817], Loss: 0.1741, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [2980/5817], Loss: 0.1690, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [2990/5817], Loss: 0.1648, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [3000/5817], Loss: 0.1787, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3010/5817], Loss: 0.1708, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [3020/5817], Loss: 0.1852, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [3030/5817], Loss: 0.1697, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [3040/5817], Loss: 0.1866, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [3050/5817], Loss: 0.1778, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3060/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [3070/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3080/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3090/5817], Loss: 0.1798, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [3100/5817], Loss: 0.1734, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3110/5817], Loss: 0.1693, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [3120/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [3130/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3140/5817], Loss: 0.1662, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [3150/5817], Loss: 0.1878, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [3160/5817], Loss: 0.1795, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3170/5817], Loss: 0.1924, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [3180/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3190/5817], Loss: 0.1643, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [3200/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3210/5817], Loss: 0.1678, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [3220/5817], Loss: 0.1689, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [3230/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3240/5817], Loss: 0.1731, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [3250/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [3260/5817], Loss: 0.1919, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [3270/5817], Loss: 0.1619, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [3280/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [3290/5817], Loss: 0.1851, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [3300/5817], Loss: 0.1798, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3310/5817], Loss: 0.1834, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [3320/5817], Loss: 0.1713, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [3330/5817], Loss: 0.1880, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [3340/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [3350/5817], Loss: 0.1658, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [3360/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3370/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [3380/5817], Loss: 0.1879, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [3390/5817], Loss: 0.1849, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [3400/5817], Loss: 0.1888, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [3410/5817], Loss: 0.1737, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3420/5817], Loss: 0.1664, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [3430/5817], Loss: 0.1744, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3440/5817], Loss: 0.1931, Accuracy: 0.7656\n",
      "Epoch [5/10], Step [3450/5817], Loss: 0.1699, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [3460/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3470/5817], Loss: 0.1687, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [3480/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [3490/5817], Loss: 0.1733, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [3500/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3510/5817], Loss: 0.1670, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [3520/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3530/5817], Loss: 0.1706, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [3540/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3550/5817], Loss: 0.1713, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [3560/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [3570/5817], Loss: 0.1723, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [3580/5817], Loss: 0.1786, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3590/5817], Loss: 0.1676, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [3600/5817], Loss: 0.1886, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [3610/5817], Loss: 0.1758, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3620/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3630/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [3640/5817], Loss: 0.1625, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [3650/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [3660/5817], Loss: 0.1869, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [3670/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3680/5817], Loss: 0.1709, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [3690/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [3700/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3710/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [3720/5817], Loss: 0.1843, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [3730/5817], Loss: 0.1920, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [3740/5817], Loss: 0.1736, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [3750/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [3760/5817], Loss: 0.1757, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3770/5817], Loss: 0.1809, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [3780/5817], Loss: 0.1682, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [3790/5817], Loss: 0.1723, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [3800/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [3810/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [3820/5817], Loss: 0.1970, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [3830/5817], Loss: 0.1640, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [3840/5817], Loss: 0.1830, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [3850/5817], Loss: 0.1616, Accuracy: 0.9062\n",
      "Epoch [5/10], Step [3860/5817], Loss: 0.1639, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [3870/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [3880/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [3890/5817], Loss: 0.1829, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [3900/5817], Loss: 0.1647, Accuracy: 0.9062\n",
      "Epoch [5/10], Step [3910/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [3920/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3930/5817], Loss: 0.1691, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [3940/5817], Loss: 0.1801, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [3950/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [3960/5817], Loss: 0.1866, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [3970/5817], Loss: 0.1861, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [3980/5817], Loss: 0.1868, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [3990/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [4000/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4010/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [4020/5817], Loss: 0.1809, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4030/5817], Loss: 0.1762, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4040/5817], Loss: 0.1623, Accuracy: 0.9062\n",
      "Epoch [5/10], Step [4050/5817], Loss: 0.1728, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [4060/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4070/5817], Loss: 0.1922, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [4080/5817], Loss: 0.1870, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [4090/5817], Loss: 0.1789, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4100/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4110/5817], Loss: 0.1652, Accuracy: 0.9062\n",
      "Epoch [5/10], Step [4120/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4130/5817], Loss: 0.1853, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4140/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [4150/5817], Loss: 0.1740, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [4160/5817], Loss: 0.1738, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [4170/5817], Loss: 0.1633, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [4180/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [4190/5817], Loss: 0.1761, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4200/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [4210/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [4220/5817], Loss: 0.1595, Accuracy: 0.9219\n",
      "Epoch [5/10], Step [4230/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4240/5817], Loss: 0.1685, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [4250/5817], Loss: 0.1726, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [4260/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [4270/5817], Loss: 0.1661, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [4280/5817], Loss: 0.1854, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [4290/5817], Loss: 0.1742, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [4300/5817], Loss: 0.1690, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [4310/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4320/5817], Loss: 0.1904, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [4330/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [4340/5817], Loss: 0.1593, Accuracy: 0.9141\n",
      "Epoch [5/10], Step [4350/5817], Loss: 0.1800, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4360/5817], Loss: 0.1778, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4370/5817], Loss: 0.1694, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [4380/5817], Loss: 0.1790, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4390/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4400/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4410/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4420/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [4430/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [4440/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [4450/5817], Loss: 0.1758, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4460/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4470/5817], Loss: 0.1857, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4480/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4490/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4500/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4510/5817], Loss: 0.1740, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [4520/5817], Loss: 0.1737, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [4530/5817], Loss: 0.1685, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [4540/5817], Loss: 0.1905, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [4550/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [4560/5817], Loss: 0.1652, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [4570/5817], Loss: 0.1887, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4580/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4590/5817], Loss: 0.1856, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4600/5817], Loss: 0.1624, Accuracy: 0.9062\n",
      "Epoch [5/10], Step [4610/5817], Loss: 0.1740, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [4620/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [4630/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [4640/5817], Loss: 0.1656, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [4650/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4660/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4670/5817], Loss: 0.1744, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4680/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [4690/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [4700/5817], Loss: 0.1570, Accuracy: 0.9297\n",
      "Epoch [5/10], Step [4710/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [4720/5817], Loss: 0.1723, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [4730/5817], Loss: 0.1834, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [4740/5817], Loss: 0.1923, Accuracy: 0.7578\n",
      "Epoch [5/10], Step [4750/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4760/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [4770/5817], Loss: 0.1670, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [4780/5817], Loss: 0.1852, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [4790/5817], Loss: 0.1839, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4800/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [4810/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4820/5817], Loss: 0.1945, Accuracy: 0.7500\n",
      "Epoch [5/10], Step [4830/5817], Loss: 0.1751, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [4840/5817], Loss: 0.1691, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [4850/5817], Loss: 0.1983, Accuracy: 0.7500\n",
      "Epoch [5/10], Step [4860/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [4870/5817], Loss: 0.1862, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [4880/5817], Loss: 0.1856, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [4890/5817], Loss: 0.1808, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [4900/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [4910/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [4920/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4930/5817], Loss: 0.1821, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [4940/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [4950/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [4960/5817], Loss: 0.1661, Accuracy: 0.8828\n",
      "Epoch [5/10], Step [4970/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [4980/5817], Loss: 0.1690, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [4990/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5000/5817], Loss: 0.1802, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5010/5817], Loss: 0.1926, Accuracy: 0.7734\n",
      "Epoch [5/10], Step [5020/5817], Loss: 0.1841, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [5030/5817], Loss: 0.1833, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5040/5817], Loss: 0.1693, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [5050/5817], Loss: 0.1685, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [5060/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5070/5817], Loss: 0.1835, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5080/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5090/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [5100/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5110/5817], Loss: 0.1812, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5120/5817], Loss: 0.1687, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [5130/5817], Loss: 0.1628, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [5140/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5150/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5160/5817], Loss: 0.1735, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [5170/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [5180/5817], Loss: 0.1838, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5190/5817], Loss: 0.1853, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [5200/5817], Loss: 0.1729, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [5210/5817], Loss: 0.1834, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5220/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [5230/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [5240/5817], Loss: 0.1879, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5250/5817], Loss: 0.1714, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [5260/5817], Loss: 0.1680, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [5270/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5280/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [5290/5817], Loss: 0.1864, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5300/5817], Loss: 0.1789, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5310/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5320/5817], Loss: 0.1747, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5330/5817], Loss: 0.1830, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5340/5817], Loss: 0.1659, Accuracy: 0.8906\n",
      "Epoch [5/10], Step [5350/5817], Loss: 0.1661, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [5360/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5370/5817], Loss: 0.1895, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [5380/5817], Loss: 0.1719, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [5390/5817], Loss: 0.1737, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [5400/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5410/5817], Loss: 0.1718, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [5420/5817], Loss: 0.1647, Accuracy: 0.8984\n",
      "Epoch [5/10], Step [5430/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5440/5817], Loss: 0.1723, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [5450/5817], Loss: 0.1821, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [5460/5817], Loss: 0.1892, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [5470/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5480/5817], Loss: 0.1796, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5490/5817], Loss: 0.1712, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [5500/5817], Loss: 0.1780, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5510/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5520/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5530/5817], Loss: 0.1817, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [5540/5817], Loss: 0.1857, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [5550/5817], Loss: 0.1793, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5560/5817], Loss: 0.1831, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5570/5817], Loss: 0.1704, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [5580/5817], Loss: 0.1691, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [5590/5817], Loss: 0.1677, Accuracy: 0.8750\n",
      "Epoch [5/10], Step [5600/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5610/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [5620/5817], Loss: 0.1700, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [5630/5817], Loss: 0.1838, Accuracy: 0.8125\n",
      "Epoch [5/10], Step [5640/5817], Loss: 0.1811, Accuracy: 0.7969\n",
      "Epoch [5/10], Step [5650/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [5660/5817], Loss: 0.1909, Accuracy: 0.7812\n",
      "Epoch [5/10], Step [5670/5817], Loss: 0.1868, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5680/5817], Loss: 0.1768, Accuracy: 0.8281\n",
      "Epoch [5/10], Step [5690/5817], Loss: 0.1864, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5700/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [5/10], Step [5710/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5720/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [5/10], Step [5730/5817], Loss: 0.1646, Accuracy: 0.9062\n",
      "Epoch [5/10], Step [5740/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [5750/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [5/10], Step [5760/5817], Loss: 0.1689, Accuracy: 0.8672\n",
      "Epoch [5/10], Step [5770/5817], Loss: 0.1700, Accuracy: 0.8594\n",
      "Epoch [5/10], Step [5780/5817], Loss: 0.1912, Accuracy: 0.7891\n",
      "Epoch [5/10], Step [5790/5817], Loss: 0.1833, Accuracy: 0.8047\n",
      "Epoch [5/10], Step [5800/5817], Loss: 0.1802, Accuracy: 0.8359\n",
      "Epoch [5/10], Step [5810/5817], Loss: 0.1769, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [10/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [20/5817], Loss: 0.1649, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [30/5817], Loss: 0.1719, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [40/5817], Loss: 0.1687, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [50/5817], Loss: 0.1727, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [60/5817], Loss: 0.1825, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [70/5817], Loss: 0.1759, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [80/5817], Loss: 0.1865, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [90/5817], Loss: 0.1862, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [100/5817], Loss: 0.1755, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [110/5817], Loss: 0.1877, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [120/5817], Loss: 0.1774, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [130/5817], Loss: 0.1716, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [140/5817], Loss: 0.1875, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [150/5817], Loss: 0.1790, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [160/5817], Loss: 0.1685, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [170/5817], Loss: 0.1662, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [180/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [190/5817], Loss: 0.1701, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [200/5817], Loss: 0.1859, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [210/5817], Loss: 0.1689, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [220/5817], Loss: 0.1778, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [230/5817], Loss: 0.1659, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [240/5817], Loss: 0.1722, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [250/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [260/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [270/5817], Loss: 0.1617, Accuracy: 0.9062\n",
      "Epoch [6/10], Step [280/5817], Loss: 0.1749, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [290/5817], Loss: 0.1724, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [300/5817], Loss: 0.1706, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [310/5817], Loss: 0.1619, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [320/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [330/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [340/5817], Loss: 0.1689, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [350/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [360/5817], Loss: 0.1665, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [370/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [380/5817], Loss: 0.1920, Accuracy: 0.7734\n",
      "Epoch [6/10], Step [390/5817], Loss: 0.1699, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [400/5817], Loss: 0.1636, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [410/5817], Loss: 0.1601, Accuracy: 0.9141\n",
      "Epoch [6/10], Step [420/5817], Loss: 0.1779, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [430/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [440/5817], Loss: 0.1686, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [450/5817], Loss: 0.1685, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [460/5817], Loss: 0.1805, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [470/5817], Loss: 0.1785, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [480/5817], Loss: 0.1654, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [490/5817], Loss: 0.1717, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [500/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [510/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [520/5817], Loss: 0.1773, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [530/5817], Loss: 0.1704, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [540/5817], Loss: 0.1768, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [550/5817], Loss: 0.1799, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [560/5817], Loss: 0.1761, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [570/5817], Loss: 0.1973, Accuracy: 0.7422\n",
      "Epoch [6/10], Step [580/5817], Loss: 0.1806, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [590/5817], Loss: 0.1745, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [600/5817], Loss: 0.1680, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [610/5817], Loss: 0.1826, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [620/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [630/5817], Loss: 0.1661, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [640/5817], Loss: 0.1747, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [650/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [660/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [670/5817], Loss: 0.1674, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [680/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [690/5817], Loss: 0.1683, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [700/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [710/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [720/5817], Loss: 0.1727, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [730/5817], Loss: 0.1686, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [740/5817], Loss: 0.1760, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [750/5817], Loss: 0.1822, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [760/5817], Loss: 0.1784, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [770/5817], Loss: 0.1731, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [780/5817], Loss: 0.1964, Accuracy: 0.7578\n",
      "Epoch [6/10], Step [790/5817], Loss: 0.1723, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [800/5817], Loss: 0.1761, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [810/5817], Loss: 0.1665, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [820/5817], Loss: 0.1847, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [830/5817], Loss: 0.1800, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [840/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [850/5817], Loss: 0.1807, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [860/5817], Loss: 0.1761, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [870/5817], Loss: 0.1840, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [880/5817], Loss: 0.1721, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [890/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [900/5817], Loss: 0.1790, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [910/5817], Loss: 0.1795, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [920/5817], Loss: 0.1818, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [930/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [940/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [950/5817], Loss: 0.1671, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [960/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [970/5817], Loss: 0.1789, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [980/5817], Loss: 0.1724, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [990/5817], Loss: 0.1723, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1000/5817], Loss: 0.1694, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1010/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1020/5817], Loss: 0.1881, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [1030/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [1040/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1050/5817], Loss: 0.1791, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [1060/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1070/5817], Loss: 0.1725, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1080/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1090/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1100/5817], Loss: 0.1899, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [1110/5817], Loss: 0.1853, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [1120/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [1130/5817], Loss: 0.1737, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1140/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1150/5817], Loss: 0.1679, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [1160/5817], Loss: 0.1615, Accuracy: 0.9062\n",
      "Epoch [6/10], Step [1170/5817], Loss: 0.1921, Accuracy: 0.7734\n",
      "Epoch [6/10], Step [1180/5817], Loss: 0.1818, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [1190/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1200/5817], Loss: 0.1798, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [1210/5817], Loss: 0.1727, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1220/5817], Loss: 0.1801, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1230/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1240/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [1250/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1260/5817], Loss: 0.1710, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [1270/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1280/5817], Loss: 0.1847, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [1290/5817], Loss: 0.1657, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [1300/5817], Loss: 0.1825, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1310/5817], Loss: 0.1655, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [1320/5817], Loss: 0.1705, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1330/5817], Loss: 0.1711, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [1340/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [1350/5817], Loss: 0.1640, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [1360/5817], Loss: 0.1829, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [1370/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [1380/5817], Loss: 0.1631, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [1390/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1400/5817], Loss: 0.1604, Accuracy: 0.9141\n",
      "Epoch [6/10], Step [1410/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1420/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1430/5817], Loss: 0.1769, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1440/5817], Loss: 0.1696, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1450/5817], Loss: 0.1697, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1460/5817], Loss: 0.1869, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [1470/5817], Loss: 0.1640, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [1480/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [1490/5817], Loss: 0.1663, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [1500/5817], Loss: 0.1739, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1510/5817], Loss: 0.1586, Accuracy: 0.9219\n",
      "Epoch [6/10], Step [1520/5817], Loss: 0.1882, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [1530/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [1540/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1550/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1560/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1570/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [1580/5817], Loss: 0.1720, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1590/5817], Loss: 0.1720, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1600/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1610/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1620/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1630/5817], Loss: 0.1710, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1640/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [1650/5817], Loss: 0.1644, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [1660/5817], Loss: 0.1826, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [1670/5817], Loss: 0.1663, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [1680/5817], Loss: 0.1864, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1690/5817], Loss: 0.1660, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [1700/5817], Loss: 0.1614, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [1710/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1720/5817], Loss: 0.1715, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [1730/5817], Loss: 0.1693, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [1740/5817], Loss: 0.1770, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1750/5817], Loss: 0.1627, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [1760/5817], Loss: 0.1666, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [1770/5817], Loss: 0.1715, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [1780/5817], Loss: 0.1702, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1790/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [1800/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [1810/5817], Loss: 0.1631, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [1820/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1830/5817], Loss: 0.1892, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [1840/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [1850/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [1860/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1870/5817], Loss: 0.1727, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [1880/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [1890/5817], Loss: 0.1703, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1900/5817], Loss: 0.1690, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1910/5817], Loss: 0.1773, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [1920/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [1930/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [1940/5817], Loss: 0.1642, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [1950/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1960/5817], Loss: 0.1774, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [1970/5817], Loss: 0.1616, Accuracy: 0.9062\n",
      "Epoch [6/10], Step [1980/5817], Loss: 0.1720, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [1990/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [2000/5817], Loss: 0.1749, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2010/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2020/5817], Loss: 0.1689, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [2030/5817], Loss: 0.1611, Accuracy: 0.9062\n",
      "Epoch [6/10], Step [2040/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [2050/5817], Loss: 0.1671, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [2060/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2070/5817], Loss: 0.1816, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [2080/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [2090/5817], Loss: 0.1875, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2100/5817], Loss: 0.1688, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [2110/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2120/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2130/5817], Loss: 0.1765, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2140/5817], Loss: 0.1819, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2150/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2160/5817], Loss: 0.1686, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [2170/5817], Loss: 0.1732, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [2180/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2190/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2200/5817], Loss: 0.1706, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [2210/5817], Loss: 0.1876, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2220/5817], Loss: 0.1667, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [2230/5817], Loss: 0.1781, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2240/5817], Loss: 0.1807, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [2250/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2260/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2270/5817], Loss: 0.1873, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [2280/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2290/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [2300/5817], Loss: 0.1668, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [2310/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [2320/5817], Loss: 0.1874, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2330/5817], Loss: 0.1639, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [2340/5817], Loss: 0.1773, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2350/5817], Loss: 0.1687, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [2360/5817], Loss: 0.1749, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2370/5817], Loss: 0.1611, Accuracy: 0.9219\n",
      "Epoch [6/10], Step [2380/5817], Loss: 0.1778, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2390/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2400/5817], Loss: 0.1641, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [2410/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2420/5817], Loss: 0.1732, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2430/5817], Loss: 0.1763, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2440/5817], Loss: 0.1832, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2450/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [2460/5817], Loss: 0.1813, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [2470/5817], Loss: 0.1711, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [2480/5817], Loss: 0.1847, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2490/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2500/5817], Loss: 0.1913, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [2510/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2520/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2530/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [2540/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2550/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [2560/5817], Loss: 0.1830, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [2570/5817], Loss: 0.1920, Accuracy: 0.7656\n",
      "Epoch [6/10], Step [2580/5817], Loss: 0.1622, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [2590/5817], Loss: 0.1828, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2600/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [2610/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [2620/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2630/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2640/5817], Loss: 0.1810, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2650/5817], Loss: 0.1656, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [2660/5817], Loss: 0.1594, Accuracy: 0.9219\n",
      "Epoch [6/10], Step [2670/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [2680/5817], Loss: 0.1715, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [2690/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2700/5817], Loss: 0.1873, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2710/5817], Loss: 0.1718, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [2720/5817], Loss: 0.1686, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [2730/5817], Loss: 0.1888, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [2740/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2750/5817], Loss: 0.1751, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2760/5817], Loss: 0.1710, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [2770/5817], Loss: 0.1700, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [2780/5817], Loss: 0.1762, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2790/5817], Loss: 0.1712, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [2800/5817], Loss: 0.1652, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [2810/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2820/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [2830/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [2840/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [2850/5817], Loss: 0.1964, Accuracy: 0.7578\n",
      "Epoch [6/10], Step [2860/5817], Loss: 0.1632, Accuracy: 0.9062\n",
      "Epoch [6/10], Step [2870/5817], Loss: 0.1722, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [2880/5817], Loss: 0.1906, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [2890/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2900/5817], Loss: 0.1678, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [2910/5817], Loss: 0.1659, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [2920/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2930/5817], Loss: 0.1642, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [2940/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [2950/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2960/5817], Loss: 0.1774, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [2970/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [2980/5817], Loss: 0.1795, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [2990/5817], Loss: 0.1711, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [3000/5817], Loss: 0.1641, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [3010/5817], Loss: 0.1757, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3020/5817], Loss: 0.1845, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [3030/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3040/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [3050/5817], Loss: 0.1805, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3060/5817], Loss: 0.1885, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [3070/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3080/5817], Loss: 0.1748, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3090/5817], Loss: 0.1871, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [3100/5817], Loss: 0.1708, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [3110/5817], Loss: 0.1638, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [3120/5817], Loss: 0.1711, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [3130/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3140/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [3150/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3160/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3170/5817], Loss: 0.1845, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [3180/5817], Loss: 0.1761, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3190/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3200/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3210/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [3220/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [3230/5817], Loss: 0.1846, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [3240/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3250/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3260/5817], Loss: 0.1858, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [3270/5817], Loss: 0.1761, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3280/5817], Loss: 0.1886, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [3290/5817], Loss: 0.1880, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [3300/5817], Loss: 0.1632, Accuracy: 0.9062\n",
      "Epoch [6/10], Step [3310/5817], Loss: 0.1904, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [3320/5817], Loss: 0.1645, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [3330/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3340/5817], Loss: 0.1745, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [3350/5817], Loss: 0.1684, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [3360/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [3370/5817], Loss: 0.1761, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3380/5817], Loss: 0.1949, Accuracy: 0.7578\n",
      "Epoch [6/10], Step [3390/5817], Loss: 0.1756, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3400/5817], Loss: 0.1666, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [3410/5817], Loss: 0.1694, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [3420/5817], Loss: 0.1855, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [3430/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3440/5817], Loss: 0.1718, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3450/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3460/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3470/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [3480/5817], Loss: 0.1634, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [3490/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [3500/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3510/5817], Loss: 0.1603, Accuracy: 0.9062\n",
      "Epoch [6/10], Step [3520/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [3530/5817], Loss: 0.1711, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3540/5817], Loss: 0.1673, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [3550/5817], Loss: 0.1774, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3560/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [3570/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3580/5817], Loss: 0.1696, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [3590/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3600/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [3610/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3620/5817], Loss: 0.1683, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [3630/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [3640/5817], Loss: 0.1756, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3650/5817], Loss: 0.1787, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3660/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3670/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [3680/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3690/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3700/5817], Loss: 0.1791, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [3710/5817], Loss: 0.1888, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [3720/5817], Loss: 0.1640, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [3730/5817], Loss: 0.1875, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [3740/5817], Loss: 0.1723, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3750/5817], Loss: 0.1642, Accuracy: 0.9062\n",
      "Epoch [6/10], Step [3760/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [3770/5817], Loss: 0.1760, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3780/5817], Loss: 0.1778, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [3790/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3800/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [3810/5817], Loss: 0.1736, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3820/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3830/5817], Loss: 0.1848, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [3840/5817], Loss: 0.1723, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3850/5817], Loss: 0.1686, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [3860/5817], Loss: 0.1745, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3870/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3880/5817], Loss: 0.1636, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [3890/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3900/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [3910/5817], Loss: 0.1710, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [3920/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3930/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [3940/5817], Loss: 0.1752, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [3950/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [3960/5817], Loss: 0.1707, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [3970/5817], Loss: 0.1796, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [3980/5817], Loss: 0.1634, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [3990/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4000/5817], Loss: 0.1774, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4010/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [4020/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4030/5817], Loss: 0.1814, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [4040/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4050/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4060/5817], Loss: 0.1727, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4070/5817], Loss: 0.1737, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4080/5817], Loss: 0.1739, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [4090/5817], Loss: 0.1795, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [4100/5817], Loss: 0.1662, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [4110/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4120/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4130/5817], Loss: 0.1764, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4140/5817], Loss: 0.1625, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [4150/5817], Loss: 0.1689, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [4160/5817], Loss: 0.1634, Accuracy: 0.9062\n",
      "Epoch [6/10], Step [4170/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4180/5817], Loss: 0.1761, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4190/5817], Loss: 0.1867, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [4200/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4210/5817], Loss: 0.1658, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [4220/5817], Loss: 0.1866, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [4230/5817], Loss: 0.1703, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [4240/5817], Loss: 0.1711, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [4250/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [4260/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [4270/5817], Loss: 0.1733, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4280/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [4290/5817], Loss: 0.1923, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [4300/5817], Loss: 0.1744, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4310/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4320/5817], Loss: 0.1715, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [4330/5817], Loss: 0.1731, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4340/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [4350/5817], Loss: 0.1831, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [4360/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4370/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [4380/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4390/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4400/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4410/5817], Loss: 0.1908, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [4420/5817], Loss: 0.1780, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4430/5817], Loss: 0.1661, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [4440/5817], Loss: 0.1832, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [4450/5817], Loss: 0.1754, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4460/5817], Loss: 0.1675, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [4470/5817], Loss: 0.1873, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [4480/5817], Loss: 0.1669, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [4490/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4500/5817], Loss: 0.1630, Accuracy: 0.9141\n",
      "Epoch [6/10], Step [4510/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4520/5817], Loss: 0.1714, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4530/5817], Loss: 0.1662, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [4540/5817], Loss: 0.1663, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [4550/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4560/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4570/5817], Loss: 0.1671, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [4580/5817], Loss: 0.1618, Accuracy: 0.9062\n",
      "Epoch [6/10], Step [4590/5817], Loss: 0.1667, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [4600/5817], Loss: 0.1900, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [4610/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4620/5817], Loss: 0.1736, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4630/5817], Loss: 0.1609, Accuracy: 0.9062\n",
      "Epoch [6/10], Step [4640/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4650/5817], Loss: 0.1822, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [4660/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [4670/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4680/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [4690/5817], Loss: 0.1722, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4700/5817], Loss: 0.1661, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [4710/5817], Loss: 0.1818, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4720/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4730/5817], Loss: 0.1894, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [4740/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4750/5817], Loss: 0.1742, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [4760/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4770/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [4780/5817], Loss: 0.1787, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4790/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [4800/5817], Loss: 0.1809, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4810/5817], Loss: 0.1857, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [4820/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [4830/5817], Loss: 0.1742, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [4840/5817], Loss: 0.1774, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4850/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4860/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4870/5817], Loss: 0.1693, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [4880/5817], Loss: 0.1705, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4890/5817], Loss: 0.1837, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [4900/5817], Loss: 0.1680, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4910/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [4920/5817], Loss: 0.1674, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [4930/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [4940/5817], Loss: 0.1696, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [4950/5817], Loss: 0.1586, Accuracy: 0.9141\n",
      "Epoch [6/10], Step [4960/5817], Loss: 0.1788, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [4970/5817], Loss: 0.1557, Accuracy: 0.9297\n",
      "Epoch [6/10], Step [4980/5817], Loss: 0.1775, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [4990/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [5000/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5010/5817], Loss: 0.1700, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [5020/5817], Loss: 0.1828, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5030/5817], Loss: 0.1705, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [5040/5817], Loss: 0.1748, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [5050/5817], Loss: 0.1646, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [5060/5817], Loss: 0.1701, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [5070/5817], Loss: 0.1725, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [5080/5817], Loss: 0.1644, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [5090/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [5100/5817], Loss: 0.1812, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5110/5817], Loss: 0.1822, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [5120/5817], Loss: 0.1902, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [5130/5817], Loss: 0.1906, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [5140/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [5150/5817], Loss: 0.1710, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [5160/5817], Loss: 0.1809, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5170/5817], Loss: 0.1621, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [5180/5817], Loss: 0.1648, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [5190/5817], Loss: 0.1752, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [5200/5817], Loss: 0.1787, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [5210/5817], Loss: 0.1848, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [5220/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5230/5817], Loss: 0.1897, Accuracy: 0.7734\n",
      "Epoch [6/10], Step [5240/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5250/5817], Loss: 0.1746, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [5260/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5270/5817], Loss: 0.1744, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [5280/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5290/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [6/10], Step [5300/5817], Loss: 0.1839, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5310/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5320/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5330/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5340/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5350/5817], Loss: 0.1719, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [5360/5817], Loss: 0.1705, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [5370/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5380/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5390/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [6/10], Step [5400/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5410/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5420/5817], Loss: 0.1680, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [5430/5817], Loss: 0.1680, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [5440/5817], Loss: 0.1680, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [5450/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5460/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5470/5817], Loss: 0.1818, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5480/5817], Loss: 0.1690, Accuracy: 0.8828\n",
      "Epoch [6/10], Step [5490/5817], Loss: 0.1716, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [5500/5817], Loss: 0.1925, Accuracy: 0.7734\n",
      "Epoch [6/10], Step [5510/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5520/5817], Loss: 0.1638, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [5530/5817], Loss: 0.1745, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [5540/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [5550/5817], Loss: 0.1853, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5560/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [5570/5817], Loss: 0.1753, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5580/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5590/5817], Loss: 0.1672, Accuracy: 0.8750\n",
      "Epoch [6/10], Step [5600/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5610/5817], Loss: 0.1844, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5620/5817], Loss: 0.1879, Accuracy: 0.8047\n",
      "Epoch [6/10], Step [5630/5817], Loss: 0.1921, Accuracy: 0.7812\n",
      "Epoch [6/10], Step [5640/5817], Loss: 0.1648, Accuracy: 0.8984\n",
      "Epoch [6/10], Step [5650/5817], Loss: 0.1800, Accuracy: 0.8125\n",
      "Epoch [6/10], Step [5660/5817], Loss: 0.1911, Accuracy: 0.7891\n",
      "Epoch [6/10], Step [5670/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [6/10], Step [5680/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5690/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [5700/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [5710/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [6/10], Step [5720/5817], Loss: 0.1654, Accuracy: 0.8906\n",
      "Epoch [6/10], Step [5730/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [6/10], Step [5740/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [5750/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [6/10], Step [5760/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5770/5817], Loss: 0.1731, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [5780/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [5790/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [6/10], Step [5800/5817], Loss: 0.1692, Accuracy: 0.8594\n",
      "Epoch [6/10], Step [5810/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [10/5817], Loss: 0.1571, Accuracy: 0.9219\n",
      "Epoch [7/10], Step [20/5817], Loss: 0.1664, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [30/5817], Loss: 0.1726, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [40/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [50/5817], Loss: 0.1762, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [60/5817], Loss: 0.1645, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [70/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [80/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [90/5817], Loss: 0.1865, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [100/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [110/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [120/5817], Loss: 0.1682, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [130/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [140/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [150/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [160/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [170/5817], Loss: 0.1910, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [180/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [190/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [200/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [210/5817], Loss: 0.1862, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [220/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [230/5817], Loss: 0.1712, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [240/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [250/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [260/5817], Loss: 0.1627, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [270/5817], Loss: 0.1654, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [280/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [290/5817], Loss: 0.1593, Accuracy: 0.9219\n",
      "Epoch [7/10], Step [300/5817], Loss: 0.1741, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [310/5817], Loss: 0.1677, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [320/5817], Loss: 0.1651, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [330/5817], Loss: 0.1588, Accuracy: 0.9141\n",
      "Epoch [7/10], Step [340/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [350/5817], Loss: 0.1792, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [360/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [370/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [380/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [390/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [400/5817], Loss: 0.1768, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [410/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [420/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [430/5817], Loss: 0.1652, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [440/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [450/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [460/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [470/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [480/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [490/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [500/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [510/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [520/5817], Loss: 0.1731, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [530/5817], Loss: 0.1723, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [540/5817], Loss: 0.1814, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [550/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [560/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [570/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [580/5817], Loss: 0.1796, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [590/5817], Loss: 0.1698, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [600/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [610/5817], Loss: 0.1629, Accuracy: 0.9062\n",
      "Epoch [7/10], Step [620/5817], Loss: 0.1803, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [630/5817], Loss: 0.1856, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [640/5817], Loss: 0.1791, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [650/5817], Loss: 0.1865, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [660/5817], Loss: 0.1784, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [670/5817], Loss: 0.1675, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [680/5817], Loss: 0.1664, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [690/5817], Loss: 0.1739, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [700/5817], Loss: 0.1632, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [710/5817], Loss: 0.1677, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [720/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [730/5817], Loss: 0.1859, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [740/5817], Loss: 0.1677, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [750/5817], Loss: 0.1717, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [760/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [770/5817], Loss: 0.1569, Accuracy: 0.9297\n",
      "Epoch [7/10], Step [780/5817], Loss: 0.1605, Accuracy: 0.9141\n",
      "Epoch [7/10], Step [790/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [800/5817], Loss: 0.1828, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [810/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [820/5817], Loss: 0.1794, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [830/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [840/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [850/5817], Loss: 0.1753, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [860/5817], Loss: 0.1662, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [870/5817], Loss: 0.1698, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [880/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [890/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [900/5817], Loss: 0.1679, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [910/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [920/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [930/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [940/5817], Loss: 0.1662, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [950/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [960/5817], Loss: 0.1699, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [970/5817], Loss: 0.1829, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [980/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [990/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1000/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1010/5817], Loss: 0.1738, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1020/5817], Loss: 0.1787, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1030/5817], Loss: 0.1625, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [1040/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [1050/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1060/5817], Loss: 0.1643, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [1070/5817], Loss: 0.1765, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1080/5817], Loss: 0.1675, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [1090/5817], Loss: 0.1674, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [1100/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1110/5817], Loss: 0.1662, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [1120/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [1130/5817], Loss: 0.1619, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [1140/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1150/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [1160/5817], Loss: 0.1668, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [1170/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [1180/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1190/5817], Loss: 0.1677, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [1200/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [1210/5817], Loss: 0.1713, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1220/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1230/5817], Loss: 0.1670, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [1240/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1250/5817], Loss: 0.1701, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [1260/5817], Loss: 0.1713, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1270/5817], Loss: 0.1660, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [1280/5817], Loss: 0.1700, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [1290/5817], Loss: 0.1758, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1300/5817], Loss: 0.1651, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [1310/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [1320/5817], Loss: 0.1790, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1330/5817], Loss: 0.1678, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [1340/5817], Loss: 0.1768, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1350/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [1360/5817], Loss: 0.1675, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [1370/5817], Loss: 0.1651, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [1380/5817], Loss: 0.1870, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [1390/5817], Loss: 0.1846, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [1400/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [1410/5817], Loss: 0.1676, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [1420/5817], Loss: 0.1828, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [1430/5817], Loss: 0.1747, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [1440/5817], Loss: 0.1671, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [1450/5817], Loss: 0.1914, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [1460/5817], Loss: 0.1779, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1470/5817], Loss: 0.1768, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [1480/5817], Loss: 0.1678, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [1490/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1500/5817], Loss: 0.1615, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [1510/5817], Loss: 0.1821, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [1520/5817], Loss: 0.1810, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1530/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [1540/5817], Loss: 0.1911, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [1550/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1560/5817], Loss: 0.1840, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [1570/5817], Loss: 0.1666, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [1580/5817], Loss: 0.1641, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [1590/5817], Loss: 0.1760, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1600/5817], Loss: 0.1689, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [1610/5817], Loss: 0.1727, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1620/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [1630/5817], Loss: 0.1692, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [1640/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1650/5817], Loss: 0.1635, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [1660/5817], Loss: 0.1630, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [1670/5817], Loss: 0.1879, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [1680/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1690/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1700/5817], Loss: 0.1649, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [1710/5817], Loss: 0.1738, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [1720/5817], Loss: 0.1805, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [1730/5817], Loss: 0.1789, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [1740/5817], Loss: 0.1628, Accuracy: 0.9062\n",
      "Epoch [7/10], Step [1750/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [1760/5817], Loss: 0.1650, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [1770/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1780/5817], Loss: 0.1686, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [1790/5817], Loss: 0.1626, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [1800/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [1810/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [1820/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1830/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1840/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [1850/5817], Loss: 0.1682, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [1860/5817], Loss: 0.1631, Accuracy: 0.9062\n",
      "Epoch [7/10], Step [1870/5817], Loss: 0.1666, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [1880/5817], Loss: 0.1608, Accuracy: 0.9141\n",
      "Epoch [7/10], Step [1890/5817], Loss: 0.1737, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1900/5817], Loss: 0.1667, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [1910/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [1920/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1930/5817], Loss: 0.1749, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [1940/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [1950/5817], Loss: 0.1698, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [1960/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [1970/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [1980/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [1990/5817], Loss: 0.1841, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2000/5817], Loss: 0.1642, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [2010/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [2020/5817], Loss: 0.1668, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [2030/5817], Loss: 0.1883, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [2040/5817], Loss: 0.1833, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2050/5817], Loss: 0.1681, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [2060/5817], Loss: 0.1677, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [2070/5817], Loss: 0.1773, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2080/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2090/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2100/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2110/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [2120/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2130/5817], Loss: 0.1860, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2140/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2150/5817], Loss: 0.1939, Accuracy: 0.7734\n",
      "Epoch [7/10], Step [2160/5817], Loss: 0.1636, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [2170/5817], Loss: 0.1647, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [2180/5817], Loss: 0.1542, Accuracy: 0.9453\n",
      "Epoch [7/10], Step [2190/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2200/5817], Loss: 0.1675, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [2210/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2220/5817], Loss: 0.1791, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2230/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2240/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2250/5817], Loss: 0.1685, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [2260/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2270/5817], Loss: 0.1690, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [2280/5817], Loss: 0.1693, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [2290/5817], Loss: 0.1629, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [2300/5817], Loss: 0.1817, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2310/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2320/5817], Loss: 0.1669, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [2330/5817], Loss: 0.1662, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [2340/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [2350/5817], Loss: 0.1774, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [2360/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [2370/5817], Loss: 0.1814, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2380/5817], Loss: 0.1704, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [2390/5817], Loss: 0.1567, Accuracy: 0.9062\n",
      "Epoch [7/10], Step [2400/5817], Loss: 0.1717, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [2410/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2420/5817], Loss: 0.1883, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2430/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2440/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [2450/5817], Loss: 0.1729, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [2460/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2470/5817], Loss: 0.1836, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2480/5817], Loss: 0.1867, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [2490/5817], Loss: 0.1778, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2500/5817], Loss: 0.1706, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2510/5817], Loss: 0.1783, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [2520/5817], Loss: 0.1674, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [2530/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2540/5817], Loss: 0.1896, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [2550/5817], Loss: 0.1588, Accuracy: 0.9141\n",
      "Epoch [7/10], Step [2560/5817], Loss: 0.1942, Accuracy: 0.7578\n",
      "Epoch [7/10], Step [2570/5817], Loss: 0.1900, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [2580/5817], Loss: 0.1810, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2590/5817], Loss: 0.1609, Accuracy: 0.9062\n",
      "Epoch [7/10], Step [2600/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [2610/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2620/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2630/5817], Loss: 0.1722, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2640/5817], Loss: 0.1741, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [2650/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2660/5817], Loss: 0.1655, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [2670/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2680/5817], Loss: 0.1664, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [2690/5817], Loss: 0.1734, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [2700/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2710/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2720/5817], Loss: 0.1724, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2730/5817], Loss: 0.1636, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [2740/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [2750/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [2760/5817], Loss: 0.1833, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [2770/5817], Loss: 0.1854, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [2780/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2790/5817], Loss: 0.1820, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [2800/5817], Loss: 0.1839, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [2810/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [2820/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2830/5817], Loss: 0.1673, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [2840/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2850/5817], Loss: 0.1828, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [2860/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2870/5817], Loss: 0.1698, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [2880/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2890/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [2900/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [2910/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2920/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [2930/5817], Loss: 0.1884, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [2940/5817], Loss: 0.1617, Accuracy: 0.9062\n",
      "Epoch [7/10], Step [2950/5817], Loss: 0.1685, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [2960/5817], Loss: 0.1820, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [2970/5817], Loss: 0.1784, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [2980/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [2990/5817], Loss: 0.1888, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [3000/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3010/5817], Loss: 0.1863, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3020/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3030/5817], Loss: 0.1801, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3040/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [3050/5817], Loss: 0.1707, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [3060/5817], Loss: 0.1720, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [3070/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [3080/5817], Loss: 0.1707, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3090/5817], Loss: 0.1735, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3100/5817], Loss: 0.1671, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [3110/5817], Loss: 0.1889, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [3120/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [3130/5817], Loss: 0.1843, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [3140/5817], Loss: 0.1617, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [3150/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3160/5817], Loss: 0.1697, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3170/5817], Loss: 0.1696, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3180/5817], Loss: 0.1663, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [3190/5817], Loss: 0.1714, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [3200/5817], Loss: 0.1761, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3210/5817], Loss: 0.1696, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3220/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [3230/5817], Loss: 0.1700, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [3240/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3250/5817], Loss: 0.1752, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3260/5817], Loss: 0.1763, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3270/5817], Loss: 0.1659, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [3280/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3290/5817], Loss: 0.1869, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3300/5817], Loss: 0.1830, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [3310/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3320/5817], Loss: 0.1704, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3330/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3340/5817], Loss: 0.1756, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3350/5817], Loss: 0.1702, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3360/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3370/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3380/5817], Loss: 0.1735, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [3390/5817], Loss: 0.1830, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [3400/5817], Loss: 0.1780, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3410/5817], Loss: 0.1675, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [3420/5817], Loss: 0.1822, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [3430/5817], Loss: 0.1854, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [3440/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3450/5817], Loss: 0.1930, Accuracy: 0.7734\n",
      "Epoch [7/10], Step [3460/5817], Loss: 0.1702, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [3470/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3480/5817], Loss: 0.1863, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [3490/5817], Loss: 0.1649, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [3500/5817], Loss: 0.1824, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [3510/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [3520/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3530/5817], Loss: 0.1714, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3540/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3550/5817], Loss: 0.1566, Accuracy: 0.9219\n",
      "Epoch [7/10], Step [3560/5817], Loss: 0.1732, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3570/5817], Loss: 0.1704, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [3580/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [3590/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3600/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3610/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3620/5817], Loss: 0.1701, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [3630/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3640/5817], Loss: 0.1897, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [3650/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3660/5817], Loss: 0.1843, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [3670/5817], Loss: 0.1839, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [3680/5817], Loss: 0.1699, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3690/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [3700/5817], Loss: 0.1668, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [3710/5817], Loss: 0.1854, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3720/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [3730/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3740/5817], Loss: 0.1718, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3750/5817], Loss: 0.1685, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [3760/5817], Loss: 0.1697, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [3770/5817], Loss: 0.1789, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [3780/5817], Loss: 0.1692, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [3790/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3800/5817], Loss: 0.1671, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [3810/5817], Loss: 0.1858, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [3820/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3830/5817], Loss: 0.1879, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [3840/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3850/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [3860/5817], Loss: 0.1840, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3870/5817], Loss: 0.1757, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3880/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [3890/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3900/5817], Loss: 0.1859, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [3910/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [3920/5817], Loss: 0.1850, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [3930/5817], Loss: 0.1687, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [3940/5817], Loss: 0.1723, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3950/5817], Loss: 0.1678, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [3960/5817], Loss: 0.1763, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [3970/5817], Loss: 0.1637, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [3980/5817], Loss: 0.1718, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [3990/5817], Loss: 0.1718, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4000/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4010/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4020/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4030/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4040/5817], Loss: 0.1756, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4050/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4060/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4070/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4080/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [4090/5817], Loss: 0.1666, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [4100/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4110/5817], Loss: 0.1804, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4120/5817], Loss: 0.1673, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [4130/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4140/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4150/5817], Loss: 0.1889, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [4160/5817], Loss: 0.1741, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4170/5817], Loss: 0.1669, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [4180/5817], Loss: 0.1714, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4190/5817], Loss: 0.1688, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [4200/5817], Loss: 0.1712, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4210/5817], Loss: 0.1723, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [4220/5817], Loss: 0.1895, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [4230/5817], Loss: 0.1867, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [4240/5817], Loss: 0.1668, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [4250/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [4260/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4270/5817], Loss: 0.1726, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [4280/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4290/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4300/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4310/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4320/5817], Loss: 0.1688, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [4330/5817], Loss: 0.1681, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4340/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4350/5817], Loss: 0.1727, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4360/5817], Loss: 0.1654, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [4370/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4380/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [4390/5817], Loss: 0.1754, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4400/5817], Loss: 0.1735, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [4410/5817], Loss: 0.1687, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [4420/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4430/5817], Loss: 0.1739, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [4440/5817], Loss: 0.1709, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4450/5817], Loss: 0.1825, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [4460/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4470/5817], Loss: 0.1834, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4480/5817], Loss: 0.1647, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [4490/5817], Loss: 0.1678, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [4500/5817], Loss: 0.1750, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4510/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [4520/5817], Loss: 0.1709, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4530/5817], Loss: 0.1621, Accuracy: 0.9062\n",
      "Epoch [7/10], Step [4540/5817], Loss: 0.1804, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4550/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4560/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4570/5817], Loss: 0.1881, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [4580/5817], Loss: 0.1809, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4590/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4600/5817], Loss: 0.1667, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [4610/5817], Loss: 0.1749, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4620/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [4630/5817], Loss: 0.1865, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [4640/5817], Loss: 0.1788, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4650/5817], Loss: 0.1779, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [4660/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4670/5817], Loss: 0.1843, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [4680/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [4690/5817], Loss: 0.1904, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [4700/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4710/5817], Loss: 0.1803, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4720/5817], Loss: 0.1642, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [4730/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [4740/5817], Loss: 0.1693, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [4750/5817], Loss: 0.1692, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [4760/5817], Loss: 0.1728, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4770/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [4780/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [4790/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4800/5817], Loss: 0.1855, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [4810/5817], Loss: 0.1688, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [4820/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4830/5817], Loss: 0.1757, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4840/5817], Loss: 0.1793, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [4850/5817], Loss: 0.1628, Accuracy: 0.9062\n",
      "Epoch [7/10], Step [4860/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4870/5817], Loss: 0.1787, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4880/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4890/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [4900/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [4910/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [4920/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4930/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [4940/5817], Loss: 0.1668, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [4950/5817], Loss: 0.1688, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [4960/5817], Loss: 0.1747, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [4970/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [4980/5817], Loss: 0.1833, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [4990/5817], Loss: 0.1719, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [5000/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [5010/5817], Loss: 0.1731, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5020/5817], Loss: 0.1617, Accuracy: 0.9062\n",
      "Epoch [7/10], Step [5030/5817], Loss: 0.1739, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5040/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5050/5817], Loss: 0.1925, Accuracy: 0.7734\n",
      "Epoch [7/10], Step [5060/5817], Loss: 0.1800, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [5070/5817], Loss: 0.1646, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [5080/5817], Loss: 0.1654, Accuracy: 0.8906\n",
      "Epoch [7/10], Step [5090/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5100/5817], Loss: 0.1744, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [5110/5817], Loss: 0.1781, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [5120/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5130/5817], Loss: 0.1717, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5140/5817], Loss: 0.1817, Accuracy: 0.8047\n",
      "Epoch [7/10], Step [5150/5817], Loss: 0.1704, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [5160/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5170/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5180/5817], Loss: 0.1858, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [5190/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [5200/5817], Loss: 0.1775, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [5210/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5220/5817], Loss: 0.1827, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [5230/5817], Loss: 0.1754, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [5240/5817], Loss: 0.1738, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5250/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5260/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [7/10], Step [5270/5817], Loss: 0.1802, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [5280/5817], Loss: 0.1756, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [5290/5817], Loss: 0.1620, Accuracy: 0.9062\n",
      "Epoch [7/10], Step [5300/5817], Loss: 0.1564, Accuracy: 0.9141\n",
      "Epoch [7/10], Step [5310/5817], Loss: 0.1716, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5320/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5330/5817], Loss: 0.1758, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [5340/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [5350/5817], Loss: 0.1707, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5360/5817], Loss: 0.1581, Accuracy: 0.9141\n",
      "Epoch [7/10], Step [5370/5817], Loss: 0.1653, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [5380/5817], Loss: 0.1799, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5390/5817], Loss: 0.1646, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [5400/5817], Loss: 0.1882, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [5410/5817], Loss: 0.1865, Accuracy: 0.8125\n",
      "Epoch [7/10], Step [5420/5817], Loss: 0.1696, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [5430/5817], Loss: 0.1727, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5440/5817], Loss: 0.1761, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5450/5817], Loss: 0.1610, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [5460/5817], Loss: 0.1683, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [5470/5817], Loss: 0.1660, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [5480/5817], Loss: 0.1776, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5490/5817], Loss: 0.1634, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [5500/5817], Loss: 0.1780, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [5510/5817], Loss: 0.1692, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [5520/5817], Loss: 0.1595, Accuracy: 0.9141\n",
      "Epoch [7/10], Step [5530/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5540/5817], Loss: 0.1665, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [5550/5817], Loss: 0.1669, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [5560/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5570/5817], Loss: 0.1711, Accuracy: 0.8672\n",
      "Epoch [7/10], Step [5580/5817], Loss: 0.1595, Accuracy: 0.9219\n",
      "Epoch [7/10], Step [5590/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5600/5817], Loss: 0.1891, Accuracy: 0.7891\n",
      "Epoch [7/10], Step [5610/5817], Loss: 0.1778, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5620/5817], Loss: 0.1912, Accuracy: 0.7812\n",
      "Epoch [7/10], Step [5630/5817], Loss: 0.1673, Accuracy: 0.8828\n",
      "Epoch [7/10], Step [5640/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5650/5817], Loss: 0.1783, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5660/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5670/5817], Loss: 0.1723, Accuracy: 0.8516\n",
      "Epoch [7/10], Step [5680/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [5690/5817], Loss: 0.1619, Accuracy: 0.9062\n",
      "Epoch [7/10], Step [5700/5817], Loss: 0.1718, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [5710/5817], Loss: 0.1816, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [5720/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [7/10], Step [5730/5817], Loss: 0.1731, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5740/5817], Loss: 0.1774, Accuracy: 0.8359\n",
      "Epoch [7/10], Step [5750/5817], Loss: 0.1624, Accuracy: 0.8984\n",
      "Epoch [7/10], Step [5760/5817], Loss: 0.1668, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5770/5817], Loss: 0.1847, Accuracy: 0.7969\n",
      "Epoch [7/10], Step [5780/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [7/10], Step [5790/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [7/10], Step [5800/5817], Loss: 0.1720, Accuracy: 0.8750\n",
      "Epoch [7/10], Step [5810/5817], Loss: 0.1801, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [10/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [20/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [30/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [40/5817], Loss: 0.1574, Accuracy: 0.9141\n",
      "Epoch [8/10], Step [50/5817], Loss: 0.1828, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [60/5817], Loss: 0.1742, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [70/5817], Loss: 0.1680, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [80/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [90/5817], Loss: 0.1666, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [100/5817], Loss: 0.1757, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [110/5817], Loss: 0.1610, Accuracy: 0.9141\n",
      "Epoch [8/10], Step [120/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [130/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [140/5817], Loss: 0.1789, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [150/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [160/5817], Loss: 0.1685, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [170/5817], Loss: 0.1650, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [180/5817], Loss: 0.1724, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [190/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [200/5817], Loss: 0.1745, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [210/5817], Loss: 0.1664, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [220/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [230/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [240/5817], Loss: 0.1701, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [250/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [260/5817], Loss: 0.1692, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [270/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [280/5817], Loss: 0.1923, Accuracy: 0.7734\n",
      "Epoch [8/10], Step [290/5817], Loss: 0.1656, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [300/5817], Loss: 0.1701, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [310/5817], Loss: 0.1784, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [320/5817], Loss: 0.1687, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [330/5817], Loss: 0.1826, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [340/5817], Loss: 0.1666, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [350/5817], Loss: 0.1739, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [360/5817], Loss: 0.1703, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [370/5817], Loss: 0.1836, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [380/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [390/5817], Loss: 0.1678, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [400/5817], Loss: 0.1847, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [410/5817], Loss: 0.1677, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [420/5817], Loss: 0.1793, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [430/5817], Loss: 0.1668, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [440/5817], Loss: 0.1509, Accuracy: 0.9453\n",
      "Epoch [8/10], Step [450/5817], Loss: 0.1728, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [460/5817], Loss: 0.1878, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [470/5817], Loss: 0.1711, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [480/5817], Loss: 0.1652, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [490/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [500/5817], Loss: 0.1623, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [510/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [520/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [530/5817], Loss: 0.1760, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [540/5817], Loss: 0.1827, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [550/5817], Loss: 0.1659, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [560/5817], Loss: 0.1817, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [570/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [580/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [590/5817], Loss: 0.1656, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [600/5817], Loss: 0.1595, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [610/5817], Loss: 0.1815, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [620/5817], Loss: 0.1731, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [630/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [640/5817], Loss: 0.1701, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [650/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [660/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [670/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [680/5817], Loss: 0.1813, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [690/5817], Loss: 0.1788, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [700/5817], Loss: 0.1742, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [710/5817], Loss: 0.1682, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [720/5817], Loss: 0.1684, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [730/5817], Loss: 0.1798, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [740/5817], Loss: 0.1658, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [750/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [760/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [770/5817], Loss: 0.1702, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [780/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [790/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [800/5817], Loss: 0.1649, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [810/5817], Loss: 0.1830, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [820/5817], Loss: 0.1712, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [830/5817], Loss: 0.1741, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [840/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [850/5817], Loss: 0.1844, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [860/5817], Loss: 0.1716, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [870/5817], Loss: 0.1674, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [880/5817], Loss: 0.1765, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [890/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [900/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [910/5817], Loss: 0.1664, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [920/5817], Loss: 0.1630, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [930/5817], Loss: 0.1693, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [940/5817], Loss: 0.1663, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [950/5817], Loss: 0.1690, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [960/5817], Loss: 0.1723, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [970/5817], Loss: 0.1655, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [980/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [990/5817], Loss: 0.1708, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1000/5817], Loss: 0.1798, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [1010/5817], Loss: 0.1738, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1020/5817], Loss: 0.1683, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1030/5817], Loss: 0.1709, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1040/5817], Loss: 0.1733, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1050/5817], Loss: 0.1761, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1060/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1070/5817], Loss: 0.1649, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [1080/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1090/5817], Loss: 0.1682, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [1100/5817], Loss: 0.1627, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [1110/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1120/5817], Loss: 0.1706, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1130/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1140/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1150/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1160/5817], Loss: 0.1635, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [1170/5817], Loss: 0.1702, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [1180/5817], Loss: 0.1629, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [1190/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1200/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1210/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1220/5817], Loss: 0.1577, Accuracy: 0.9297\n",
      "Epoch [8/10], Step [1230/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1240/5817], Loss: 0.1737, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1250/5817], Loss: 0.1739, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1260/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1270/5817], Loss: 0.1735, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1280/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1290/5817], Loss: 0.1798, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [1300/5817], Loss: 0.1783, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1310/5817], Loss: 0.1689, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1320/5817], Loss: 0.1831, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [1330/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [1340/5817], Loss: 0.1700, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [1350/5817], Loss: 0.1920, Accuracy: 0.7734\n",
      "Epoch [8/10], Step [1360/5817], Loss: 0.1755, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1370/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [1380/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1390/5817], Loss: 0.1559, Accuracy: 0.9219\n",
      "Epoch [8/10], Step [1400/5817], Loss: 0.1652, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [1410/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [1420/5817], Loss: 0.1600, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [1430/5817], Loss: 0.1818, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [1440/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1450/5817], Loss: 0.1739, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1460/5817], Loss: 0.1760, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1470/5817], Loss: 0.1787, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [1480/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1490/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [1500/5817], Loss: 0.1702, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1510/5817], Loss: 0.1613, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [1520/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1530/5817], Loss: 0.1801, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [1540/5817], Loss: 0.1711, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1550/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [1560/5817], Loss: 0.1647, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [1570/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1580/5817], Loss: 0.1608, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [1590/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [1600/5817], Loss: 0.1573, Accuracy: 0.9219\n",
      "Epoch [8/10], Step [1610/5817], Loss: 0.1901, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [1620/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1630/5817], Loss: 0.1661, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [1640/5817], Loss: 0.1652, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [1650/5817], Loss: 0.1652, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [1660/5817], Loss: 0.1631, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [1670/5817], Loss: 0.1705, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [1680/5817], Loss: 0.1836, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [1690/5817], Loss: 0.1611, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [1700/5817], Loss: 0.1710, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [1710/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [1720/5817], Loss: 0.1810, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [1730/5817], Loss: 0.1645, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [1740/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1750/5817], Loss: 0.1676, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [1760/5817], Loss: 0.1649, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [1770/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [1780/5817], Loss: 0.1893, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [1790/5817], Loss: 0.1851, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [1800/5817], Loss: 0.1630, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [1810/5817], Loss: 0.1929, Accuracy: 0.7656\n",
      "Epoch [8/10], Step [1820/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1830/5817], Loss: 0.1899, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [1840/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1850/5817], Loss: 0.1879, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [1860/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [1870/5817], Loss: 0.1727, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1880/5817], Loss: 0.1665, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [1890/5817], Loss: 0.1583, Accuracy: 0.9297\n",
      "Epoch [8/10], Step [1900/5817], Loss: 0.1683, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [1910/5817], Loss: 0.1706, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1920/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1930/5817], Loss: 0.1714, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [1940/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [1950/5817], Loss: 0.1726, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1960/5817], Loss: 0.1858, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [1970/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [1980/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [1990/5817], Loss: 0.1769, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2000/5817], Loss: 0.1803, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2010/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2020/5817], Loss: 0.1646, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [2030/5817], Loss: 0.1586, Accuracy: 0.9219\n",
      "Epoch [8/10], Step [2040/5817], Loss: 0.1812, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [2050/5817], Loss: 0.1746, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2060/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [2070/5817], Loss: 0.1670, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [2080/5817], Loss: 0.1783, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2090/5817], Loss: 0.1805, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2100/5817], Loss: 0.1728, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2110/5817], Loss: 0.1816, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2120/5817], Loss: 0.1710, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [2130/5817], Loss: 0.1650, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [2140/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [2150/5817], Loss: 0.1852, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [2160/5817], Loss: 0.1827, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [2170/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [2180/5817], Loss: 0.1794, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [2190/5817], Loss: 0.1693, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [2200/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2210/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2220/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2230/5817], Loss: 0.1865, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [2240/5817], Loss: 0.1871, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [2250/5817], Loss: 0.1862, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [2260/5817], Loss: 0.1653, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [2270/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [2280/5817], Loss: 0.1702, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [2290/5817], Loss: 0.1646, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [2300/5817], Loss: 0.1747, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2310/5817], Loss: 0.1767, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2320/5817], Loss: 0.1670, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [2330/5817], Loss: 0.1808, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2340/5817], Loss: 0.1684, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [2350/5817], Loss: 0.1761, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2360/5817], Loss: 0.1796, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [2370/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2380/5817], Loss: 0.1663, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [2390/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [2400/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [2410/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [2420/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [2430/5817], Loss: 0.1706, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2440/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2450/5817], Loss: 0.1663, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [2460/5817], Loss: 0.1620, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [2470/5817], Loss: 0.1665, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [2480/5817], Loss: 0.1633, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [2490/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2500/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2510/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2520/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2530/5817], Loss: 0.1834, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [2540/5817], Loss: 0.1859, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [2550/5817], Loss: 0.1705, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [2560/5817], Loss: 0.1718, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [2570/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2580/5817], Loss: 0.1848, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [2590/5817], Loss: 0.1690, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2600/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2610/5817], Loss: 0.1820, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [2620/5817], Loss: 0.1716, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [2630/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [2640/5817], Loss: 0.1557, Accuracy: 0.9219\n",
      "Epoch [8/10], Step [2650/5817], Loss: 0.1726, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [2660/5817], Loss: 0.1663, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [2670/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [2680/5817], Loss: 0.1663, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [2690/5817], Loss: 0.1737, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2700/5817], Loss: 0.1743, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2710/5817], Loss: 0.1686, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [2720/5817], Loss: 0.1620, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [2730/5817], Loss: 0.1659, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [2740/5817], Loss: 0.1844, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [2750/5817], Loss: 0.1636, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [2760/5817], Loss: 0.1880, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [2770/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2780/5817], Loss: 0.1632, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [2790/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2800/5817], Loss: 0.1587, Accuracy: 0.9141\n",
      "Epoch [8/10], Step [2810/5817], Loss: 0.1922, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [2820/5817], Loss: 0.1869, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [2830/5817], Loss: 0.1772, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2840/5817], Loss: 0.1727, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [2850/5817], Loss: 0.1809, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2860/5817], Loss: 0.1678, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [2870/5817], Loss: 0.1717, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [2880/5817], Loss: 0.1676, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [2890/5817], Loss: 0.1661, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [2900/5817], Loss: 0.1622, Accuracy: 0.9141\n",
      "Epoch [8/10], Step [2910/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [2920/5817], Loss: 0.1619, Accuracy: 0.9141\n",
      "Epoch [8/10], Step [2930/5817], Loss: 0.1789, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2940/5817], Loss: 0.1727, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [2950/5817], Loss: 0.1763, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [2960/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [2970/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [2980/5817], Loss: 0.1693, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [2990/5817], Loss: 0.1603, Accuracy: 0.9141\n",
      "Epoch [8/10], Step [3000/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3010/5817], Loss: 0.1779, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3020/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3030/5817], Loss: 0.1717, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3040/5817], Loss: 0.1566, Accuracy: 0.9219\n",
      "Epoch [8/10], Step [3050/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3060/5817], Loss: 0.1727, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [3070/5817], Loss: 0.1640, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [3080/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3090/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3100/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [3110/5817], Loss: 0.1689, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3120/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3130/5817], Loss: 0.1846, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [3140/5817], Loss: 0.1748, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3150/5817], Loss: 0.1687, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3160/5817], Loss: 0.1623, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [3170/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3180/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3190/5817], Loss: 0.1761, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3200/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3210/5817], Loss: 0.1774, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3220/5817], Loss: 0.1675, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [3230/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3240/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3250/5817], Loss: 0.1653, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [3260/5817], Loss: 0.1675, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3270/5817], Loss: 0.1709, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3280/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3290/5817], Loss: 0.1660, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [3300/5817], Loss: 0.1696, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [3310/5817], Loss: 0.1649, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [3320/5817], Loss: 0.1684, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3330/5817], Loss: 0.1621, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [3340/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3350/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3360/5817], Loss: 0.1820, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3370/5817], Loss: 0.1703, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [3380/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [3390/5817], Loss: 0.1767, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3400/5817], Loss: 0.1815, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3410/5817], Loss: 0.1643, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [3420/5817], Loss: 0.1667, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [3430/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [3440/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [3450/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3460/5817], Loss: 0.1578, Accuracy: 0.9297\n",
      "Epoch [8/10], Step [3470/5817], Loss: 0.1773, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3480/5817], Loss: 0.1681, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [3490/5817], Loss: 0.1701, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3500/5817], Loss: 0.1761, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3510/5817], Loss: 0.1605, Accuracy: 0.9141\n",
      "Epoch [8/10], Step [3520/5817], Loss: 0.1898, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [3530/5817], Loss: 0.1907, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [3540/5817], Loss: 0.1647, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [3550/5817], Loss: 0.1853, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [3560/5817], Loss: 0.1894, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [3570/5817], Loss: 0.1739, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3580/5817], Loss: 0.2014, Accuracy: 0.7422\n",
      "Epoch [8/10], Step [3590/5817], Loss: 0.1870, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [3600/5817], Loss: 0.1688, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [3610/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3620/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3630/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [3640/5817], Loss: 0.1697, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3650/5817], Loss: 0.1776, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [3660/5817], Loss: 0.1877, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [3670/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [3680/5817], Loss: 0.1849, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [3690/5817], Loss: 0.1612, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [3700/5817], Loss: 0.1713, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [3710/5817], Loss: 0.1704, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [3720/5817], Loss: 0.1624, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [3730/5817], Loss: 0.1692, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3740/5817], Loss: 0.1808, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [3750/5817], Loss: 0.1695, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [3760/5817], Loss: 0.1679, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3770/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3780/5817], Loss: 0.1796, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [3790/5817], Loss: 0.1601, Accuracy: 0.9219\n",
      "Epoch [8/10], Step [3800/5817], Loss: 0.1800, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [3810/5817], Loss: 0.1888, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [3820/5817], Loss: 0.1703, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3830/5817], Loss: 0.1797, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [3840/5817], Loss: 0.1824, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [3850/5817], Loss: 0.1883, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [3860/5817], Loss: 0.1702, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3870/5817], Loss: 0.1710, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3880/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3890/5817], Loss: 0.1707, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [3900/5817], Loss: 0.1722, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [3910/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [3920/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [3930/5817], Loss: 0.1670, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3940/5817], Loss: 0.1671, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3950/5817], Loss: 0.1642, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [3960/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [3970/5817], Loss: 0.1800, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [3980/5817], Loss: 0.1701, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [3990/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4000/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [4010/5817], Loss: 0.1615, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [4020/5817], Loss: 0.1665, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [4030/5817], Loss: 0.1680, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [4040/5817], Loss: 0.1686, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [4050/5817], Loss: 0.1712, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [4060/5817], Loss: 0.1688, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4070/5817], Loss: 0.1808, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4080/5817], Loss: 0.1658, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [4090/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4100/5817], Loss: 0.1607, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [4110/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4120/5817], Loss: 0.1845, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4130/5817], Loss: 0.1662, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [4140/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4150/5817], Loss: 0.1697, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [4160/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [4170/5817], Loss: 0.1671, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [4180/5817], Loss: 0.1875, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [4190/5817], Loss: 0.1725, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4200/5817], Loss: 0.1629, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [4210/5817], Loss: 0.1895, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [4220/5817], Loss: 0.1649, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [4230/5817], Loss: 0.1861, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [4240/5817], Loss: 0.1816, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4250/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4260/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4270/5817], Loss: 0.1835, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4280/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4290/5817], Loss: 0.1683, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [4300/5817], Loss: 0.1711, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [4310/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4320/5817], Loss: 0.1744, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4330/5817], Loss: 0.1664, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [4340/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [4350/5817], Loss: 0.1552, Accuracy: 0.9375\n",
      "Epoch [8/10], Step [4360/5817], Loss: 0.1771, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4370/5817], Loss: 0.1840, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4380/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4390/5817], Loss: 0.1764, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4400/5817], Loss: 0.1688, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [4410/5817], Loss: 0.1778, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4420/5817], Loss: 0.1736, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4430/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4440/5817], Loss: 0.1807, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4450/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4460/5817], Loss: 0.1865, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [4470/5817], Loss: 0.1713, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [4480/5817], Loss: 0.1517, Accuracy: 0.9609\n",
      "Epoch [8/10], Step [4490/5817], Loss: 0.1807, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4500/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [4510/5817], Loss: 0.1820, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4520/5817], Loss: 0.1761, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4530/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4540/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [4550/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4560/5817], Loss: 0.1856, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [4570/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [4580/5817], Loss: 0.1654, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [4590/5817], Loss: 0.1678, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [4600/5817], Loss: 0.1653, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [4610/5817], Loss: 0.1827, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4620/5817], Loss: 0.1881, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [4630/5817], Loss: 0.1659, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [4640/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4650/5817], Loss: 0.1697, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4660/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4670/5817], Loss: 0.1583, Accuracy: 0.9219\n",
      "Epoch [8/10], Step [4680/5817], Loss: 0.1653, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [4690/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4700/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [4710/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4720/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4730/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4740/5817], Loss: 0.1737, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4750/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [4760/5817], Loss: 0.1891, Accuracy: 0.7812\n",
      "Epoch [8/10], Step [4770/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4780/5817], Loss: 0.1661, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [4790/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [4800/5817], Loss: 0.1835, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4810/5817], Loss: 0.1724, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [4820/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [4830/5817], Loss: 0.1724, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4840/5817], Loss: 0.1730, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [4850/5817], Loss: 0.1649, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [4860/5817], Loss: 0.1875, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [4870/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4880/5817], Loss: 0.1791, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [4890/5817], Loss: 0.1757, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4900/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4910/5817], Loss: 0.1611, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [4920/5817], Loss: 0.1710, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4930/5817], Loss: 0.1735, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4940/5817], Loss: 0.1620, Accuracy: 0.9141\n",
      "Epoch [8/10], Step [4950/5817], Loss: 0.1745, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [4960/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4970/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [4980/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [4990/5817], Loss: 0.1641, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [5000/5817], Loss: 0.1655, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [5010/5817], Loss: 0.1819, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [5020/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [5030/5817], Loss: 0.1613, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [5040/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [5050/5817], Loss: 0.1748, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5060/5817], Loss: 0.1754, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5070/5817], Loss: 0.1645, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [5080/5817], Loss: 0.1704, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [5090/5817], Loss: 0.1867, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [5100/5817], Loss: 0.1738, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5110/5817], Loss: 0.1836, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [5120/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [5130/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [8/10], Step [5140/5817], Loss: 0.1645, Accuracy: 0.8984\n",
      "Epoch [8/10], Step [5150/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5160/5817], Loss: 0.1804, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [5170/5817], Loss: 0.1769, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [5180/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5190/5817], Loss: 0.1720, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [5200/5817], Loss: 0.1590, Accuracy: 0.9219\n",
      "Epoch [8/10], Step [5210/5817], Loss: 0.1591, Accuracy: 0.9141\n",
      "Epoch [8/10], Step [5220/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [5230/5817], Loss: 0.1690, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [5240/5817], Loss: 0.1722, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5250/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [5260/5817], Loss: 0.1761, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [5270/5817], Loss: 0.1675, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [5280/5817], Loss: 0.1746, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [5290/5817], Loss: 0.1578, Accuracy: 0.9219\n",
      "Epoch [8/10], Step [5300/5817], Loss: 0.1739, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [5310/5817], Loss: 0.1732, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5320/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5330/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [5340/5817], Loss: 0.1851, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [5350/5817], Loss: 0.1728, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5360/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [5370/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [8/10], Step [5380/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [5390/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5400/5817], Loss: 0.1760, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5410/5817], Loss: 0.1699, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [5420/5817], Loss: 0.1746, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [5430/5817], Loss: 0.1761, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5440/5817], Loss: 0.1587, Accuracy: 0.9219\n",
      "Epoch [8/10], Step [5450/5817], Loss: 0.1777, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5460/5817], Loss: 0.1824, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [5470/5817], Loss: 0.1842, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [5480/5817], Loss: 0.1745, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5490/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [5500/5817], Loss: 0.1911, Accuracy: 0.7891\n",
      "Epoch [8/10], Step [5510/5817], Loss: 0.1751, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5520/5817], Loss: 0.1702, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [5530/5817], Loss: 0.1646, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [5540/5817], Loss: 0.1593, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [5550/5817], Loss: 0.1675, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [5560/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [8/10], Step [5570/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [8/10], Step [5580/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5590/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [5600/5817], Loss: 0.1717, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [5610/5817], Loss: 0.1682, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [5620/5817], Loss: 0.1831, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [5630/5817], Loss: 0.1655, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [5640/5817], Loss: 0.1647, Accuracy: 0.8906\n",
      "Epoch [8/10], Step [5650/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [5660/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [5670/5817], Loss: 0.1759, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5680/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [8/10], Step [5690/5817], Loss: 0.1801, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [5700/5817], Loss: 0.1766, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [5710/5817], Loss: 0.1666, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [5720/5817], Loss: 0.1638, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [5730/5817], Loss: 0.1793, Accuracy: 0.8438\n",
      "Epoch [8/10], Step [5740/5817], Loss: 0.1681, Accuracy: 0.8828\n",
      "Epoch [8/10], Step [5750/5817], Loss: 0.1673, Accuracy: 0.8750\n",
      "Epoch [8/10], Step [5760/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [8/10], Step [5770/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [8/10], Step [5780/5817], Loss: 0.1872, Accuracy: 0.7969\n",
      "Epoch [8/10], Step [5790/5817], Loss: 0.1609, Accuracy: 0.9062\n",
      "Epoch [8/10], Step [5800/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [8/10], Step [5810/5817], Loss: 0.1779, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [10/5817], Loss: 0.1791, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [20/5817], Loss: 0.1708, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [30/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [40/5817], Loss: 0.1744, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [50/5817], Loss: 0.1651, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [60/5817], Loss: 0.1659, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [70/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [80/5817], Loss: 0.1682, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [90/5817], Loss: 0.1719, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [100/5817], Loss: 0.1673, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [110/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [120/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [130/5817], Loss: 0.1689, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [140/5817], Loss: 0.1661, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [150/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [160/5817], Loss: 0.1701, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [170/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [180/5817], Loss: 0.1652, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [190/5817], Loss: 0.1702, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [200/5817], Loss: 0.1714, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [210/5817], Loss: 0.1655, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [220/5817], Loss: 0.1810, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [230/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [240/5817], Loss: 0.1702, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [250/5817], Loss: 0.1761, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [260/5817], Loss: 0.1627, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [270/5817], Loss: 0.1693, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [280/5817], Loss: 0.1723, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [290/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [300/5817], Loss: 0.1741, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [310/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [320/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [330/5817], Loss: 0.1898, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [340/5817], Loss: 0.1626, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [350/5817], Loss: 0.1778, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [360/5817], Loss: 0.1727, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [370/5817], Loss: 0.1604, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [380/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [390/5817], Loss: 0.1670, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [400/5817], Loss: 0.1834, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [410/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [420/5817], Loss: 0.1755, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [430/5817], Loss: 0.1642, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [440/5817], Loss: 0.1655, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [450/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [460/5817], Loss: 0.1688, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [470/5817], Loss: 0.1655, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [480/5817], Loss: 0.1639, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [490/5817], Loss: 0.1911, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [500/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [510/5817], Loss: 0.1792, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [520/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [530/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [540/5817], Loss: 0.1697, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [550/5817], Loss: 0.1574, Accuracy: 0.9219\n",
      "Epoch [9/10], Step [560/5817], Loss: 0.1661, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [570/5817], Loss: 0.1829, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [580/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [590/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [600/5817], Loss: 0.1655, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [610/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [620/5817], Loss: 0.1640, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [630/5817], Loss: 0.1662, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [640/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [650/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [660/5817], Loss: 0.1622, Accuracy: 0.9141\n",
      "Epoch [9/10], Step [670/5817], Loss: 0.1733, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [680/5817], Loss: 0.1722, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [690/5817], Loss: 0.1680, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [700/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [710/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [720/5817], Loss: 0.1669, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [730/5817], Loss: 0.1798, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [740/5817], Loss: 0.1659, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [750/5817], Loss: 0.1712, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [760/5817], Loss: 0.1673, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [770/5817], Loss: 0.1738, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [780/5817], Loss: 0.1781, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [790/5817], Loss: 0.1666, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [800/5817], Loss: 0.1731, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [810/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [820/5817], Loss: 0.1645, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [830/5817], Loss: 0.1731, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [840/5817], Loss: 0.1727, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [850/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [860/5817], Loss: 0.1761, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [870/5817], Loss: 0.1753, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [880/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [890/5817], Loss: 0.1711, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [900/5817], Loss: 0.1802, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [910/5817], Loss: 0.1635, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [920/5817], Loss: 0.1758, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [930/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [940/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [950/5817], Loss: 0.1687, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [960/5817], Loss: 0.1607, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [970/5817], Loss: 0.1746, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [980/5817], Loss: 0.1631, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [990/5817], Loss: 0.1686, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1000/5817], Loss: 0.1738, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1010/5817], Loss: 0.1738, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1020/5817], Loss: 0.1616, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [1030/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1040/5817], Loss: 0.1584, Accuracy: 0.9141\n",
      "Epoch [9/10], Step [1050/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [1060/5817], Loss: 0.1722, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [1070/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [1080/5817], Loss: 0.1747, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1090/5817], Loss: 0.1672, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [1100/5817], Loss: 0.1701, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [1110/5817], Loss: 0.1624, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [1120/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1130/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [1140/5817], Loss: 0.1647, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [1150/5817], Loss: 0.1626, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [1160/5817], Loss: 0.1914, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [1170/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1180/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [1190/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [1200/5817], Loss: 0.1656, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [1210/5817], Loss: 0.1594, Accuracy: 0.9219\n",
      "Epoch [9/10], Step [1220/5817], Loss: 0.1745, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1230/5817], Loss: 0.1688, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [1240/5817], Loss: 0.1861, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [1250/5817], Loss: 0.1742, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1260/5817], Loss: 0.1719, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1270/5817], Loss: 0.1645, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [1280/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [1290/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [1300/5817], Loss: 0.1860, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [1310/5817], Loss: 0.1676, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [1320/5817], Loss: 0.1619, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [1330/5817], Loss: 0.1718, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1340/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1350/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1360/5817], Loss: 0.1810, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1370/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1380/5817], Loss: 0.1683, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1390/5817], Loss: 0.1654, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [1400/5817], Loss: 0.1677, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [1410/5817], Loss: 0.1705, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [1420/5817], Loss: 0.1699, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1430/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1440/5817], Loss: 0.1655, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [1450/5817], Loss: 0.1727, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1460/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1470/5817], Loss: 0.1670, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [1480/5817], Loss: 0.1822, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [1490/5817], Loss: 0.1799, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1500/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [1510/5817], Loss: 0.1680, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [1520/5817], Loss: 0.1615, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [1530/5817], Loss: 0.1634, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [1540/5817], Loss: 0.1782, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1550/5817], Loss: 0.1615, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [1560/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1570/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1580/5817], Loss: 0.1937, Accuracy: 0.7656\n",
      "Epoch [9/10], Step [1590/5817], Loss: 0.1622, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [1600/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [1610/5817], Loss: 0.1663, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [1620/5817], Loss: 0.1853, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [1630/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1640/5817], Loss: 0.1582, Accuracy: 0.9297\n",
      "Epoch [9/10], Step [1650/5817], Loss: 0.1585, Accuracy: 0.9141\n",
      "Epoch [9/10], Step [1660/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1670/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [1680/5817], Loss: 0.1633, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [1690/5817], Loss: 0.1765, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1700/5817], Loss: 0.1698, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [1710/5817], Loss: 0.1823, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [1720/5817], Loss: 0.1680, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [1730/5817], Loss: 0.1653, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [1740/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1750/5817], Loss: 0.1542, Accuracy: 0.9297\n",
      "Epoch [9/10], Step [1760/5817], Loss: 0.1674, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [1770/5817], Loss: 0.1701, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1780/5817], Loss: 0.1704, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [1790/5817], Loss: 0.1648, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [1800/5817], Loss: 0.1763, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1810/5817], Loss: 0.1835, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [1820/5817], Loss: 0.1731, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [1830/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [1840/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [1850/5817], Loss: 0.1971, Accuracy: 0.7500\n",
      "Epoch [9/10], Step [1860/5817], Loss: 0.1793, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [1870/5817], Loss: 0.1774, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1880/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1890/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1900/5817], Loss: 0.1827, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [1910/5817], Loss: 0.1822, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [1920/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [1930/5817], Loss: 0.1853, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [1940/5817], Loss: 0.1612, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [1950/5817], Loss: 0.1649, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [1960/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [1970/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [1980/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [1990/5817], Loss: 0.1603, Accuracy: 0.9141\n",
      "Epoch [9/10], Step [2000/5817], Loss: 0.1597, Accuracy: 0.9141\n",
      "Epoch [9/10], Step [2010/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2020/5817], Loss: 0.1676, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [2030/5817], Loss: 0.1719, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [2040/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2050/5817], Loss: 0.1627, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [2060/5817], Loss: 0.1831, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [2070/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2080/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2090/5817], Loss: 0.1639, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [2100/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2110/5817], Loss: 0.1748, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2120/5817], Loss: 0.1752, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2130/5817], Loss: 0.1606, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [2140/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2150/5817], Loss: 0.1857, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [2160/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2170/5817], Loss: 0.1632, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [2180/5817], Loss: 0.1761, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2190/5817], Loss: 0.1671, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2200/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2210/5817], Loss: 0.1668, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [2220/5817], Loss: 0.1661, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [2230/5817], Loss: 0.1777, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2240/5817], Loss: 0.1876, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [2250/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2260/5817], Loss: 0.1698, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2270/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2280/5817], Loss: 0.1616, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [2290/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2300/5817], Loss: 0.1650, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [2310/5817], Loss: 0.1699, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [2320/5817], Loss: 0.1748, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2330/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2340/5817], Loss: 0.1819, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [2350/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2360/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2370/5817], Loss: 0.1741, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2380/5817], Loss: 0.1639, Accuracy: 0.9141\n",
      "Epoch [9/10], Step [2390/5817], Loss: 0.1760, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2400/5817], Loss: 0.1660, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [2410/5817], Loss: 0.1627, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [2420/5817], Loss: 0.1727, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [2430/5817], Loss: 0.1787, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2440/5817], Loss: 0.1742, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2450/5817], Loss: 0.1778, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2460/5817], Loss: 0.1746, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2470/5817], Loss: 0.1796, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2480/5817], Loss: 0.1584, Accuracy: 0.9219\n",
      "Epoch [9/10], Step [2490/5817], Loss: 0.1704, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2500/5817], Loss: 0.1627, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [2510/5817], Loss: 0.1716, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [2520/5817], Loss: 0.1708, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2530/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2540/5817], Loss: 0.1848, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [2550/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2560/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [2570/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2580/5817], Loss: 0.1765, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2590/5817], Loss: 0.1664, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [2600/5817], Loss: 0.1780, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2610/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2620/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2630/5817], Loss: 0.1746, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2640/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [2650/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2660/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2670/5817], Loss: 0.1830, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2680/5817], Loss: 0.1719, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2690/5817], Loss: 0.1668, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [2700/5817], Loss: 0.1678, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2710/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2720/5817], Loss: 0.1687, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2730/5817], Loss: 0.1753, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [2740/5817], Loss: 0.1796, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [2750/5817], Loss: 0.1709, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2760/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2770/5817], Loss: 0.1718, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [2780/5817], Loss: 0.1623, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [2790/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2800/5817], Loss: 0.1865, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [2810/5817], Loss: 0.1764, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2820/5817], Loss: 0.1774, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2830/5817], Loss: 0.1753, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2840/5817], Loss: 0.1761, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2850/5817], Loss: 0.1678, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [2860/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2870/5817], Loss: 0.1761, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2880/5817], Loss: 0.1680, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [2890/5817], Loss: 0.1610, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [2900/5817], Loss: 0.1858, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [2910/5817], Loss: 0.1784, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [2920/5817], Loss: 0.1631, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [2930/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [2940/5817], Loss: 0.1633, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [2950/5817], Loss: 0.1630, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [2960/5817], Loss: 0.1657, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [2970/5817], Loss: 0.1802, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [2980/5817], Loss: 0.1558, Accuracy: 0.9297\n",
      "Epoch [9/10], Step [2990/5817], Loss: 0.1717, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3000/5817], Loss: 0.1840, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [3010/5817], Loss: 0.1792, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3020/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [3030/5817], Loss: 0.1605, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [3040/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [3050/5817], Loss: 0.1660, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [3060/5817], Loss: 0.1661, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [3070/5817], Loss: 0.1813, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3080/5817], Loss: 0.1825, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3090/5817], Loss: 0.1571, Accuracy: 0.9219\n",
      "Epoch [9/10], Step [3100/5817], Loss: 0.1844, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [3110/5817], Loss: 0.1575, Accuracy: 0.9219\n",
      "Epoch [9/10], Step [3120/5817], Loss: 0.1667, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [3130/5817], Loss: 0.1661, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [3140/5817], Loss: 0.1817, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3150/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3160/5817], Loss: 0.1782, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3170/5817], Loss: 0.1692, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3180/5817], Loss: 0.1707, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3190/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [3200/5817], Loss: 0.1668, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3210/5817], Loss: 0.1768, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3220/5817], Loss: 0.1727, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [3230/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [3240/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3250/5817], Loss: 0.1933, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [3260/5817], Loss: 0.1575, Accuracy: 0.9141\n",
      "Epoch [9/10], Step [3270/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3280/5817], Loss: 0.1681, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3290/5817], Loss: 0.1723, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [3300/5817], Loss: 0.1863, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3310/5817], Loss: 0.1828, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [3320/5817], Loss: 0.1710, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3330/5817], Loss: 0.1835, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3340/5817], Loss: 0.1709, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3350/5817], Loss: 0.1785, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3360/5817], Loss: 0.1716, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [3370/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3380/5817], Loss: 0.1655, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [3390/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3400/5817], Loss: 0.1803, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3410/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3420/5817], Loss: 0.1722, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [3430/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3440/5817], Loss: 0.1726, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3450/5817], Loss: 0.1912, Accuracy: 0.7656\n",
      "Epoch [9/10], Step [3460/5817], Loss: 0.1923, Accuracy: 0.7812\n",
      "Epoch [9/10], Step [3470/5817], Loss: 0.1777, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3480/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3490/5817], Loss: 0.1735, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3500/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3510/5817], Loss: 0.1737, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3520/5817], Loss: 0.1680, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [3530/5817], Loss: 0.1703, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [3540/5817], Loss: 0.1616, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [3550/5817], Loss: 0.1764, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3560/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3570/5817], Loss: 0.1672, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [3580/5817], Loss: 0.1761, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3590/5817], Loss: 0.1666, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [3600/5817], Loss: 0.1645, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [3610/5817], Loss: 0.1697, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [3620/5817], Loss: 0.1714, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3630/5817], Loss: 0.1841, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [3640/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3650/5817], Loss: 0.1646, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [3660/5817], Loss: 0.1782, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [3670/5817], Loss: 0.1886, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [3680/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3690/5817], Loss: 0.1656, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [3700/5817], Loss: 0.1735, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3710/5817], Loss: 0.1830, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3720/5817], Loss: 0.1797, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3730/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3740/5817], Loss: 0.1675, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [3750/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3760/5817], Loss: 0.1750, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3770/5817], Loss: 0.1669, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [3780/5817], Loss: 0.1631, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [3790/5817], Loss: 0.1707, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3800/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [3810/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3820/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [3830/5817], Loss: 0.1655, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [3840/5817], Loss: 0.1535, Accuracy: 0.9375\n",
      "Epoch [9/10], Step [3850/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [3860/5817], Loss: 0.1885, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [3870/5817], Loss: 0.1573, Accuracy: 0.9219\n",
      "Epoch [9/10], Step [3880/5817], Loss: 0.1709, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3890/5817], Loss: 0.1783, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [3900/5817], Loss: 0.1834, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [3910/5817], Loss: 0.1823, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [3920/5817], Loss: 0.1695, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3930/5817], Loss: 0.1837, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [3940/5817], Loss: 0.1643, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [3950/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [3960/5817], Loss: 0.1810, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [3970/5817], Loss: 0.1716, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [3980/5817], Loss: 0.1818, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [3990/5817], Loss: 0.1690, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4000/5817], Loss: 0.1737, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4010/5817], Loss: 0.1866, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [4020/5817], Loss: 0.1683, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [4030/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4040/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4050/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4060/5817], Loss: 0.1608, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [4070/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4080/5817], Loss: 0.1657, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [4090/5817], Loss: 0.1675, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4100/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4110/5817], Loss: 0.1784, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4120/5817], Loss: 0.1661, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [4130/5817], Loss: 0.1711, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4140/5817], Loss: 0.1673, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [4150/5817], Loss: 0.1865, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [4160/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4170/5817], Loss: 0.1606, Accuracy: 0.9141\n",
      "Epoch [9/10], Step [4180/5817], Loss: 0.1767, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4190/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [4200/5817], Loss: 0.1719, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4210/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4220/5817], Loss: 0.1703, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4230/5817], Loss: 0.1693, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4240/5817], Loss: 0.1784, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4250/5817], Loss: 0.1771, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4260/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4270/5817], Loss: 0.1646, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [4280/5817], Loss: 0.1577, Accuracy: 0.9219\n",
      "Epoch [9/10], Step [4290/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4300/5817], Loss: 0.1692, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4310/5817], Loss: 0.1688, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4320/5817], Loss: 0.1783, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4330/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4340/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4350/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4360/5817], Loss: 0.1726, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4370/5817], Loss: 0.1733, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4380/5817], Loss: 0.1702, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4390/5817], Loss: 0.1734, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4400/5817], Loss: 0.1855, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [4410/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4420/5817], Loss: 0.1710, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4430/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4440/5817], Loss: 0.1814, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4450/5817], Loss: 0.1644, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [4460/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4470/5817], Loss: 0.1682, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4480/5817], Loss: 0.1648, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [4490/5817], Loss: 0.1661, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [4500/5817], Loss: 0.1646, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [4510/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [4520/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4530/5817], Loss: 0.1659, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [4540/5817], Loss: 0.1714, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4550/5817], Loss: 0.1665, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [4560/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4570/5817], Loss: 0.1791, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4580/5817], Loss: 0.1761, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4590/5817], Loss: 0.1683, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [4600/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [4610/5817], Loss: 0.1609, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [4620/5817], Loss: 0.1690, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4630/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4640/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [4650/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4660/5817], Loss: 0.1592, Accuracy: 0.9141\n",
      "Epoch [9/10], Step [4670/5817], Loss: 0.1645, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [4680/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4690/5817], Loss: 0.1805, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4700/5817], Loss: 0.1777, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4710/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4720/5817], Loss: 0.1729, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4730/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [4740/5817], Loss: 0.1604, Accuracy: 0.9141\n",
      "Epoch [9/10], Step [4750/5817], Loss: 0.1842, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [4760/5817], Loss: 0.1631, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [4770/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4780/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [4790/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4800/5817], Loss: 0.1855, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [4810/5817], Loss: 0.1800, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4820/5817], Loss: 0.1655, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [4830/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [4840/5817], Loss: 0.1707, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [4850/5817], Loss: 0.1642, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [4860/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4870/5817], Loss: 0.1625, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [4880/5817], Loss: 0.1797, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [4890/5817], Loss: 0.1651, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [4900/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [4910/5817], Loss: 0.1645, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [4920/5817], Loss: 0.1718, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4930/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4940/5817], Loss: 0.1659, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [4950/5817], Loss: 0.1753, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [4960/5817], Loss: 0.1742, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [4970/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [4980/5817], Loss: 0.1629, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [4990/5817], Loss: 0.1616, Accuracy: 0.9141\n",
      "Epoch [9/10], Step [5000/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5010/5817], Loss: 0.1705, Accuracy: 0.8594\n",
      "Epoch [9/10], Step [5020/5817], Loss: 0.1635, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [5030/5817], Loss: 0.1658, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [5040/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [5050/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [5060/5817], Loss: 0.1783, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [5070/5817], Loss: 0.1772, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5080/5817], Loss: 0.1723, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5090/5817], Loss: 0.1764, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5100/5817], Loss: 0.1654, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [5110/5817], Loss: 0.1950, Accuracy: 0.7578\n",
      "Epoch [9/10], Step [5120/5817], Loss: 0.1835, Accuracy: 0.8125\n",
      "Epoch [9/10], Step [5130/5817], Loss: 0.1724, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5140/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5150/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [5160/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [5170/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [5180/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [5190/5817], Loss: 0.1761, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5200/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [5210/5817], Loss: 0.1652, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [5220/5817], Loss: 0.1627, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [5230/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5240/5817], Loss: 0.1719, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [5250/5817], Loss: 0.1826, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [5260/5817], Loss: 0.1706, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5270/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [5280/5817], Loss: 0.1648, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [5290/5817], Loss: 0.1661, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [5300/5817], Loss: 0.1794, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5310/5817], Loss: 0.1620, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [5320/5817], Loss: 0.1703, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [5330/5817], Loss: 0.1693, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [5340/5817], Loss: 0.1774, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5350/5817], Loss: 0.1926, Accuracy: 0.7656\n",
      "Epoch [9/10], Step [5360/5817], Loss: 0.1585, Accuracy: 0.9219\n",
      "Epoch [9/10], Step [5370/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [5380/5817], Loss: 0.1609, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [5390/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5400/5817], Loss: 0.1717, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5410/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [5420/5817], Loss: 0.1863, Accuracy: 0.7969\n",
      "Epoch [9/10], Step [5430/5817], Loss: 0.1698, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [5440/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5450/5817], Loss: 0.1846, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [5460/5817], Loss: 0.1720, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5470/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5480/5817], Loss: 0.1656, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [5490/5817], Loss: 0.1907, Accuracy: 0.7891\n",
      "Epoch [9/10], Step [5500/5817], Loss: 0.1566, Accuracy: 0.9219\n",
      "Epoch [9/10], Step [5510/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [5520/5817], Loss: 0.1576, Accuracy: 0.9219\n",
      "Epoch [9/10], Step [5530/5817], Loss: 0.1676, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [5540/5817], Loss: 0.1677, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [5550/5817], Loss: 0.1694, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [5560/5817], Loss: 0.1769, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5570/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [5580/5817], Loss: 0.1763, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5590/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5600/5817], Loss: 0.1752, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [5610/5817], Loss: 0.1689, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [5620/5817], Loss: 0.1833, Accuracy: 0.8203\n",
      "Epoch [9/10], Step [5630/5817], Loss: 0.1666, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [5640/5817], Loss: 0.1639, Accuracy: 0.8984\n",
      "Epoch [9/10], Step [5650/5817], Loss: 0.1659, Accuracy: 0.8906\n",
      "Epoch [9/10], Step [5660/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [9/10], Step [5670/5817], Loss: 0.1674, Accuracy: 0.8750\n",
      "Epoch [9/10], Step [5680/5817], Loss: 0.1721, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5690/5817], Loss: 0.1836, Accuracy: 0.8047\n",
      "Epoch [9/10], Step [5700/5817], Loss: 0.1784, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [5710/5817], Loss: 0.1701, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [5720/5817], Loss: 0.1971, Accuracy: 0.7578\n",
      "Epoch [9/10], Step [5730/5817], Loss: 0.1749, Accuracy: 0.8281\n",
      "Epoch [9/10], Step [5740/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5750/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [9/10], Step [5760/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5770/5817], Loss: 0.1621, Accuracy: 0.9062\n",
      "Epoch [9/10], Step [5780/5817], Loss: 0.1754, Accuracy: 0.8438\n",
      "Epoch [9/10], Step [5790/5817], Loss: 0.1745, Accuracy: 0.8516\n",
      "Epoch [9/10], Step [5800/5817], Loss: 0.1682, Accuracy: 0.8828\n",
      "Epoch [9/10], Step [5810/5817], Loss: 0.1692, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [10/5817], Loss: 0.1600, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [20/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [30/5817], Loss: 0.1709, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [40/5817], Loss: 0.1677, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [50/5817], Loss: 0.1692, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [60/5817], Loss: 0.1657, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [70/5817], Loss: 0.1576, Accuracy: 0.9219\n",
      "Epoch [10/10], Step [80/5817], Loss: 0.1632, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [90/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [100/5817], Loss: 0.1876, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [110/5817], Loss: 0.1648, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [120/5817], Loss: 0.1720, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [130/5817], Loss: 0.1759, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [140/5817], Loss: 0.1657, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [150/5817], Loss: 0.1694, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [160/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [170/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [180/5817], Loss: 0.1670, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [190/5817], Loss: 0.1695, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [200/5817], Loss: 0.1567, Accuracy: 0.9219\n",
      "Epoch [10/10], Step [210/5817], Loss: 0.1629, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [220/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [230/5817], Loss: 0.1586, Accuracy: 0.9219\n",
      "Epoch [10/10], Step [240/5817], Loss: 0.1776, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [250/5817], Loss: 0.1624, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [260/5817], Loss: 0.1635, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [270/5817], Loss: 0.1824, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [280/5817], Loss: 0.1697, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [290/5817], Loss: 0.1824, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [300/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [310/5817], Loss: 0.1761, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [320/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [330/5817], Loss: 0.1672, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [340/5817], Loss: 0.1781, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [350/5817], Loss: 0.1710, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [360/5817], Loss: 0.1651, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [370/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [380/5817], Loss: 0.1754, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [390/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [400/5817], Loss: 0.1710, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [410/5817], Loss: 0.1874, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [420/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [430/5817], Loss: 0.1668, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [440/5817], Loss: 0.1716, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [450/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [460/5817], Loss: 0.1827, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [470/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [480/5817], Loss: 0.1641, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [490/5817], Loss: 0.1706, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [500/5817], Loss: 0.1720, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [510/5817], Loss: 0.1770, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [520/5817], Loss: 0.1627, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [530/5817], Loss: 0.1646, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [540/5817], Loss: 0.1613, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [550/5817], Loss: 0.1721, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [560/5817], Loss: 0.1749, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [570/5817], Loss: 0.1631, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [580/5817], Loss: 0.1549, Accuracy: 0.9453\n",
      "Epoch [10/10], Step [590/5817], Loss: 0.1681, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [600/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [610/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [620/5817], Loss: 0.1742, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [630/5817], Loss: 0.1724, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [640/5817], Loss: 0.1749, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [650/5817], Loss: 0.1629, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [660/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [670/5817], Loss: 0.1679, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [680/5817], Loss: 0.1720, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [690/5817], Loss: 0.1563, Accuracy: 0.9219\n",
      "Epoch [10/10], Step [700/5817], Loss: 0.1836, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [710/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [720/5817], Loss: 0.1611, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [730/5817], Loss: 0.1732, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [740/5817], Loss: 0.1675, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [750/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [760/5817], Loss: 0.1614, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [770/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [780/5817], Loss: 0.1730, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [790/5817], Loss: 0.1640, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [800/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [810/5817], Loss: 0.1600, Accuracy: 0.9219\n",
      "Epoch [10/10], Step [820/5817], Loss: 0.1615, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [830/5817], Loss: 0.1766, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [840/5817], Loss: 0.1583, Accuracy: 0.9141\n",
      "Epoch [10/10], Step [850/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [860/5817], Loss: 0.1735, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [870/5817], Loss: 0.1836, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [880/5817], Loss: 0.1586, Accuracy: 0.9297\n",
      "Epoch [10/10], Step [890/5817], Loss: 0.1685, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [900/5817], Loss: 0.1690, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [910/5817], Loss: 0.1653, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [920/5817], Loss: 0.1742, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [930/5817], Loss: 0.1615, Accuracy: 0.9141\n",
      "Epoch [10/10], Step [940/5817], Loss: 0.1753, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [950/5817], Loss: 0.1635, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [960/5817], Loss: 0.1622, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [970/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [980/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [990/5817], Loss: 0.1752, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1000/5817], Loss: 0.1785, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1010/5817], Loss: 0.1785, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1020/5817], Loss: 0.1574, Accuracy: 0.9141\n",
      "Epoch [10/10], Step [1030/5817], Loss: 0.1678, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1040/5817], Loss: 0.1738, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1050/5817], Loss: 0.1549, Accuracy: 0.9297\n",
      "Epoch [10/10], Step [1060/5817], Loss: 0.1693, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1070/5817], Loss: 0.1717, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [1080/5817], Loss: 0.1735, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [1090/5817], Loss: 0.1651, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [1100/5817], Loss: 0.1645, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [1110/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1120/5817], Loss: 0.1750, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1130/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1140/5817], Loss: 0.1668, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [1150/5817], Loss: 0.1796, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1160/5817], Loss: 0.1893, Accuracy: 0.7812\n",
      "Epoch [10/10], Step [1170/5817], Loss: 0.1710, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [1180/5817], Loss: 0.1791, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [1190/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1200/5817], Loss: 0.1800, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1210/5817], Loss: 0.1796, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1220/5817], Loss: 0.1599, Accuracy: 0.9219\n",
      "Epoch [10/10], Step [1230/5817], Loss: 0.1749, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1240/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1250/5817], Loss: 0.1823, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [1260/5817], Loss: 0.1703, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1270/5817], Loss: 0.1719, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1280/5817], Loss: 0.1724, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1290/5817], Loss: 0.1676, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [1300/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1310/5817], Loss: 0.1643, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [1320/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1330/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1340/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1350/5817], Loss: 0.1662, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [1360/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1370/5817], Loss: 0.1762, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1380/5817], Loss: 0.1610, Accuracy: 0.9141\n",
      "Epoch [10/10], Step [1390/5817], Loss: 0.1647, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [1400/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1410/5817], Loss: 0.1843, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [1420/5817], Loss: 0.1634, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [1430/5817], Loss: 0.1730, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1440/5817], Loss: 0.1647, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [1450/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [1460/5817], Loss: 0.1762, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1470/5817], Loss: 0.1757, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1480/5817], Loss: 0.1833, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [1490/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1500/5817], Loss: 0.1634, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [1510/5817], Loss: 0.1595, Accuracy: 0.9141\n",
      "Epoch [10/10], Step [1520/5817], Loss: 0.1688, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1530/5817], Loss: 0.1587, Accuracy: 0.9219\n",
      "Epoch [10/10], Step [1540/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1550/5817], Loss: 0.1789, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1560/5817], Loss: 0.1716, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1570/5817], Loss: 0.1643, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [1580/5817], Loss: 0.1750, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1590/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1600/5817], Loss: 0.1624, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [1610/5817], Loss: 0.1662, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1620/5817], Loss: 0.1671, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [1630/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1640/5817], Loss: 0.1647, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [1650/5817], Loss: 0.1703, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1660/5817], Loss: 0.1771, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1670/5817], Loss: 0.1778, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1680/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1690/5817], Loss: 0.1757, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1700/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1710/5817], Loss: 0.1816, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1720/5817], Loss: 0.1781, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1730/5817], Loss: 0.1662, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [1740/5817], Loss: 0.1709, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1750/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1760/5817], Loss: 0.1807, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1770/5817], Loss: 0.1674, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [1780/5817], Loss: 0.1755, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1790/5817], Loss: 0.1811, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [1800/5817], Loss: 0.1787, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [1810/5817], Loss: 0.1609, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [1820/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [1830/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [1840/5817], Loss: 0.1730, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1850/5817], Loss: 0.1908, Accuracy: 0.7812\n",
      "Epoch [10/10], Step [1860/5817], Loss: 0.1622, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [1870/5817], Loss: 0.1662, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [1880/5817], Loss: 0.1792, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [1890/5817], Loss: 0.1689, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1900/5817], Loss: 0.1747, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1910/5817], Loss: 0.1741, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [1920/5817], Loss: 0.1652, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [1930/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [1940/5817], Loss: 0.1706, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [1950/5817], Loss: 0.1688, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1960/5817], Loss: 0.1837, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [1970/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [1980/5817], Loss: 0.1720, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [1990/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2000/5817], Loss: 0.1651, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [2010/5817], Loss: 0.1677, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [2020/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2030/5817], Loss: 0.1629, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [2040/5817], Loss: 0.1845, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [2050/5817], Loss: 0.1764, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2060/5817], Loss: 0.1725, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2070/5817], Loss: 0.1753, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2080/5817], Loss: 0.1648, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [2090/5817], Loss: 0.1786, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2100/5817], Loss: 0.1634, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [2110/5817], Loss: 0.1688, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [2120/5817], Loss: 0.1660, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [2130/5817], Loss: 0.1659, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [2140/5817], Loss: 0.1641, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [2150/5817], Loss: 0.1707, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2160/5817], Loss: 0.1625, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [2170/5817], Loss: 0.1621, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [2180/5817], Loss: 0.1747, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2190/5817], Loss: 0.1737, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2200/5817], Loss: 0.1770, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2210/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2220/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2230/5817], Loss: 0.1666, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [2240/5817], Loss: 0.1809, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [2250/5817], Loss: 0.1673, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2260/5817], Loss: 0.1697, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2270/5817], Loss: 0.1795, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2280/5817], Loss: 0.1832, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [2290/5817], Loss: 0.1676, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2300/5817], Loss: 0.1709, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2310/5817], Loss: 0.1684, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2320/5817], Loss: 0.1670, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2330/5817], Loss: 0.1686, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2340/5817], Loss: 0.1731, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2350/5817], Loss: 0.1808, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2360/5817], Loss: 0.1634, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [2370/5817], Loss: 0.1729, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2380/5817], Loss: 0.1799, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2390/5817], Loss: 0.1656, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [2400/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2410/5817], Loss: 0.1788, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2420/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2430/5817], Loss: 0.1697, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2440/5817], Loss: 0.1753, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2450/5817], Loss: 0.1686, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [2460/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2470/5817], Loss: 0.1681, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [2480/5817], Loss: 0.1675, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2490/5817], Loss: 0.1692, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2500/5817], Loss: 0.1708, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2510/5817], Loss: 0.1721, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2520/5817], Loss: 0.1695, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2530/5817], Loss: 0.1721, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2540/5817], Loss: 0.1691, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2550/5817], Loss: 0.1689, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2560/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2570/5817], Loss: 0.1744, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2580/5817], Loss: 0.1784, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2590/5817], Loss: 0.1706, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2600/5817], Loss: 0.1838, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [2610/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2620/5817], Loss: 0.1749, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2630/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2640/5817], Loss: 0.1811, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2650/5817], Loss: 0.1754, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2660/5817], Loss: 0.1747, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2670/5817], Loss: 0.1719, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2680/5817], Loss: 0.1668, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [2690/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [2700/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2710/5817], Loss: 0.1686, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2720/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2730/5817], Loss: 0.1631, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [2740/5817], Loss: 0.1805, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2750/5817], Loss: 0.1728, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2760/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2770/5817], Loss: 0.1648, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [2780/5817], Loss: 0.1608, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [2790/5817], Loss: 0.1780, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2800/5817], Loss: 0.1620, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [2810/5817], Loss: 0.1799, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [2820/5817], Loss: 0.1639, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [2830/5817], Loss: 0.1683, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [2840/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2850/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [2860/5817], Loss: 0.1695, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2870/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2880/5817], Loss: 0.1687, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [2890/5817], Loss: 0.1795, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [2900/5817], Loss: 0.1685, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [2910/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [2920/5817], Loss: 0.1629, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [2930/5817], Loss: 0.1722, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [2940/5817], Loss: 0.1810, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [2950/5817], Loss: 0.1634, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [2960/5817], Loss: 0.1798, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [2970/5817], Loss: 0.1893, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [2980/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [2990/5817], Loss: 0.1769, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3000/5817], Loss: 0.1803, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [3010/5817], Loss: 0.1695, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3020/5817], Loss: 0.1707, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3030/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3040/5817], Loss: 0.1683, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [3050/5817], Loss: 0.1661, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3060/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3070/5817], Loss: 0.1767, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3080/5817], Loss: 0.1650, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [3090/5817], Loss: 0.1700, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3100/5817], Loss: 0.1669, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [3110/5817], Loss: 0.1719, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3120/5817], Loss: 0.1695, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3130/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3140/5817], Loss: 0.1812, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [3150/5817], Loss: 0.1662, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [3160/5817], Loss: 0.1837, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [3170/5817], Loss: 0.1747, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3180/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3190/5817], Loss: 0.1605, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [3200/5817], Loss: 0.1782, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [3210/5817], Loss: 0.1790, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3220/5817], Loss: 0.1733, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [3230/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [3240/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3250/5817], Loss: 0.1649, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [3260/5817], Loss: 0.1782, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [3270/5817], Loss: 0.1694, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3280/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3290/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3300/5817], Loss: 0.1610, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [3310/5817], Loss: 0.1661, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [3320/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3330/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3340/5817], Loss: 0.1680, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [3350/5817], Loss: 0.1806, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [3360/5817], Loss: 0.1678, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [3370/5817], Loss: 0.1613, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [3380/5817], Loss: 0.1732, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [3390/5817], Loss: 0.1761, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3400/5817], Loss: 0.1733, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3410/5817], Loss: 0.1684, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3420/5817], Loss: 0.1681, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3430/5817], Loss: 0.1713, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [3440/5817], Loss: 0.1646, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [3450/5817], Loss: 0.1764, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3460/5817], Loss: 0.1727, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3470/5817], Loss: 0.1725, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3480/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [3490/5817], Loss: 0.1609, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [3500/5817], Loss: 0.1730, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [3510/5817], Loss: 0.1817, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [3520/5817], Loss: 0.1690, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3530/5817], Loss: 0.1680, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3540/5817], Loss: 0.1758, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [3550/5817], Loss: 0.1763, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [3560/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3570/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3580/5817], Loss: 0.1681, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3590/5817], Loss: 0.1680, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [3600/5817], Loss: 0.1772, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3610/5817], Loss: 0.1731, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3620/5817], Loss: 0.1701, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [3630/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3640/5817], Loss: 0.1773, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [3650/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [3660/5817], Loss: 0.1757, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3670/5817], Loss: 0.1554, Accuracy: 0.9297\n",
      "Epoch [10/10], Step [3680/5817], Loss: 0.1700, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [3690/5817], Loss: 0.1753, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3700/5817], Loss: 0.1665, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [3710/5817], Loss: 0.1699, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3720/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3730/5817], Loss: 0.1658, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [3740/5817], Loss: 0.1740, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [3750/5817], Loss: 0.1678, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3760/5817], Loss: 0.1627, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [3770/5817], Loss: 0.1775, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [3780/5817], Loss: 0.1610, Accuracy: 0.9141\n",
      "Epoch [10/10], Step [3790/5817], Loss: 0.1693, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [3800/5817], Loss: 0.1636, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [3810/5817], Loss: 0.1720, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3820/5817], Loss: 0.1681, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [3830/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3840/5817], Loss: 0.1584, Accuracy: 0.9219\n",
      "Epoch [10/10], Step [3850/5817], Loss: 0.1611, Accuracy: 0.9141\n",
      "Epoch [10/10], Step [3860/5817], Loss: 0.1771, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [3870/5817], Loss: 0.1796, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3880/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [3890/5817], Loss: 0.1674, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [3900/5817], Loss: 0.1810, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [3910/5817], Loss: 0.1906, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [3920/5817], Loss: 0.1697, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3930/5817], Loss: 0.1673, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [3940/5817], Loss: 0.1628, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [3950/5817], Loss: 0.1763, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [3960/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [3970/5817], Loss: 0.1700, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [3980/5817], Loss: 0.1723, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [3990/5817], Loss: 0.1660, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [4000/5817], Loss: 0.1790, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [4010/5817], Loss: 0.1702, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4020/5817], Loss: 0.1778, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4030/5817], Loss: 0.1724, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4040/5817], Loss: 0.1688, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [4050/5817], Loss: 0.1615, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [4060/5817], Loss: 0.1678, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4070/5817], Loss: 0.1688, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [4080/5817], Loss: 0.1677, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [4090/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [4100/5817], Loss: 0.1633, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [4110/5817], Loss: 0.1700, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [4120/5817], Loss: 0.1674, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [4130/5817], Loss: 0.1801, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4140/5817], Loss: 0.1681, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4150/5817], Loss: 0.1719, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4160/5817], Loss: 0.1725, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4170/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4180/5817], Loss: 0.1569, Accuracy: 0.9219\n",
      "Epoch [10/10], Step [4190/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4200/5817], Loss: 0.1619, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [4210/5817], Loss: 0.1644, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [4220/5817], Loss: 0.1690, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4230/5817], Loss: 0.1709, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4240/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4250/5817], Loss: 0.1743, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4260/5817], Loss: 0.1775, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4270/5817], Loss: 0.1650, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4280/5817], Loss: 0.1812, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [4290/5817], Loss: 0.1779, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4300/5817], Loss: 0.1711, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4310/5817], Loss: 0.1832, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [4320/5817], Loss: 0.1788, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [4330/5817], Loss: 0.1672, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4340/5817], Loss: 0.1793, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [4350/5817], Loss: 0.1719, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4360/5817], Loss: 0.1737, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4370/5817], Loss: 0.1764, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4380/5817], Loss: 0.1617, Accuracy: 0.9141\n",
      "Epoch [10/10], Step [4390/5817], Loss: 0.1633, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [4400/5817], Loss: 0.1806, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [4410/5817], Loss: 0.1701, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4420/5817], Loss: 0.1661, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [4430/5817], Loss: 0.1736, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4440/5817], Loss: 0.1759, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4450/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [4460/5817], Loss: 0.1794, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [4470/5817], Loss: 0.1731, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4480/5817], Loss: 0.1666, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [4490/5817], Loss: 0.1636, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [4500/5817], Loss: 0.1673, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4510/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4520/5817], Loss: 0.1676, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4530/5817], Loss: 0.1666, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [4540/5817], Loss: 0.1624, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [4550/5817], Loss: 0.1716, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4560/5817], Loss: 0.1748, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4570/5817], Loss: 0.1793, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4580/5817], Loss: 0.1765, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4590/5817], Loss: 0.1746, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4600/5817], Loss: 0.1699, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4610/5817], Loss: 0.1710, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4620/5817], Loss: 0.1768, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4630/5817], Loss: 0.1713, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4640/5817], Loss: 0.1724, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4650/5817], Loss: 0.1816, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [4660/5817], Loss: 0.1629, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [4670/5817], Loss: 0.1666, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4680/5817], Loss: 0.1794, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4690/5817], Loss: 0.1822, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [4700/5817], Loss: 0.1941, Accuracy: 0.7578\n",
      "Epoch [10/10], Step [4710/5817], Loss: 0.1692, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4720/5817], Loss: 0.1714, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4730/5817], Loss: 0.1672, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [4740/5817], Loss: 0.1648, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [4750/5817], Loss: 0.1804, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [4760/5817], Loss: 0.1712, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4770/5817], Loss: 0.1764, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [4780/5817], Loss: 0.1715, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4790/5817], Loss: 0.1713, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4800/5817], Loss: 0.1742, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4810/5817], Loss: 0.1795, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [4820/5817], Loss: 0.1743, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [4830/5817], Loss: 0.1639, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [4840/5817], Loss: 0.1774, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4850/5817], Loss: 0.1694, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4860/5817], Loss: 0.1788, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [4870/5817], Loss: 0.1830, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [4880/5817], Loss: 0.1717, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4890/5817], Loss: 0.1710, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4900/5817], Loss: 0.1824, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [4910/5817], Loss: 0.1748, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [4920/5817], Loss: 0.1695, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4930/5817], Loss: 0.1696, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [4940/5817], Loss: 0.1596, Accuracy: 0.9141\n",
      "Epoch [10/10], Step [4950/5817], Loss: 0.1800, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [4960/5817], Loss: 0.1679, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [4970/5817], Loss: 0.1648, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [4980/5817], Loss: 0.1766, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [4990/5817], Loss: 0.1689, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [5000/5817], Loss: 0.1710, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5010/5817], Loss: 0.1638, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [5020/5817], Loss: 0.1774, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [5030/5817], Loss: 0.1657, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [5040/5817], Loss: 0.1640, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [5050/5817], Loss: 0.1699, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5060/5817], Loss: 0.1917, Accuracy: 0.7734\n",
      "Epoch [10/10], Step [5070/5817], Loss: 0.1894, Accuracy: 0.7812\n",
      "Epoch [10/10], Step [5080/5817], Loss: 0.1696, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [5090/5817], Loss: 0.1643, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [5100/5817], Loss: 0.1775, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5110/5817], Loss: 0.1619, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [5120/5817], Loss: 0.1703, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5130/5817], Loss: 0.1653, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [5140/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5150/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5160/5817], Loss: 0.1866, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [5170/5817], Loss: 0.1612, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [5180/5817], Loss: 0.1881, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [5190/5817], Loss: 0.1693, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [5200/5817], Loss: 0.1632, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [5210/5817], Loss: 0.1774, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [5220/5817], Loss: 0.1756, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [5230/5817], Loss: 0.1738, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [5240/5817], Loss: 0.1595, Accuracy: 0.9062\n",
      "Epoch [10/10], Step [5250/5817], Loss: 0.1672, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [5260/5817], Loss: 0.1733, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5270/5817], Loss: 0.1767, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5280/5817], Loss: 0.1705, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5290/5817], Loss: 0.1741, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5300/5817], Loss: 0.1681, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5310/5817], Loss: 0.1756, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5320/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5330/5817], Loss: 0.1728, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5340/5817], Loss: 0.1801, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [5350/5817], Loss: 0.1738, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [5360/5817], Loss: 0.1784, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5370/5817], Loss: 0.1673, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [5380/5817], Loss: 0.1710, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5390/5817], Loss: 0.1615, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [5400/5817], Loss: 0.1720, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5410/5817], Loss: 0.1751, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5420/5817], Loss: 0.1734, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5430/5817], Loss: 0.1668, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [5440/5817], Loss: 0.1638, Accuracy: 0.8906\n",
      "Epoch [10/10], Step [5450/5817], Loss: 0.1788, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5460/5817], Loss: 0.1767, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [5470/5817], Loss: 0.1711, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5480/5817], Loss: 0.1683, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [5490/5817], Loss: 0.1923, Accuracy: 0.7734\n",
      "Epoch [10/10], Step [5500/5817], Loss: 0.1728, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5510/5817], Loss: 0.1682, Accuracy: 0.8750\n",
      "Epoch [10/10], Step [5520/5817], Loss: 0.1852, Accuracy: 0.8047\n",
      "Epoch [10/10], Step [5530/5817], Loss: 0.1677, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [5540/5817], Loss: 0.1828, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [5550/5817], Loss: 0.1517, Accuracy: 0.9453\n",
      "Epoch [10/10], Step [5560/5817], Loss: 0.1704, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5570/5817], Loss: 0.1851, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [5580/5817], Loss: 0.1681, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5590/5817], Loss: 0.1809, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5600/5817], Loss: 0.1626, Accuracy: 0.8984\n",
      "Epoch [10/10], Step [5610/5817], Loss: 0.1739, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5620/5817], Loss: 0.1732, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5630/5817], Loss: 0.1704, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5640/5817], Loss: 0.1740, Accuracy: 0.8516\n",
      "Epoch [10/10], Step [5650/5817], Loss: 0.1736, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5660/5817], Loss: 0.1859, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [5670/5817], Loss: 0.1714, Accuracy: 0.8594\n",
      "Epoch [10/10], Step [5680/5817], Loss: 0.1813, Accuracy: 0.8281\n",
      "Epoch [10/10], Step [5690/5817], Loss: 0.1847, Accuracy: 0.7969\n",
      "Epoch [10/10], Step [5700/5817], Loss: 0.1821, Accuracy: 0.8203\n",
      "Epoch [10/10], Step [5710/5817], Loss: 0.1755, Accuracy: 0.8438\n",
      "Epoch [10/10], Step [5720/5817], Loss: 0.1778, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5730/5817], Loss: 0.1512, Accuracy: 0.9531\n",
      "Epoch [10/10], Step [5740/5817], Loss: 0.1786, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5750/5817], Loss: 0.1776, Accuracy: 0.8359\n",
      "Epoch [10/10], Step [5760/5817], Loss: 0.1707, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5770/5817], Loss: 0.1829, Accuracy: 0.8125\n",
      "Epoch [10/10], Step [5780/5817], Loss: 0.1713, Accuracy: 0.8672\n",
      "Epoch [10/10], Step [5790/5817], Loss: 0.1682, Accuracy: 0.8828\n",
      "Epoch [10/10], Step [5800/5817], Loss: 0.1868, Accuracy: 0.7891\n",
      "Epoch [10/10], Step [5810/5817], Loss: 0.1564, Accuracy: 0.9219\n",
      "Test accuracy: 0.8305\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path, embedding_dim, word_index, max_features):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Load the GloVe embeddings matrix\n",
    "glove_file_path = 'glove.twitter.27B.200d.txt'  # Update the path to the downloaded GloVe file\n",
    "embedding_dim = 200\n",
    "embedding_matrix = load_glove_embeddings(glove_file_path, embedding_dim, tokenizer.word_index, max_features)\n",
    "\n",
    "class CNNTLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_dim=200, lstm_out=384, dropout_rate=0.2, num_classes=3, num_filters=128, filter_size=3, pool_size=2):\n",
    "        super(CNNTLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.conv1d = nn.Conv1d(embed_dim, num_filters, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool1d = nn.MaxPool1d(pool_size)\n",
    "        self.bi_lstm = nn.LSTM(num_filters, lstm_out // 2, batch_first=True, bidirectional=True, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(lstm_out, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.bi_lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return self.softmax(x)\n",
    "    \n",
    "y_train_np = y_train.values.reshape(-1, 1)\n",
    "y_test_np = y_test.values.reshape(-1, 1)\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Initialize model, loss function and optimizer\n",
    "model = CNNTLSTM(embedding_matrix).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "accumulation_steps = 4  # Adjust this value based on your GPU memory capacity\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (texts, labels) in enumerate(train_loader):\n",
    "        texts, labels = texts.cuda(), labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels) / accumulation_steps\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == labels).sum().item() / labels.size(0)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item() * accumulation_steps\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "test_dataset = TensorDataset(X_test_torch, y_test_torch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)  # Set a smaller batch size for test dat| qZ\\ASWQ1\n",
    "\n",
    "total_correct = 0 \n",
    "total_samples = 0\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.cuda(), labels.cuda()\n",
    "        test_outputs = model(texts)\n",
    "        _, predicted = torch.max(test_outputs.data, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = total_correct / total_samples\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.87     52960\n",
      "           1       0.84      0.87      0.85     53005\n",
      "           2       0.80      0.75      0.77     53561\n",
      "\n",
      "    accuracy                           0.83    159526\n",
      "   macro avg       0.83      0.83      0.83    159526\n",
      "weighted avg       0.83      0.83      0.83    159526\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46781  1740  4439]\n",
      " [ 1604 45987  5414]\n",
      " [ 6527  7028 40006]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_save_path = \"cnntlstm_model56.pth\"  # Choose the path where you want to save the model\n",
    "torch.save(model.state_dict(), model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to load a saved model\n",
    "def load_saved_model(model_path):\n",
    "    model = CNNTLSTM(embedding_matrix).cuda()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Read the CSV file\n",
    "csv_file_path = 'SampleTweets.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Tokenize and pad the preprocessed texts\n",
    "tokenized_texts = tokenizer.texts_to_sequences(df['processed_text'])\n",
    "padded_texts = pad_sequences(tokenized_texts, maxlen=100)\n",
    "\n",
    "# Convert the tokenized and padded texts to PyTorch tensors\n",
    "texts_torch = torch.tensor(padded_texts, dtype=torch.long)\n",
    "\n",
    "# Load the saved model\n",
    "model_path = 'cnntlstm_model56.pth'\n",
    "loaded_model = load_saved_model(model_path)\n",
    "\n",
    "# Use the model to predict sentiment labels for the text data\n",
    "label_map = {0: \"NEGATIVE\", 1: \"POSITIVE\", 2: \"NEUTRAL\"}\n",
    "\n",
    "with torch.no_grad():\n",
    "    texts_torch = texts_torch.cuda()\n",
    "    output_probs = loaded_model(texts_torch)\n",
    "    _, predictions = torch.max(output_probs.data, 1)\n",
    "    predictions = predictions.cpu().numpy()\n",
    "\n",
    "# Convert the predicted labels back to their string representations\n",
    "predicted_labels = [label_map[pred] for pred in predictions]\n",
    "\n",
    "# Add the predicted labels to the original DataFrame and save it to a new CSV file\n",
    "df['predicted_sentiment'] = predicted_labels\n",
    "df.to_csv('predictionsg.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Function to load a saved model\n",
    "# def load_saved_model(model_path):\n",
    "#     model = CNNTLSTM(embedding_matrix).cuda()\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "#     model.eval()\n",
    "#     return model\n",
    "\n",
    "# # Read the CSV file\n",
    "# csv_file_path = 'SampleTweets.csv'\n",
    "# df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# # Tokenize and pad the preprocessed texts\n",
    "# tokenized_texts = tokenizer.texts_to_sequences(df['processed_text'])\n",
    "# padded_texts = pad_sequences(tokenized_texts, maxlen=100)\n",
    "\n",
    "# # Convert the tokenized and padded texts to PyTorch tensors\n",
    "# texts_torch = torch.tensor(padded_texts, dtype=torch.long)\n",
    "\n",
    "# # Load the saved model\n",
    "# model_path = 'cnntlstm_model3.pth'\n",
    "# loaded_model = load_saved_model(model_path)\n",
    "\n",
    "# # Use the model to predict sentiment labels for the text data\n",
    "# label_map = {0: \"NEGATIVE\", 1: \"POSITIVE\", 2: \"NEUTRAL\"}\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     texts_torch = texts_torch.cuda()\n",
    "#     output_probs = loaded_model(texts_torch)\n",
    "#     _, predictions = torch.max(output_probs.data, 1)\n",
    "#     predictions = predictions.cpu().numpy()\n",
    "\n",
    "# # Convert the predicted labels back to their string representations\n",
    "# predicted_labels = [label_map[pred] for pred in predictions]\n",
    "\n",
    "# # Add the predicted labels to the original DataFrame and save it to a new CSV file\n",
    "# df['predicted_sentiment'] = predicted_labels\n",
    "# df.to_csv('predictions5.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU Implementation of CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "# from keras.models import Sequential\n",
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# # Load GloVe embeddings\n",
    "# def load_glove_embeddings(file_path, embedding_dim, word_index, max_features):\n",
    "#     embeddings_index = {}\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "#             coefs = np.asarray(values[1:], dtype='float32')\n",
    "#             embeddings_index[word] = coefs\n",
    "    \n",
    "#     embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "#     for word, i in word_index.items():\n",
    "#         if i >= max_features:\n",
    "#             continue\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             embedding_matrix[i] = embedding_vector\n",
    "#     return embedding_matrix\n",
    "\n",
    "# # Load the GloVe embeddings matrix\n",
    "# glove_file_path = 'glove.twitter.27B.200d.txt'  # Update the path to the downloaded GloVe file\n",
    "# embedding_dim = 200\n",
    "# embedding_matrix = load_glove_embeddings(glove_file_path, embedding_dim, tokenizer.word_index, max_features)\n",
    "\n",
    "# from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "# def create_cnn_lstm_model(embedding_matrix, embed_dim=200, lstm_out=256, dropout_rate=0.2, optimizer='adam', num_classes=3, num_filters=64, filter_size=5, pool_size=2):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(max_features, embed_dim, weights=[embedding_matrix], input_length=X.shape[1], trainable=False))\n",
    "#     model.add(Conv1D(num_filters, filter_size, activation='relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=pool_size))\n",
    "#     model.add(Bidirectional(LSTM(lstm_out, dropout=dropout_rate, recurrent_dropout=dropout_rate)))\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "#     model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# model = create_cnn_lstm_model(embedding_matrix)\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# model_checkpoint = ModelCheckpoint('best_cnn_lstm_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# history = model.fit(X_train, y_train, epochs=5, batch_size=500, validation_split=0.1, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "# # Predict sentiment labels for the test data\n",
    "# y_pred_probs = model.predict(X_test)\n",
    "# y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# # Calculate the accuracy of the predictions\n",
    "# correct_predictions = np.sum(y_pred == y_test)\n",
    "# total_predictions = len(y_test)\n",
    "# prediction_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "# print(f\"Prediction accuracy: {prediction_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-ienv",
   "language": "python",
   "name": "gpu-ienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
