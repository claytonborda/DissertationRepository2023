{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib as plt \n",
    "\n",
    "tweets = pd.read_csv('combined.csv', encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['tweet_date_created'], axis=1)\n",
    "tweets = tweets.drop(['sentiment_score'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for duplicates and deleting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 762643 duplicate tweet ids. Removing duplicates...\n"
     ]
    }
   ],
   "source": [
    "duplicates = tweets[tweets.duplicated(subset=['tweet_id'], keep=False)]\n",
    "\n",
    "if not duplicates.empty:\n",
    "    print(f\"Found {len(duplicates)} duplicate tweet ids. Removing duplicates...\")\n",
    "    tweets.drop_duplicates(subset=['tweet_id'], inplace=True)\n",
    "else:\n",
    "    print(\"No duplicate tweet ids found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['tweet_id'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting NULLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_text    0\n",
      "language      0\n",
      "sentiment     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets.dropna()\n",
    "print(tweets.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values in the 'language' column are 'en'\n"
     ]
    }
   ],
   "source": [
    "all_english = (tweets['language'] == 'en').all()\n",
    "\n",
    "if all_english:\n",
    "    print(\"All values in the 'language' column are 'en'\")\n",
    "else:\n",
    "    print(\"Not all values in the 'language' column are 'en'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['language'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          2\n",
      "1          2\n",
      "2          2\n",
      "3          0\n",
      "4          2\n",
      "          ..\n",
      "5393957    2\n",
      "5393958    0\n",
      "5393959    2\n",
      "5393960    2\n",
      "5393961    2\n",
      "Name: sentiment_values, Length: 5012534, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bayer Leverkusen goalkeeper Bernd Leno will no...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gary Speed v Blackburn at St James in 2001/02 ...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ChelseaFC Don't make him regret it and start ...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@LiverpoolFF @AnfieldEdition He's a liar, made...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@theesk @Everton Didn't realise Kenwright is d...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text sentiment   \n",
       "0  Bayer Leverkusen goalkeeper Bernd Leno will no...   NEUTRAL  \\\n",
       "1  Gary Speed v Blackburn at St James in 2001/02 ...   NEUTRAL   \n",
       "2  @ChelseaFC Don't make him regret it and start ...   NEUTRAL   \n",
       "3  @LiverpoolFF @AnfieldEdition He's a liar, made...  NEGATIVE   \n",
       "4  @theesk @Everton Didn't realise Kenwright is d...   NEUTRAL   \n",
       "\n",
       "   sentiment_values  \n",
       "0                 2  \n",
       "1                 2  \n",
       "2                 2  \n",
       "3                 0  \n",
       "4                 2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_map = {\"NEGATIVE\": 0, \"POSITIVE\": 1, \"NEUTRAL\": 2, \"MIXED\": 3}\n",
    "\n",
    "# Map the sentiment labels to their numeric values\n",
    "tweets['sentiment_values'] = tweets['sentiment'].map(sentiment_map)\n",
    "\n",
    "# Print the new column that contains the mapped values\n",
    "print(tweets['sentiment_values'])\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices of rows with Mixed sentiment\n",
    "mixed_indices = tweets[tweets['sentiment_values'] == 3].index\n",
    "tweets = tweets.drop(mixed_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of positive tagged tweets is: 1070334\n",
      "No of negative tagged tweets is: 354501\n",
      "No of neutral tagged tweets is: 3549918\n"
     ]
    }
   ],
   "source": [
    "negative_tweets = tweets[tweets['sentiment_values'] == 0]\n",
    "positive_tweets = tweets[tweets['sentiment_values'] == 1]\n",
    "neutral_tweets = tweets[tweets['sentiment_values'] == 2]\n",
    "\n",
    "print('No of positive tagged tweets is: {}'.format(len(positive_tweets)))\n",
    "print('No of negative tagged tweets is: {}'.format(len(negative_tweets)))\n",
    "print('No of neutral tagged tweets is: {}'.format(len(neutral_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of positive tagged tweets is: 354501\n",
      "No of negative tagged tweets is: 354501\n",
      "No of neutral tagged tweets is: 354501\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Shuffle the DataFrame to ensure that the downsampling is random\n",
    "tweets = tweets.sample(frac=1, random_state=42)\n",
    "\n",
    "# Count the number of tweets in each sentiment class\n",
    "counts = tweets['sentiment_values'].value_counts()\n",
    "\n",
    "# Find the smallest class size\n",
    "smallest_size = counts.min()\n",
    "\n",
    "# Downsample each class to the smallest size\n",
    "positive_tweets = tweets[tweets['sentiment_values'] == 1].sample(n=smallest_size, random_state=42)\n",
    "negative_tweets = tweets[tweets['sentiment_values'] == 0].sample(n=smallest_size, random_state=42)\n",
    "neutral_tweets = tweets[tweets['sentiment_values'] == 2].sample(n=smallest_size, random_state=42)\n",
    "\n",
    "# Concatenate the downsampled DataFrames\n",
    "tweets = pd.concat([positive_tweets, negative_tweets, neutral_tweets], ignore_index=True)\n",
    "\n",
    "# Print the new counts of tweets in each class\n",
    "print('No of positive tagged tweets is: {}'.format(len(tweets[tweets['sentiment_values'] == 1])))\n",
    "print('No of negative tagged tweets is: {}'.format(len(tweets[tweets['sentiment_values'] == 0])))\n",
    "print('No of neutral tagged tweets is: {}'.format(len(tweets[tweets['sentiment_values'] == 2])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tweet_text'] = tweets['tweet_text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string \n",
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# Add custom stopwords\n",
    "custom_stopwords = ['dont', 'shouldve', 'arent', 'couldnt', 'didnt', 'doesnt', 'hadnt', 'havent', 'mustnt', 'shouldnt', 'wasnt', 'werent', \n",
    "                    'wont', 'wouldnt']\n",
    "english_stopwords.extend(custom_stopwords)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Get the default NLTK English stopwords\n",
    "    \n",
    "    # List of words to keep\n",
    "    #words_to_keep = {\"off\", \"over\", \"under\", \"few\", \"more\", \"no\", \"not\", \"don't\", \"should\", \"should've\", \"aren't\", \n",
    "    #                 \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"haven't\", \"mustn't\", \"shouldn't\", \"wasn't\", \"weren't\",\n",
    "    #                 \"won't\", \"wouldn't\"}\n",
    "    # Create a custom stopwords list\n",
    "    #custom_stopwords = default_stopwords - words_to_keep\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove whitespace\n",
    "    text = text.strip()\n",
    "    # Remove custom stopwords and join the words in a single string\n",
    "    text = ' '.join([word for word in text.split() if word not in english_stopwords])\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # List of words to keep\n",
    "    #words_to_keep = {\"off\", \"over\", \"under\", \"few\", \"more\", \"no\", \"not\", \"don't\", \"should\", \"should've\", \"aren't\", \n",
    "    #                 \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"haven't\", \"mustn't\", \"shouldn't\", \"wasn't\", \"weren't\",\n",
    "    #                 \"won't\", \"wouldn't\"}\n",
    "    # Create a custom stopwords list\n",
    "    #custom_stopwords = default_stopwords - words_to_keep\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Apply the preprocessing function to the 'text' column\n",
    "tweets['processed_text'] = tweets['tweet_text'].apply(preprocess_text)\n",
    "\n",
    "tweets =  shuffle(tweets).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows containing the word 'not' in the 'processed_text' column\n",
    "rows_with_not = tweets.loc[tweets['processed_text'].str.contains(r'\\bwont\\b', regex=True)]\n",
    "\n",
    "# Print the rows containing the word 'not'\n",
    "print(rows_with_not)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_na(text):\n",
    "    # Remove \"nan\" and \"na\" and join the words in a single string\n",
    "    text = ' '.join([word for word in text.split() if word not in ('nan', 'na', 'n/')])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['sentiment'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets['processed_text']\n",
    "y = tweets['sentiment_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "total_rows = tweets.shape[0]\n",
    "print(\"Total number of rows:\", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tweet_lengths = tweets['processed_text'].apply(lambda x: len(x.split()))\n",
    "mean_length = np.mean(tweet_lengths)\n",
    "std_length = np.std(tweet_lengths)\n",
    "\n",
    "print(\"Average tweet length:\", mean_length)\n",
    "print(\"Standard deviation of tweet length:\", std_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "max_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, maxlen=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into 70% training and 30% combined validation and testing\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the temporary dataset (30% of the entire dataset) into 50% validation and 50% testing\n",
    "# This results in 15% validation and 15% testing of the entire dataset\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Training data size:\", len(X_train))\n",
    "print(\"Validation data size:\", len(X_val))\n",
    "print(\"Testing data size:\", len(X_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU with LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load GloVe embeddings\n",
    "# def load_glove_embeddings(file_path, embedding_dim, word_index, max_features):\n",
    "#     embeddings_index = {}\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "#             coefs = np.asarray(values[1:], dtype='float32')\n",
    "#             embeddings_index[word] = coefs\n",
    "    \n",
    "#     embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "#     for word, i in word_index.items():\n",
    "#         if i >= max_features:\n",
    "#             continue\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             embedding_matrix[i] = embedding_vector\n",
    "#     return torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "# # Load the GloVe embeddings matrix\n",
    "# glove_file_path = 'glove.twitter.27B.200d.txt'  # Update the path to the downloaded GloVe file\n",
    "# embedding_dim = 200\n",
    "# embedding_matrix = load_glove_embeddings(glove_file_path, embedding_dim, tokenizer.word_index, max_features)\n",
    "\n",
    "# # Define the LSTM model with GloVe embeddings\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, embedding_matrix, embed_dim=200, lstm_out=256, dropout_rate=0.2, num_classes=3):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "#         self.lstm = nn.LSTM(embed_dim, lstm_out, bidirectional=True, batch_first=True, dropout=dropout_rate)\n",
    "#         self.fc = nn.Linear(lstm_out * 2, num_classes)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         x, _ = self.lstm(x)\n",
    "#         x = self.dropout(x[:, -1, :])  # Get the last hidden state of the LSTM\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# # Create and train the model with GloVe embeddings\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = LSTMModel(embedding_matrix).to(device)\n",
    "\n",
    "# X_train_torch = torch.LongTensor(X_train).to(device)\n",
    "# y_train_torch = y_train.to(torch.int64).to(device)  # Change data type to int64\n",
    "# X_test_torch = torch.LongTensor(X_test).to(device)\n",
    "# y_test_torch = y_test.to(torch.int64).to(device)  # Change data type to int64\n",
    "\n",
    "# train_data = TensorDataset(X_train_torch, y_train_torch)\n",
    "# train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# num_epochs = 5\n",
    "# accumulation_steps = 4  # Adjust this value based on your needs\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for i, (batch_X, batch_y) in enumerate(train_dataloader):\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(batch_X)\n",
    "#         loss = criterion(outputs, batch_y) / accumulation_steps  # Normalize the loss\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # Accumulate gradients and update weights every accumulation_steps\n",
    "#         if (i + 1) % accumulation_steps == 0:\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#         running_loss += loss.item() * accumulation_steps \n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Step [{i + 1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# # Create DataLoader for the test set\n",
    "# test_data = TensorDataset(X_test_torch, y_test_torch)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# model.eval()\n",
    "# y_pred = []\n",
    "# y_true = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch_X, batch_y in test_dataloader:\n",
    "#         test_outputs = model(batch_X)\n",
    "#         _, batch_pred = torch.max(test_outputs, 1)\n",
    "#         y_pred.extend(batch_pred.cpu().numpy())\n",
    "#         y_true.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# test_accuracy = accuracy_score(y_true, y_pred)\n",
    "# print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specified Hyper-parameters Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m[I 2023-04-29 13:29:50,226]\u001b[0m A new study created in memory with name: no-name-b55de5ec-41d2-4d36-aa85-1d366c3a31c1\u001b[0m\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_21376\\364108414.py:80: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 64, 256, 64))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_21376\\364108414.py:82: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
      "C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_21376\\364108414.py:83: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
      "c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[33m[W 2023-04-29 13:30:36,946]\u001b[0m Trial 0 failed with parameters: {'num_filters': 256.0, 'pool_size': 4.0, 'lstm_out': 64.0, 'dropout_rate': 0.2} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\clayt\\AppData\\Local\\Temp\\ipykernel_21376\\364108414.py\", line 99, in objective\n",
      "    texts, labels = texts.cuda(), labels.cuda()\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-04-29 13:30:36,990]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 140\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[39m# Optimize using Optuna\u001b[39;00m\n\u001b[0;32m    139\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 140\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m96\u001b[39;49m)\n\u001b[0;32m    142\u001b[0m     \u001b[39m# Print the best hyperparameters\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     _optimize(\n\u001b[0;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    435\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\clayt\\anaconda3\\envs\\gpu-ienv\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[24], line 99\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     98\u001b[0m     \u001b[39mfor\u001b[39;00m i, (texts, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m---> 99\u001b[0m         texts, labels \u001b[39m=\u001b[39m texts\u001b[39m.\u001b[39;49mcuda(), labels\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m    101\u001b[0m         \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m    102\u001b[0m         outputs \u001b[39m=\u001b[39m model(texts)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import optuna\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "accumulation_steps = 4 \n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path, embedding_dim, word_index, max_features):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Load the GloVe embeddings matrix\n",
    "glove_file_path = 'glove.twitter.27B.200d.txt'  # Update the path to the downloaded GloVe file\n",
    "embedding_dim = 200\n",
    "embedding_matrix = load_glove_embeddings(glove_file_path, embedding_dim, tokenizer.word_index, max_features)\n",
    "\n",
    "class CNNTLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_dim=200, lstm_out=256, dropout_rate=0.2, num_classes=3, num_filters=64, filter_size=5, pool_size=2):\n",
    "        super(CNNTLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.conv1d = nn.Conv1d(embed_dim, num_filters, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool1d = nn.MaxPool1d(pool_size)\n",
    "        self.bi_lstm = nn.LSTM(num_filters, lstm_out // 2, batch_first=True, bidirectional=True, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(lstm_out, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.bi_lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_val_torch = torch.tensor(X_val, dtype=torch.long)\n",
    "y_val_torch = torch.tensor(y_val.values, dtype=torch.long)\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "y_train_np = y_train.values.reshape(-1, 1)\n",
    "y_val_np = y_val.values.reshape(-1, 1)  # Added this line\n",
    "y_test_np = y_test.values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "hyperparameters_accuracies = []\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be optimized\n",
    "    num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 64, 256, 64))\n",
    "    filter_size = 3\n",
    "    pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
    "    lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.4, step=0.1)\n",
    "\n",
    "\n",
    "    model = CNNTLSTM(embedding_matrix, embed_dim=embedding_dim, lstm_out=lstm_out, dropout_rate=dropout_rate, num_classes=3, num_filters=num_filters, filter_size=filter_size, pool_size=pool_size).cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Define DataLoader within the objective function\n",
    "    train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train the model with the given hyperparameters\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (texts, labels) in enumerate(train_loader):\n",
    "            texts, labels = texts.cuda(), labels.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels) / accumulation_steps\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_dataset = TensorDataset(X_val_torch, y_val_torch)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_val_predictions = []\n",
    "    all_val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            texts, labels = texts.cuda(), labels.cuda()\n",
    "            val_outputs = model(texts)\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            all_val_predictions.extend(predicted.cpu().numpy())\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate the validation accuracy\n",
    "    val_accuracy = accuracy_score(y_val_np, all_val_predictions)\n",
    "\n",
    "    # Print the accuracy for each set of hyperparameters\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f} with hyperparameters: {trial.params}\")\n",
    "\n",
    "    # Append the accuracy and hyperparameters to the list\n",
    "    hyperparameters_accuracies.append((val_accuracy, trial.params))\n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "    # Optimize using Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=96)\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameters Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import optuna\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "accumulation_steps = 4 \n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path, embedding_dim, word_index, max_features):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Load the GloVe embeddings matrix\n",
    "glove_file_path = 'glove.twitter.27B.200d.txt'  # Update the path to the downloaded GloVe file\n",
    "embedding_dim = 200\n",
    "embedding_matrix = load_glove_embeddings(glove_file_path, embedding_dim, tokenizer.word_index, max_features)\n",
    "\n",
    "class CNNTLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_dim=200, lstm_out=256, dropout_rate=0.2, num_classes=3, num_filters=64, filter_size=5, pool_size=2):\n",
    "        super(CNNTLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.conv1d = nn.Conv1d(embed_dim, num_filters, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool1d = nn.MaxPool1d(pool_size)\n",
    "        self.bi_lstm = nn.LSTM(num_filters, lstm_out // 2, batch_first=True, bidirectional=True, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(lstm_out, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.bi_lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_val_torch = torch.tensor(X_val, dtype=torch.long)\n",
    "y_val_torch = torch.tensor(y_val.values, dtype=torch.long)\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "y_train_np = y_train.values.reshape(-1, 1)\n",
    "y_val_np = y_val.values.reshape(-1, 1)  # Added this line\n",
    "y_test_np = y_test.values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "hyperparameters_accuracies = []\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be optimized\n",
    "    num_filters = int(trial.suggest_discrete_uniform(\"num_filters\", 32, 256, 32))\n",
    "    filter_size = int(trial.suggest_discrete_uniform(\"filter_size\", 3, 7, 2))\n",
    "    pool_size = int(trial.suggest_discrete_uniform(\"pool_size\", 2, 4, 2))\n",
    "    lstm_out = int(trial.suggest_discrete_uniform(\"lstm_out\", 64, 512, 64))\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.4, step=0.1)\n",
    "\n",
    "    model = CNNTLSTM(embedding_matrix, embed_dim=embedding_dim, lstm_out=lstm_out, dropout_rate=dropout_rate, num_classes=3, num_filters=num_filters, filter_size=filter_size, pool_size=pool_size).cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Define DataLoader within the objective function\n",
    "    train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train the model with the given hyperparameters\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (texts, labels) in enumerate(train_loader):\n",
    "            texts, labels = texts.cuda(), labels.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels) / accumulation_steps\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_dataset = TensorDataset(X_val_torch, y_val_torch)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_val_predictions = []\n",
    "    all_val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            texts, labels = texts.cuda(), labels.cuda()\n",
    "            val_outputs = model(texts)\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            all_val_predictions.extend(predicted.cpu().numpy())\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate the validation accuracy\n",
    "    val_accuracy = accuracy_score(y_val_np, all_val_predictions)\n",
    "\n",
    "    # Print the accuracy for each set of hyperparameters\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f} with hyperparameters: {trial.params}\")\n",
    "\n",
    "    # Append the accuracy and hyperparameters to the list\n",
    "    hyperparameters_accuracies.append((val_accuracy, trial.params))\n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "    # Optimize using Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path, embedding_dim, word_index, max_features):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Load the GloVe embeddings matrix\n",
    "glove_file_path = 'glove.twitter.27B.200d.txt'  # Update the path to the downloaded GloVe file\n",
    "embedding_dim = 200\n",
    "embedding_matrix = load_glove_embeddings(glove_file_path, embedding_dim, tokenizer.word_index, max_features)\n",
    "\n",
    "class CNNTLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_dim=200, lstm_out=448, dropout_rate=0.2, num_classes=3, num_filters=256, filter_size=3, pool_size=2):\n",
    "        super(CNNTLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.conv1d = nn.Conv1d(embed_dim, num_filters, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool1d = nn.MaxPool1d(pool_size)\n",
    "        self.bi_lstm = nn.LSTM(num_filters, lstm_out // 2, batch_first=True, bidirectional=True, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(lstm_out, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.bi_lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return self.softmax(x)\n",
    "    \n",
    "y_train_np = y_train.values.reshape(-1, 1)\n",
    "y_val_np = y_val.values.reshape(-1, 1)\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_val_torch = torch.tensor(X_val, dtype=torch.long)\n",
    "y_val_torch = torch.tensor(y_val.values, dtype=torch.long)\n",
    "\n",
    "# Initialize model, loss function and optimizer\n",
    "model = CNNTLSTM(embedding_matrix).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "accumulation_steps = 4  # Adjust this value based on your GPU memory capacity\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (texts, labels) in enumerate(train_loader):\n",
    "        texts, labels = texts.cuda(), labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels) / accumulation_steps\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == labels).sum().item() / labels.size(0)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item() * accumulation_steps\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "test_dataset = TensorDataset(X_val_torch, y_val_torch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)  # Set a smaller batch size for test dat| qZ\\ASWQ1\n",
    "\n",
    "total_correct = 0 \n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.cuda(), labels.cuda()\n",
    "        test_outputs = model(texts)\n",
    "        _, predicted = torch.max(test_outputs.data, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = total_correct / total_samples\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Save the trained model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model_save_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcnntlstm_modelSP.pth\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Choose the path where you want to save the model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), model_save_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = \"cnntlstm_modelSP.pth\"  # Choose the path where you want to save the model\n",
    "torch.save(model.state_dict(), model_save_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO MODEL WITH VALIDATION - DO NOT USE ONLY ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path, embedding_dim, word_index, max_features):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Load the GloVe embeddings matrix\n",
    "glove_file_path = 'glove.twitter.27B.200d.txt'  # Update the path to the downloaded GloVe file\n",
    "embedding_dim = 200\n",
    "embedding_matrix = load_glove_embeddings(glove_file_path, embedding_dim, tokenizer.word_index, max_features)\n",
    "\n",
    "class CNNTLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_dim=200, lstm_out=384, dropout_rate=0.2, num_classes=3, num_filters=128, filter_size=3, pool_size=2):\n",
    "        super(CNNTLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.conv1d = nn.Conv1d(embed_dim, num_filters, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool1d = nn.MaxPool1d(pool_size)\n",
    "        self.bi_lstm = nn.LSTM(num_filters, lstm_out // 2, batch_first=True, bidirectional=True, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(lstm_out, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.bi_lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return self.softmax(x)\n",
    "    \n",
    "y_train_np = y_train.values.reshape(-1, 1)\n",
    "y_test_np = y_test.values.reshape(-1, 1)\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_torch = torch.tensor(X_val, dtype=torch.long)\n",
    "y_test_torch = torch.tensor(y_val.values, dtype=torch.long)\n",
    "\n",
    "# Initialize model, loss function and optimizer\n",
    "model = CNNTLSTM(embedding_matrix).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "accumulation_steps = 4  # Adjust this value based on your GPU memory capacity\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (texts, labels) in enumerate(train_loader):\n",
    "        texts, labels = texts.cuda(), labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels) / accumulation_steps\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == labels).sum().item() / labels.size(0)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item() * accumulation_steps\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "test_dataset = TensorDataset(X_test_torch, y_test_torch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)  # Set a smaller batch size for test dat| qZ\\ASWQ1\n",
    "\n",
    "total_correct = 0 \n",
    "total_samples = 0\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.cuda(), labels.cuda()\n",
    "        test_outputs = model(texts)\n",
    "        _, predicted = torch.max(test_outputs.data, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = total_correct / total_samples\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU with CNN+LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets =  shuffle(tweets).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path, embedding_dim, word_index, max_features):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Load the GloVe embeddings matrix\n",
    "glove_file_path = 'glove.twitter.27B.200d.txt'  # Update the path to the downloaded GloVe file\n",
    "embedding_dim = 200\n",
    "embedding_matrix = load_glove_embeddings(glove_file_path, embedding_dim, tokenizer.word_index, max_features)\n",
    "\n",
    "class CNNTLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_dim=200, lstm_out=448, dropout_rate=0.2, num_classes=3, num_filters=256, filter_size=3, pool_size=2):\n",
    "        super(CNNTLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.conv1d = nn.Conv1d(embed_dim, num_filters, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool1d = nn.MaxPool1d(pool_size)\n",
    "        self.bi_lstm = nn.LSTM(num_filters, lstm_out // 2, batch_first=True, bidirectional=True, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(lstm_out, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.bi_lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return self.softmax(x)\n",
    "    \n",
    "y_train_np = y_train.values.reshape(-1, 1)\n",
    "y_test_np = y_test.values.reshape(-1, 1)\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Initialize model, loss function and optimizer\n",
    "model = CNNTLSTM(embedding_matrix).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "accumulation_steps = 4  # Adjust this value based on your GPU memory capacity\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (texts, labels) in enumerate(train_loader):\n",
    "        texts, labels = texts.cuda(), labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels) / accumulation_steps\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == labels).sum().item() / labels.size(0)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item() * accumulation_steps\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "test_dataset = TensorDataset(X_test_torch, y_test_torch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)  # Set a smaller batch size for test dat| qZ\\ASWQ1\n",
    "\n",
    "total_correct = 0 \n",
    "total_samples = 0\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.cuda(), labels.cuda()\n",
    "        test_outputs = model(texts)\n",
    "        _, predicted = torch.max(test_outputs.data, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = total_correct / total_samples\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the classification report and confusion matrix\n",
    "report = classification_report(all_labels, all_predictions)\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Print the classification report and confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to load a saved model\n",
    "def load_saved_model(model_path):\n",
    "    model = CNNTLSTM(embedding_matrix).cuda()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Read the CSV file\n",
    "csv_file_path = 'WOLVESNP.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "df['processed_text'] = df['processed_text'].astype(str)\n",
    "\n",
    "# Tokenize and pad the preprocessed texts\n",
    "tokenized_texts = tokenizer.texts_to_sequences(df['processed_text'])\n",
    "padded_texts = pad_sequences(tokenized_texts, maxlen=100)\n",
    "\n",
    "# Convert the tokenized and padded texts to PyTorch tensors\n",
    "texts_torch = torch.tensor(padded_texts, dtype=torch.long)\n",
    "\n",
    "# Load the saved model\n",
    "model_path = 'cnntlstm_modelSP6.pth'\n",
    "loaded_model = load_saved_model(model_path)\n",
    "\n",
    "# Use the model to predict sentiment labels for the text data\n",
    "label_map = {0: \"NEGATIVE\", 1: \"POSITIVE\", 2: \"NEUTRAL\"}\n",
    "\n",
    "# Process the data in smaller batches\n",
    "batch_size = 128\n",
    "num_batches = len(texts_torch) // batch_size + (len(texts_torch) % batch_size > 0)\n",
    "predictions = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch_start = i * batch_size\n",
    "    batch_end = min((i + 1) * batch_size, len(texts_torch))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_texts = texts_torch[batch_start:batch_end].cuda()\n",
    "        output_probs = loaded_model(batch_texts)\n",
    "        _, batch_predictions = torch.max(output_probs.data, 1)\n",
    "        batch_predictions = batch_predictions.cpu().numpy()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "# Convert the predicted labels back to their string representations\n",
    "predicted_labels = [label_map[pred] for pred in predictions]\n",
    "\n",
    "# Add the predicted labels to the original DataFrame and save it to a new CSV file\n",
    "df['predicted_sentiment'] = predicted_labels\n",
    "df.to_csv('WOLVESNPg.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-ienv",
   "language": "python",
   "name": "gpu-ienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
