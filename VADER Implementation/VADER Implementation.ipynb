{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('betsentiment-EN-tweets-sentiment-teams.csv', encoding='latin')\n",
    "\n",
    "tweets = tweets.drop(['tweet_date_created'], axis=1)\n",
    "tweets = tweets.drop(['sentiment_score'], axis=1)\n",
    "tweets = tweets.drop(['language'], axis=1)\n",
    "\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['tweet_id'], axis=1)\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices of rows with Mixed sentiment\n",
    "mixed_indices = tweets[tweets['sentiment'] == 'MIXED'].index\n",
    "\n",
    "# Delete the rows with Mixed sentiment\n",
    "tweets = tweets.drop(mixed_indices)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the values in the \"sentiment\" column to lowercase\n",
    "tweets[\"sentiment\"] = tweets[\"sentiment\"].str.lower()\n",
    "\n",
    "# Print the first 5 rows of the DataFrame to verify the changes\n",
    "print(tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.dropna()\n",
    "print(tweets.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Shuffle the DataFrame to ensure that the downsampling is random\n",
    "tweets = tweets.sample(frac=1, random_state=42)\n",
    "\n",
    "# Count the number of tweets in each sentiment class\n",
    "counts = tweets['sentiment'].value_counts()\n",
    "\n",
    "# Find the smallest class size\n",
    "smallest_size = counts.min()\n",
    "\n",
    "# Downsample each class to the smallest size\n",
    "positive_tweets = tweets[tweets['sentiment'] == 'positive'].sample(n=smallest_size, random_state=42)\n",
    "negative_tweets = tweets[tweets['sentiment'] == 'negative'].sample(n=smallest_size, random_state=42)\n",
    "neutral_tweets = tweets[tweets['sentiment'] == 'neutral'].sample(n=smallest_size, random_state=42)\n",
    "\n",
    "# Concatenate the downsampled DataFrames\n",
    "tweets = pd.concat([positive_tweets, negative_tweets, neutral_tweets], ignore_index=True)\n",
    "\n",
    "# Print the new counts of tweets in each class\n",
    "print('No of positive tagged tweets is: {}'.format(len(tweets[tweets['sentiment'] == 'positive'])))\n",
    "print('No of negative tagged tweets is: {}'.format(len(tweets[tweets['sentiment'] == 'negative'])))\n",
    "print('No of neutral tagged tweets is: {}'.format(len(tweets[tweets['sentiment'] == 'neutral'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove whitespace\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Apply the preprocessing function to the 'text' column\n",
    "tweets['processed_text'] = tweets['tweet_text'].apply(preprocess_text)\n",
    "\n",
    "tweets =  shuffle(tweets).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(output_dict):\n",
    "  \n",
    "  polarity = \"neutral\"\n",
    "\n",
    "  if(output_dict['compound']>= 0.05):\n",
    "    polarity = \"positive\"\n",
    "\n",
    "  elif(output_dict['compound']<= -0.05):\n",
    "    polarity = \"negative\"\n",
    "\n",
    "  return polarity\n",
    "\n",
    "def predict_sentiment(text):\n",
    "  \n",
    "  output_dict =  sent_analyzer.polarity_scores(text)\n",
    "  return format_output(output_dict)\n",
    "\n",
    "# Convert \"tweet_text\" column to string values\n",
    "tweets[\"processed_text\"] = tweets[\"processed_text\"].astype(str)\n",
    "\n",
    "# Run the predictions\n",
    "tweets[\"vader_prediction\"] = tweets[\"processed_text\"].apply(predict_sentiment)\n",
    "\n",
    "# Show 5 random rows of the data\n",
    "tweets.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(tweets['sentiment'], tweets['vader_prediction'])\n",
    "accuracy_percent = accuracy * 100\n",
    "print(\"Accuracy: {}\\n\".format(accuracy_percent))\n",
    "\n",
    "# Show the classification report\n",
    "print(classification_report(tweets['sentiment'], tweets['vader_prediction']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Stopwords Removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string \n",
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# Add custom stopwords\n",
    "custom_stopwords = ['dont', 'shouldve', 'arent', 'couldnt', 'didnt', 'doesnt', 'hadnt', 'havent', 'mustnt', 'shouldnt', 'wasnt', 'werent', 'wont', 'wouldnt']\n",
    "english_stopwords.extend(custom_stopwords)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove whitespace\n",
    "    text = text.strip()\n",
    "    # Remove custom stopwords and join the words in a single string\n",
    "    text = ' '.join([word for word in text.split() if word not in english_stopwords])\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Stopwords Removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string \n",
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "def preprocess_text(text):\n",
    "    # List of words to keep\n",
    "    words_to_keep = {\"off\", \"over\", \"under\", \"few\", \"more\", \"no\", \"not\", \"don't\", \"should\", \"should've\", \"aren't\", \n",
    "                     \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"haven't\", \"mustn't\", \"shouldn't\", \"wasn't\", \"weren't\",\n",
    "                     \"won't\", \"wouldn't\"}\n",
    "    # Create a custom stopwords list\n",
    "    custom_stopwords = english_stopwords - words_to_keep\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove whitespace\n",
    "    text = text.strip()\n",
    "    # Remove custom stopwords and join the words in a single string\n",
    "    text = ' '.join([word for word in text.split() if word not in custom_stopwords])\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(output_dict):\n",
    "  \n",
    "  polarity = \"neutral\"\n",
    "\n",
    "  if(output_dict['compound']>= 0.05):\n",
    "    polarity = \"positive\"\n",
    "\n",
    "  elif(output_dict['compound']<= -0.05):\n",
    "    polarity = \"negative\"\n",
    "\n",
    "  return polarity\n",
    "\n",
    "def predict_sentiment(text):\n",
    "  \n",
    "  output_dict =  sent_analyzer.polarity_scores(text)\n",
    "  return format_output(output_dict)\n",
    "\n",
    "# Convert \"tweet_text\" column to string values\n",
    "tweets[\"processed_text\"] = tweets[\"processed_text\"].astype(str)\n",
    "\n",
    "# Run the predictions\n",
    "tweets[\"vader_prediction\"] = tweets[\"processed_text\"].apply(predict_sentiment)\n",
    "\n",
    "# Show 5 random rows of the data\n",
    "tweets.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(tweets['sentiment'], tweets['vader_prediction'])\n",
    "accuracy_percent = accuracy * 100\n",
    "print(\"Accuracy: {}\\n\".format(accuracy_percent))\n",
    "\n",
    "# Show the classification report\n",
    "print(classification_report(tweets['sentiment'], tweets['vader_prediction']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5394edd7dbe54ff5ee01d77964ae3a51c12f52fc62d5a457fcc64c12a95467e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
